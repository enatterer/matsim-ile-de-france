{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import xml.etree.ElementTree as ET\n",
    "import gzip\n",
    "import json\n",
    "import fiona\n",
    "from collections import defaultdict\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "\n",
    "base_dir = '../../../../data/pop_1pm_simulations/idf_1pm/' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "The idea is that this notebook has to be executed just once. It is then used in understand_simulation_input_data.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        facility_id link_id          x           y activity_type\n",
      "0           edu_100  282093   652608.3   6861929.0     education\n",
      "1         edu_10001  446367   647403.7   6876773.0     education\n",
      "2         edu_10006  712030   727470.7   6825852.0     education\n",
      "3         edu_10012  446110   652276.8   6864355.0     education\n",
      "4         edu_10014  253210   654721.8   6859894.0     education\n",
      "...             ...     ...        ...         ...           ...\n",
      "440221  work_995812   17429  647364.31  6859018.95          work\n",
      "440222  work_997555   87857  649467.87  6863718.43          work\n",
      "440223  work_997914  571839  651728.87   6862703.2          work\n",
      "440224  work_998482  216858  662185.05  6868229.08          work\n",
      "440225  work_999534  508818  647903.65  6859406.96          work\n",
      "\n",
      "[440226 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Function to parse the XML and create a DataFrame\n",
    "def parse_facilities(file_path):\n",
    "    # Decompress and parse the XML\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "\n",
    "    # List to hold the rows of data\n",
    "    data = []\n",
    "\n",
    "    # Loop through each facility\n",
    "    for facility in root.findall('facility'):\n",
    "        # Get facility attributes\n",
    "        facility_id = facility.get('id')\n",
    "        link_id = facility.get('linkId')\n",
    "        x_coord = facility.get('x')\n",
    "        y_coord = facility.get('y')\n",
    "        \n",
    "        # Loop through each activity within the facility\n",
    "        for activity in facility.findall('activity'):\n",
    "            # Get the activity type\n",
    "            activity_type = activity.get('type')\n",
    "            \n",
    "            # Add a row to the data list\n",
    "            data.append({\n",
    "                'facility_id': facility_id,\n",
    "                'link_id': link_id,\n",
    "                'x': x_coord,\n",
    "                'y': y_coord,\n",
    "                'activity_type': activity_type\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Path to your .xml.gz file\n",
    "path_facilities = base_dir + \"idf_1pm_facilities.xml.gz\"\n",
    "\n",
    "# Parse the file and create a DataFrame\n",
    "facilities_df = parse_facilities(path_facilities)\n",
    "\n",
    "# Show the DataFrame\n",
    "print(facilities_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your .xml.gz file\n",
    "path_transit_schedule = base_dir + 'idf_1pm_transit_schedule.xml.gz'\n",
    "\n",
    "# Function to parse the transit schedule and create DataFrames\n",
    "def parse_transit_schedule(file_path):\n",
    "    # Decompress and parse the XML\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "\n",
    "    # Lists to hold the rows of data for stopFacilities and relations\n",
    "    stop_facility_data = []\n",
    "    relation_data = []\n",
    "\n",
    "    # Find the transitStops and minimalTransferTimes sections\n",
    "    transit_stops = root.find('transitStops')\n",
    "    minimal_transfer_times = root.find('minimalTransferTimes')\n",
    "\n",
    "    # Parse stopFacilities within transitStops\n",
    "    for stop_facility in transit_stops.findall('stopFacility'):\n",
    "        stop_facility_data.append({\n",
    "            'id': stop_facility.get('id'),\n",
    "            'linkRefId': stop_facility.get('linkRefId'),\n",
    "            'x': stop_facility.get('x'),\n",
    "            'y': stop_facility.get('y'),\n",
    "            'name': stop_facility.get('name'),\n",
    "            'stopAreaId': stop_facility.get('stopAreaId'),\n",
    "            'isBlocking': stop_facility.get('isBlocking')\n",
    "        })\n",
    "\n",
    "    # Parse relations within minimalTransferTimes\n",
    "    for relation in minimal_transfer_times.findall('relation'):\n",
    "        relation_data.append({\n",
    "            'fromStop': relation.get('fromStop'),\n",
    "            'toStop': relation.get('toStop'),\n",
    "            'transferTime': relation.get('transferTime')\n",
    "        })\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    stop_facility_df = pd.DataFrame(stop_facility_data)\n",
    "    relation_df = pd.DataFrame(relation_data)\n",
    "\n",
    "    return stop_facility_df, relation_df\n",
    "\n",
    "# Parse the transit schedule and get the DataFrames\n",
    "stop_facility_df, relation_df = parse_transit_schedule(path_transit_schedule)\n",
    "\n",
    "# # Show the DataFrames\n",
    "# print(\"Stop Facility DataFrame:\")\n",
    "# print(stop_facility_df.head())\n",
    "\n",
    "# print(\"\\nRelation DataFrame:\")\n",
    "# print(relation_df.head())\n",
    "\n",
    "stop_facility_df.to_csv('intermediate_results/transit_schedule_stop_facilities.csv', index=False)\n",
    "relation_df.to_csv('intermediate_results/transit_schedule_relations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse the population data and create a DataFrame\n",
    "def parse_population(file_path):\n",
    "    # Decompress and parse the XML\n",
    "    with gzip.open(file_path, 'rb') as f:\n",
    "        tree = ET.parse(f)\n",
    "        root = tree.getroot()\n",
    "\n",
    "    # List to hold the rows of data\n",
    "    population_data = []\n",
    "\n",
    "    # Parse persons and their attributes within the population file\n",
    "    for person in root.findall('.//person'):\n",
    "        person_data = {'id': person.get('id')}\n",
    "        \n",
    "        # Parse the person's attributes\n",
    "        attributes = person.find('attributes')\n",
    "        if attributes is not None:\n",
    "            for attribute in attributes.findall('attribute'):\n",
    "                person_data[attribute.get('name')] = attribute.text\n",
    "\n",
    "        # Add the row to the list\n",
    "        population_data.append(person_data)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    population_df = pd.DataFrame(population_data)\n",
    "\n",
    "    return population_df\n",
    "\n",
    "# Path to your .xml.gz file\n",
    "path_population = base_dir + 'idf_1pm_population.xml.gz'\n",
    "\n",
    "# Parse the population file and get the DataFrame\n",
    "population_df = parse_population(path_population)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "population_df.to_csv('intermediate_results/population.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
