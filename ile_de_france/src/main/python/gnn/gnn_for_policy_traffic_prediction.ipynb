{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This is the current working version.\n",
    "The steps are the following:\n",
    "\n",
    "1. Load data\n",
    "2. Pick a loss function\n",
    "3. Split into train and test data\n",
    "4. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "num_epochs = 10\n",
    "batch_size = 50\n",
    "lr = 0.001\n",
    "wandb_name = 'gnn_6'\n",
    "train_ratio = 0.7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../results/results_pop_1pm_first_1400.pkl', 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = torch_geometric.nn.GCNConv(1, 16)\n",
    "        self.conv2 = torch_geometric.nn.GCNConv(16, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func, device):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.inference_mode():\n",
    "        for idx, data in enumerate(valid_dl):\n",
    "            data.x.to(device)\n",
    "            targets = data.y.to(device)\n",
    "            # input_node_feats, targets = data.x.to(device), data.y.to(device)\n",
    "            predicted = model(data)\n",
    "            val_loss += loss_func(predicted, targets)*targets.size(0)\n",
    "    return val_loss \n",
    "\n",
    "def create_dataloader(is_train, batch_size, dataset):\n",
    "    dataset_length = len(dataset)\n",
    "    print(f\"Total dataset length: {dataset_length}\")\n",
    "\n",
    "    # Calculate split index for training and validation\n",
    "    split_idx = int(dataset_length * train_ratio)\n",
    "    \n",
    "    # Calculate the maximum number of samples that fit into complete batches for training and validation\n",
    "    train_samples = (split_idx // batch_size) * batch_size\n",
    "    valid_samples = ((dataset_length - split_idx) // batch_size) * batch_size\n",
    "\n",
    "    if is_train:\n",
    "        indices = range(0, train_samples)\n",
    "    else:\n",
    "        indices = range(split_idx, split_idx + valid_samples)\n",
    "    \n",
    "    sub_dataset = Subset(dataset, indices)\n",
    "    print(f\"{'Training' if is_train else 'Validation'} subset length: {len(sub_dataset)}\")\n",
    "    return DataLoader(dataset=sub_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "class MyGeometricDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "    \n",
    "def collate_fn(data_list):\n",
    "    return Batch.from_data_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your data objects\n",
    "datalist = []\n",
    "counter = 0\n",
    "linegraph_transformation = LineGraph()\n",
    "\n",
    "for key, df in results_dict.items():\n",
    "    counter += 1\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "        gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "        \n",
    "        # Create dictionaries for nodes and edges\n",
    "        nodes = pd.concat([gdf['from_node'], gdf['to_node']]).unique()\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        \n",
    "        gdf['from_idx'] = gdf['from_node'].map(node_to_idx)\n",
    "        gdf['to_idx'] = gdf['to_node'].map(node_to_idx)\n",
    "        \n",
    "        edges = gdf[['from_idx', 'to_idx']].values\n",
    "        edge_car_volumes = gdf['vol_car'].values\n",
    "        capacities = gdf['capacity'].values\n",
    "        edge_positions = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                    (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                   for geom in gdf.geometry])\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_positions_tensor = torch.tensor(edge_positions, dtype=torch.float)\n",
    "        x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "        \n",
    "        # Create Data object\n",
    "        target_values = torch.tensor(edge_car_volumes, dtype=torch.float).unsqueeze(1)\n",
    "        data = Data(edge_index=edge_index, x=x, pos=edge_positions_tensor)\n",
    "        \n",
    "        # Transform to line graph\n",
    "        linegraph_data = linegraph_transformation(data)\n",
    "        \n",
    "        # Prepare the x for line graph: index and capacity\n",
    "        linegraph_x = torch.tensor(capacities, dtype=torch.float).unsqueeze(1)\n",
    "        linegraph_data.x = linegraph_x\n",
    "        \n",
    "        # Target tensor for car volumes\n",
    "        linegraph_data.y = target_values\n",
    "        \n",
    "        if linegraph_data.validate(raise_on_error=True):\n",
    "            datalist.append(linegraph_data)\n",
    "        else:\n",
    "            print(\"Invalid line graph data\")\n",
    "            \n",
    "dataset = MyGeometricDataset(datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1382"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:ag38mzjw) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "799822e2cba641d8ad76ae73866d080e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▇▆▆▅▅▄▄▄▃▃▄▂▂▂▂▃▃▂▂▂▂▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train_loss</td><td>215.16132</td></tr><tr><td>val_loss</td><td>1665498880.0</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dry-durian-6</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/gnn_4/runs/ag38mzjw' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_4/runs/ag38mzjw</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/gnn_4' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_4</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240702_082823-ag38mzjw/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:ag38mzjw). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fe4ebfc9c4949d182efca84ae3fdf6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011157198611181229, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240702_090418-no6j4hwg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/gnn_4/runs/no6j4hwg' target=\"_blank\">generous-music-7</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/gnn_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/gnn_4' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/gnn_4/runs/no6j4hwg' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_4/runs/no6j4hwg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "wandb.init(\n",
    "        project=wandb_name,\n",
    "        config={\n",
    "            \"epochs\": num_epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lr\": lr,\n",
    "            \"dropout\": random.uniform(0.01, 0.80),\n",
    "            })\n",
    "config = wandb.config\n",
    "\n",
    "model = GnnModel().to(device)\n",
    "\n",
    "# Define loss and optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "loss_fct = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset length: 1382\n",
      "Training subset length: 1100\n",
      "Total dataset length: 1382\n",
      "Validation subset length: 250\n"
     ]
    }
   ],
   "source": [
    "train_dl = create_dataloader(dataset=dataset, is_train=True, batch_size=config.batch_size)\n",
    "valid_dl = create_dataloader(dataset=dataset, is_train=False, batch_size=config.batch_size)\n",
    "n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, step: 0, loss: 92403.8203125\n",
      "epoch: 0, step: 1, loss: 81413.0\n",
      "epoch: 0, step: 2, loss: 70463.890625\n",
      "epoch: 0, step: 3, loss: 61922.67578125\n",
      "epoch: 0, step: 4, loss: 53736.08203125\n",
      "epoch: 0, step: 5, loss: 46164.12890625\n",
      "epoch: 0, step: 6, loss: 39773.41796875\n",
      "epoch: 0, step: 7, loss: 33934.484375\n",
      "epoch: 0, step: 8, loss: 29176.30078125\n",
      "epoch: 0, step: 9, loss: 24949.392578125\n",
      "epoch: 0, step: 10, loss: 21355.7265625\n",
      "epoch: 0, step: 11, loss: 18376.630859375\n",
      "epoch: 0, step: 12, loss: 15542.197265625\n",
      "epoch: 0, step: 13, loss: 13142.541015625\n",
      "epoch: 0, step: 14, loss: 11058.7919921875\n",
      "epoch: 0, step: 15, loss: 9408.4609375\n",
      "epoch: 0, step: 16, loss: 7937.03076171875\n",
      "epoch: 0, step: 17, loss: 6720.37255859375\n",
      "epoch: 0, step: 18, loss: 5666.1591796875\n",
      "epoch: 0, step: 19, loss: 4838.82861328125\n",
      "epoch: 0, step: 20, loss: 4096.30615234375\n",
      "epoch: 0, step: 21, loss: 3446.8291015625\n",
      "epoch: 0, val_loss: 1918638592.0\n",
      "epoch: 1, step: 0, loss: 2932.89794921875\n",
      "epoch: 1, step: 1, loss: 2502.73193359375\n",
      "epoch: 1, step: 2, loss: 2103.822021484375\n",
      "epoch: 1, step: 3, loss: 1805.11669921875\n",
      "epoch: 1, step: 4, loss: 1536.312255859375\n",
      "epoch: 1, step: 5, loss: 1316.390625\n",
      "epoch: 1, step: 6, loss: 1136.8616943359375\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(config.epochs):\n",
    "    model.train()\n",
    "    for step, data in enumerate(train_dl):\n",
    "        input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        predicted = model(data)\n",
    "        train_loss = loss_fct(predicted, targets)\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        wandb.log({\"train_loss\": train_loss.item()})\n",
    "        # print(f\"epoch: {epoch}, step: {step}, loss: {train_loss.item()}\")\n",
    "        \n",
    "    val_loss = validate_model(model, valid_dl, loss_fct, device)\n",
    "    wandb.log({\"val_loss\": val_loss})\n",
    "    print(f\"epoch: {epoch}, val_loss: {val_loss}\")\n",
    "        \n",
    "wandb.summary[\"val_loss\"] = val_loss\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     pred = model(data).cpu()\n",
    "#     target = data.y.view(-1, 1).cpu()\n",
    "#     mse = F.mse_loss(pred, target).item()\n",
    "#     rmse = torch.sqrt(torch.tensor(mse)).item()\n",
    "#     print(f'Mean Squared Error: {mse:.4f}')\n",
    "#     print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "\n",
    "# # Calculate target value statistics for comparison\n",
    "# target_values = target.numpy()\n",
    "# mean_target = target_values.mean()\n",
    "# std_target = target_values.std()\n",
    "# min_target = target_values.min()\n",
    "# max_target = target_values.max()\n",
    "\n",
    "# print(f'Mean of target values: {mean_target:.4f}')\n",
    "# print(f'Standard deviation of target values: {std_target:.4f}')\n",
    "# print(f'Minimum target value: {min_target:.4f}')\n",
    "# print(f'Maximum target value: {max_target:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
