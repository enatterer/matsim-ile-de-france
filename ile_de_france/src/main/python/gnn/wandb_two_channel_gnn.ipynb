{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import gnn_io\n",
    "\n",
    "from my_gnn import MyGNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy: policy introduced in Arrondissement(s) 5, 6\n",
      "Policy: policy introduced in Arrondissement(s) 1, 2, 3\n",
      "Policy: policy introduced in Arrondissement(s) 3, 4\n",
      "Policy: policy introduced in Arrondissement(s) 16, 17, 18\n",
      "Policy: policy introduced in Arrondissement(s) 13, 14, 15\n",
      "Policy: policy introduced in Arrondissement(s) 8, 9, 10, 11\n",
      "Policy: policy introduced in Arrondissement(s) 12, 13\n",
      "Policy: policy introduced in Arrondissement(s) 2, 3, 4, 5, 6, 7\n",
      "Policy: policy introduced in Arrondissement(s) 9, 10, 11\n",
      "Policy: policy introduced in Arrondissement(s) 15, 16, 17, 18, 19\n",
      "Policy: policy introduced in Arrondissement(s) 7, 8, 9, 10, 11, 12\n",
      "Policy: policy introduced in Arrondissement(s) 13, 14\n",
      "Policy: policy introduced in Arrondissement(s) 2, 3, 4, 5, 6\n",
      "Policy: policy introduced in Arrondissement(s) 5\n",
      "Policy: policy introduced in Arrondissement(s) 2\n",
      "Policy: policy introduced in Arrondissement(s) 11, 12, 13, 14\n",
      "Policy: policy introduced in Arrondissement(s) 12, 13, 14, 15\n",
      "Policy: policy introduced in Arrondissement(s) 14, 15, 16, 17, 18\n",
      "Policy: policy introduced in Arrondissement(s) 3, 4, 5, 6, 7\n",
      "Policy: policy introduced in Arrondissement(s) 17, 18, 19\n",
      "Policy: policy introduced in Arrondissement(s) 14, 15, 16, 17\n",
      "Policy: policy introduced in Arrondissement(s) 4, 5, 6, 7, 8\n",
      "Policy: policy introduced in Arrondissement(s) 3\n",
      "Policy: policy introduced in Arrondissement(s) 4\n",
      "Policy: policy introduced in Arrondissement(s) 8, 9\n",
      "Policy: policy introduced in Arrondissement(s) 9, 10, 11, 12\n",
      "Policy: policy introduced in Arrondissement(s) 1, 2, 3, 4, 5\n",
      "Policy: policy introduced in Arrondissement(s) 10, 11, 12, 13, 14, 15\n",
      "Policy: policy introduced in Arrondissement(s) 4, 5, 6, 7\n",
      "Policy: policy introduced in Arrondissement(s) 10, 11, 12, 13\n",
      "Policy: policy introduced in Arrondissement(s) 8, 9, 10\n",
      "Policy: policy introduced in Arrondissement(s) 5, 6, 7, 8, 9, 10\n",
      "Policy: policy introduced in Arrondissement(s) 4, 5\n",
      "Policy: policy introduced in Arrondissement(s) 15, 16, 17, 18\n",
      "Policy: policy introduced in Arrondissement(s) 6, 7, 8, 9\n",
      "Policy: policy introduced in Arrondissement(s) 7, 8, 9, 10\n",
      "Policy: policy introduced in Arrondissement(s) 17, 18, 19, 20\n",
      "Policy: policy introduced in Arrondissement(s) 10, 11\n",
      "Policy: policy introduced in Arrondissement(s) 1, 2, 3, 4\n",
      "Policy: policy introduced in Arrondissement(s) 3, 4, 5, 6, 7, 8\n",
      "Policy: policy introduced in Arrondissement(s) 13, 14, 15, 16, 17, 18\n",
      "Policy: policy introduced in Arrondissement(s) 2, 3, 4, 5\n",
      "Policy: policy introduced in Arrondissement(s) 6, 7\n",
      "Policy: policy introduced in Arrondissement(s) 11, 12, 13, 14, 15\n",
      "Policy: policy introduced in Arrondissement(s) 14, 15, 16, 17, 18, 19\n",
      "Policy: policy introduced in Arrondissement(s) 6, 7, 8\n",
      "Policy: policy introduced in Arrondissement(s) 13\n",
      "Policy: policy introduced in Arrondissement(s) 3, 4, 5, 6\n",
      "Policy: policy introduced in Arrondissement(s) 14\n",
      "Policy: policy introduced in Arrondissement(s) 15, 16\n",
      "Policy: policy introduced in Arrondissement(s) 10, 11, 12\n",
      "Policy: policy introduced in Arrondissement(s) 9, 10, 11, 12, 13\n",
      "Policy: policy introduced in Arrondissement(s) 15, 16, 17\n",
      "Policy: policy introduced in Arrondissement(s) 15\n",
      "Policy: policy introduced in Arrondissement(s) 12\n",
      "Policy: policy introduced in Arrondissement(s) 5, 6, 7, 8, 9\n",
      "Policy: policy introduced in Arrondissement(s) 7, 8\n",
      "Policy: policy introduced in Arrondissement(s) 4, 5, 6, 7, 8, 9\n",
      "Policy: policy introduced in Arrondissement(s) 16, 17\n",
      "Policy: policy introduced in Arrondissement(s) 3, 4, 5\n",
      "Policy: policy introduced in Arrondissement(s) 1, 2\n",
      "Policy: policy introduced in Arrondissement(s) 7, 8, 9\n",
      "Policy: policy introduced in Arrondissement(s) 12, 13, 14\n",
      "Policy: policy introduced in Arrondissement(s) 5, 6, 7\n",
      "Policy: policy introduced in Arrondissement(s) 6\n",
      "Policy: policy introduced in Arrondissement(s) 18, 19, 20\n",
      "Policy: policy introduced in Arrondissement(s) 8\n",
      "Policy: policy introduced in Arrondissement(s) 6, 7, 8, 9, 10\n",
      "Policy: policy introduced in Arrondissement(s) 17, 18\n",
      "Policy: policy introduced in Arrondissement(s) 13, 14, 15, 16, 17\n",
      "Policy: policy introduced in Arrondissement(s) 9\n",
      "Policy: policy introduced in Arrondissement(s) 8, 9, 10, 11, 12\n",
      "Policy: policy introduced in Arrondissement(s) 7\n",
      "Policy: policy introduced in Arrondissement(s) 15, 16, 17, 18, 19, 20\n",
      "Policy: policy introduced in Arrondissement(s) 6, 7, 8, 9, 10, 11\n",
      "Policy: policy introduced in Arrondissement(s) 14, 15\n",
      "Policy: policy introduced in Arrondissement(s) 16, 17, 18, 19\n",
      "Policy: policy introduced in Arrondissement(s) 2, 3\n",
      "Policy: policy introduced in Arrondissement(s) 1, 2, 3, 4, 5, 6\n",
      "Policy: policy introduced in Arrondissement(s) 11, 12, 13, 14, 15, 16\n",
      "Policy: policy introduced in Arrondissement(s) 7, 8, 9, 10, 11\n",
      "Policy: policy introduced in Arrondissement(s) 9, 10\n",
      "Policy: policy introduced in Arrondissement(s) 12, 13, 14, 15, 16\n",
      "Policy: policy introduced in Arrondissement(s) 11, 12, 13\n",
      "Policy: policy introduced in Arrondissement(s) 14, 15, 16\n",
      "Policy: policy introduced in Arrondissement(s) 10, 11, 12, 13, 14\n",
      "Policy: policy introduced in Arrondissement(s) 5, 6, 7, 8\n",
      "Policy: policy introduced in Arrondissement(s) 2, 3, 4\n",
      "Policy: policy introduced in Arrondissement(s) 18, 19\n",
      "Policy: policy introduced in Arrondissement(s) 17\n",
      "Policy: policy introduced in Arrondissement(s) 10\n",
      "Policy: policy introduced in Arrondissement(s) 11, 12\n",
      "Policy: policy introduced in Arrondissement(s) 19\n",
      "Policy: policy introduced in Arrondissement(s) 8, 9, 10, 11, 12, 13\n",
      "Policy: policy introduced in Arrondissement(s) 20\n",
      "Policy: policy introduced in Arrondissement(s) 4, 5, 6\n",
      "Policy: policy introduced in Arrondissement(s) 18\n",
      "Policy: policy introduced in Arrondissement(s) 11\n",
      "Policy: policy introduced in Arrondissement(s) 16, 17, 18, 19, 20\n",
      "Policy: policy introduced in Arrondissement(s) 16\n",
      "Policy: policy introduced in Arrondissement(s) 13, 14, 15, 16\n",
      "Policy: policy introduced in Arrondissement(s) 19, 20\n",
      "Policy: policy introduced in Arrondissement(s) 9, 10, 11, 12, 13, 14\n",
      "Policy: policy introduced in Arrondissement(s) 12, 13, 14, 15, 16, 17\n",
      "Policy: base_network_no_policies\n"
     ]
    }
   ],
   "source": [
    "# Load the dictionary\n",
    "with open('../results/result_dic.pkl', 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "\n",
    "datasets = []\n",
    "for key, df in results_dict.items():\n",
    "    print(f\"Policy: {key}\")\n",
    "    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "        gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "        edge_index, car_volume_tensor, policy_tensor, nodes = gnn_io.create_edge_index_and_tensors(gdf)\n",
    "        datasets.append((policy_tensor, car_volume_tensor))\n",
    "    else:\n",
    "        print(f\"The value for key '{key}' is not a GeoDataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 31216])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = MyGNN(policy_input_dim=3, traffic_input_dim=1, hidden_dim=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asdfasf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_input_dim = 3  # Dimensionality of policy features: capacity, freespeed flow, modes\n",
    "traffic_input_dim = 1  # Dimensionality of traffic flow features\n",
    "hidden_dim = 32  # Dimensionality of hidden representations\n",
    "num_nodes = max(max(edge_index[0]), max(edge_index[1])) + 1  # Number of nodes in the graph\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def get_dataloader(is_train, batch_size, slice=5):\n",
    "    \"Get a training dataloader\"\n",
    "    # full_dataset = torchvision.datasets.MNIST(root=\".\", train=is_train, transform=T.ToTensor(), download=True)\n",
    "    # sub_dataset = torch.utils.data.Subset(full_dataset, indices=range(0, len(full_dataset), slice))\n",
    "    loader = torch.utils.data.DataLoader(dataset=datasets, \n",
    "                                         batch_size=batch_size, \n",
    "                                         shuffle=True if is_train else False, \n",
    "                                         pin_memory=True, num_workers=2)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def normalize(tensor):\n",
    "    mean = tensor.mean(dim=0, keepdim=True)\n",
    "    std = tensor.std(dim=0, keepdim=True) + 1e-6  # Add a small epsilon to avoid division by zero\n",
    "    return (tensor - mean) / std\n",
    "\n",
    "def validate_model(model, valid_dl, loss_func):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0 \n",
    "    with torch.inference_mode():\n",
    "        correct = 0\n",
    "        total = 0  # Add a total counter for accuracy calculation\n",
    "\n",
    "        for i, (policy_features, flow_targets) in enumerate(valid_dl):\n",
    "             # Normalize data and labels\n",
    "            policy_features = normalize(policy_features.float())  # Shape: [105, 31216, 3]\n",
    "            flow_targets = normalize(flow_targets.float().unsqueeze(2))  # Shape: [105, 31216, 1]\n",
    "\n",
    "            # Forward pass ➡\n",
    "            outputs = gnn(edge_index, policy_features, flow_targets)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_func(outputs, flow_targets)\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            val_loss += loss.item() * flow_targets.size(0)  # Sum up the batch loss scaled by the number of examples\n",
    "\n",
    "            # Compute accuracy and accumulate\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += flow_targets.size(0)  # Increment total by the number of labels\n",
    "            print(\"Flow targets shape: \"+ str(flow_targets.size(0)))\n",
    "            correct += (predicted == flow_targets.squeeze()).sum().item()  # Squeeze flow_targets to match predicted shape\n",
    "\n",
    "\n",
    "            # val_loss += loss_func(outputs, flow_targets).item() * flow_targets\n",
    "            \n",
    "            # # Compute accuracy and accumulate\n",
    "            # _, predicted = torch.max(outputs.data, 1)\n",
    "            # total += flow_targets.size(0)  # Increment total by the number of labels\n",
    "            # correct += (predicted == flow_targets.size(0)).sum().item()\n",
    "            \n",
    "    # return val_loss / len(valid_dl.dataset), correct / len(valid_dl.dataset)\n",
    "    return val_loss / total, correct / total  # Average loss over all examples, accuracy as correct predictions over total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240603_174741-81ri1hpl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/81ri1hpl' target=\"_blank\">trim-night-1</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/81ri1hpl' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/81ri1hpl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: -16221.8515625\n",
      "Loss: -17120.380859375\n",
      "Loss: -17906.666015625\n",
      "Train loss tensor(-16425.2617, grad_fn=<DivBackward1>)\n",
      "Valid loss -16918.226171875\n",
      "Accuracy 0.0\n",
      "epoch 1\n",
      "Loss: -16227.1611328125\n",
      "Loss: -17126.28515625\n",
      "Loss: -17912.80859375\n",
      "Train loss tensor(-16447.4375, grad_fn=<DivBackward1>)\n",
      "Valid loss -16923.940234375\n",
      "Accuracy 0.0\n",
      "epoch 2\n",
      "Loss: -16232.630859375\n",
      "Loss: -17132.19140625\n",
      "Loss: -17918.953125\n",
      "Train loss tensor(-17948.2383, grad_fn=<DivBackward1>)\n",
      "Valid loss -16929.71953125\n",
      "Accuracy 0.0\n",
      "epoch 3\n",
      "Loss: -16238.3037109375\n",
      "Loss: -17137.5234375\n",
      "Loss: -17924.71484375\n",
      "Train loss tensor(-17598.5723, grad_fn=<DivBackward1>)\n",
      "Valid loss -16935.273828125\n",
      "Accuracy 0.0\n",
      "epoch 4\n",
      "Loss: -16243.6728515625\n",
      "Loss: -17143.80859375\n",
      "Loss: -17931.0\n",
      "Train loss tensor(-18256.5469, grad_fn=<DivBackward1>)\n",
      "Valid loss -16941.192578125\n",
      "Accuracy 0.0\n",
      "epoch 5\n",
      "Loss: -16249.2734375\n",
      "Loss: -17149.5234375\n",
      "Loss: -17937.046875\n",
      "Train loss tensor(-16592.5723, grad_fn=<DivBackward1>)\n",
      "Valid loss -16946.928125\n",
      "Accuracy 0.0\n",
      "epoch 6\n",
      "Loss: -16254.857421875\n",
      "Loss: -17155.23828125\n",
      "Loss: -17943.046875\n",
      "Train loss tensor(-17943.1309, grad_fn=<DivBackward1>)\n",
      "Valid loss -16952.64765625\n",
      "Accuracy 0.0\n",
      "epoch 7\n",
      "Loss: -16260.3271484375\n",
      "Loss: -17161.333984375\n",
      "Loss: -17949.28515625\n",
      "Train loss tensor(-16955., grad_fn=<DivBackward1>)\n",
      "Valid loss -16958.521484375\n",
      "Accuracy 0.0\n",
      "epoch 8\n",
      "Loss: -16265.98828125\n",
      "Loss: -17167.046875\n",
      "Loss: -17955.619140625\n",
      "Train loss tensor(-16492.1914, grad_fn=<DivBackward1>)\n",
      "Valid loss -16964.337890625\n",
      "Accuracy 0.0\n",
      "epoch 9\n",
      "Loss: -16271.380859375\n",
      "Loss: -17172.572265625\n",
      "Loss: -17961.904296875\n",
      "Train loss tensor(-17412.2617, grad_fn=<DivBackward1>)\n",
      "Valid loss -16969.962109375\n",
      "Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec4f54945c94696a2adf2a3311d8e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>▃▄▅▆█▄▄▃▅▂▇▅█▅▄▅▄▅▄▇▅▆▄▂▃▅▆▆▆▅▆▁▇▃▃▃▅▄▄▃</td></tr><tr><td>val/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>█▇▆▆▅▄▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>1050</td></tr><tr><td>train/train_loss</td><td>-17412.26172</td></tr><tr><td>val/val_accuracy</td><td>0.0</td></tr><tr><td>val/val_loss</td><td>-16969.96211</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trim-night-1</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/81ri1hpl' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/81ri1hpl</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_174741-81ri1hpl/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240603_174824-ionba7qt</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/ionba7qt' target=\"_blank\">volcanic-donkey-2</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/ionba7qt' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/ionba7qt</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: -16326.63671875\n",
      "Loss: -17231.046875\n",
      "Loss: -18022.953125\n",
      "Train loss tensor(-16146.9766, grad_fn=<DivBackward1>)\n",
      "Valid loss -17027.6640625\n",
      "Accuracy 0.0\n",
      "epoch 1\n",
      "Loss: -16381.8095703125\n",
      "Loss: -17289.333984375\n",
      "Loss: -18084.4765625\n",
      "Train loss tensor(-17423.5000, grad_fn=<DivBackward1>)\n",
      "Valid loss -17085.352734375\n",
      "Accuracy 0.0\n",
      "epoch 2\n",
      "Loss: -16437.095703125\n",
      "Loss: -17347.427734375\n",
      "Loss: -18145.857421875\n",
      "Train loss tensor(-16843.6191, grad_fn=<DivBackward1>)\n",
      "Valid loss -17142.980859375\n",
      "Accuracy 0.0\n",
      "epoch 3\n",
      "Loss: -16492.345703125\n",
      "Loss: -17405.5234375\n",
      "Loss: -18207.23828125\n",
      "Train loss tensor(-14436.7617, grad_fn=<DivBackward1>)\n",
      "Valid loss -17200.5953125\n",
      "Accuracy 0.0\n",
      "epoch 4\n",
      "Loss: -16547.81640625\n",
      "Loss: -17464.19140625\n",
      "Loss: -18268.953125\n",
      "Train loss tensor(-15856.1309, grad_fn=<DivBackward1>)\n",
      "Valid loss -17258.59375\n",
      "Accuracy 0.0\n",
      "epoch 5\n",
      "Loss: -16603.625\n",
      "Loss: -17523.23828125\n",
      "Loss: -18330.953125\n",
      "Train loss tensor(-18004.7617, grad_fn=<DivBackward1>)\n",
      "Valid loss -17316.9359375\n",
      "Accuracy 0.0\n",
      "epoch 6\n",
      "Loss: -16659.68359375\n",
      "Loss: -17582.095703125\n",
      "Loss: -18393.19140625\n",
      "Train loss tensor(-18645.1914, grad_fn=<DivBackward1>)\n",
      "Valid loss -17375.35\n",
      "Accuracy 0.0\n",
      "epoch 7\n",
      "Loss: -16716.046875\n",
      "Loss: -17641.5234375\n",
      "Loss: -18456.095703125\n",
      "Train loss tensor(-18205.2852, grad_fn=<DivBackward1>)\n",
      "Valid loss -17434.247265625\n",
      "Accuracy 0.0\n",
      "epoch 8\n",
      "Loss: -16772.7734375\n",
      "Loss: -17701.904296875\n",
      "Loss: -18519.142578125\n",
      "Train loss tensor(-18463.4648, grad_fn=<DivBackward1>)\n",
      "Valid loss -17493.699609375\n",
      "Accuracy 0.0\n",
      "epoch 9\n",
      "Loss: -16829.76171875\n",
      "Loss: -17761.5234375\n",
      "Loss: -18582.666015625\n",
      "Train loss tensor(-17054.3809, grad_fn=<DivBackward1>)\n",
      "Valid loss -17553.047265625\n",
      "Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0e72348202d41a0863c80d7496d227e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>▄▃▄▇▃▅▄▆▅█▁▆█▃▄▃▅▇▁▃▂▅▇▄▆▅▄▃▆▂▅▃▂▄▆▅▄▄▂▄</td></tr><tr><td>val/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>█▇▆▆▅▄▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>1050</td></tr><tr><td>train/train_loss</td><td>-17054.38086</td></tr><tr><td>val/val_accuracy</td><td>0.0</td></tr><tr><td>val/val_loss</td><td>-17553.04727</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">volcanic-donkey-2</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/ionba7qt' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/ionba7qt</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_174824-ionba7qt/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "662d5f6afabd432da626612b90e693a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167917132843286, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240603_174905-1m3joy89</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/1m3joy89' target=\"_blank\">drawn-galaxy-3</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/1m3joy89' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/1m3joy89</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: -17401.78515625\n",
      "Loss: -18364.19140625\n",
      "Loss: -19218.4765625\n",
      "Train loss tensor(-19154.8574, grad_fn=<DivBackward1>)\n",
      "Valid loss -18150.0859375\n",
      "Accuracy 0.0\n",
      "epoch 1\n",
      "Loss: -17987.107421875\n",
      "Loss: -18981.333984375\n",
      "Loss: -19868.857421875\n",
      "Train loss tensor(-17171.6191, grad_fn=<DivBackward1>)\n",
      "Valid loss -18761.148046875\n",
      "Accuracy 0.0\n",
      "epoch 2\n",
      "Loss: -18581.6015625\n",
      "Loss: -19608.0\n",
      "Loss: -20528.857421875\n",
      "Train loss tensor(-21471.2383, grad_fn=<DivBackward1>)\n",
      "Valid loss -19381.612109375\n",
      "Accuracy 0.0\n",
      "epoch 3\n",
      "Loss: -19196.642578125\n",
      "Loss: -20256.380859375\n",
      "Loss: -21212.28515625\n",
      "Train loss tensor(-19828.4766, grad_fn=<DivBackward1>)\n",
      "Valid loss -20023.66640625\n",
      "Accuracy 0.0\n",
      "epoch 4\n",
      "Loss: -19825.572265625\n",
      "Loss: -20919.80859375\n",
      "Loss: -21910.953125\n",
      "Train loss tensor(-20403.8223, grad_fn=<DivBackward1>)\n",
      "Valid loss -20680.34296875\n",
      "Accuracy 0.0\n",
      "epoch 5\n",
      "Loss: -20471.322265625\n",
      "Loss: -21600.19140625\n",
      "Loss: -22627.427734375\n",
      "Train loss tensor(-23131.0469, grad_fn=<DivBackward1>)\n",
      "Valid loss -21354.091015625\n",
      "Accuracy 0.0\n",
      "epoch 6\n",
      "Loss: -21140.572265625\n",
      "Loss: -22305.904296875\n",
      "Loss: -23371.046875\n",
      "Train loss tensor(-21992.7383, grad_fn=<DivBackward1>)\n",
      "Valid loss -22052.8\n",
      "Accuracy 0.0\n",
      "epoch 7\n",
      "Loss: -21827.53515625\n",
      "Loss: -23029.142578125\n",
      "Loss: -24134.0\n",
      "Train loss tensor(-21501.4277, grad_fn=<DivBackward1>)\n",
      "Valid loss -22769.47109375\n",
      "Accuracy 0.0\n",
      "epoch 8\n",
      "Loss: -22534.046875\n",
      "Loss: -23774.095703125\n",
      "Loss: -24919.046875\n",
      "Train loss tensor(-22145.2910, grad_fn=<DivBackward1>)\n",
      "Valid loss -23507.06640625\n",
      "Accuracy 0.0\n",
      "epoch 9\n",
      "Loss: -23262.369140625\n",
      "Loss: -24541.904296875\n",
      "Loss: -25728.095703125\n",
      "Train loss tensor(-23823.5723, grad_fn=<DivBackward1>)\n",
      "Valid loss -24267.328515625\n",
      "Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a365c7d0cc0240338d9edf2a3ec53b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.008 MB uploaded\\r'), FloatProgress(value=0.12314667320181351, max=1.…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>▇▇▇▆▆▆▆█▇▆▅▆▆▅▅▆▇▅▅▄▅▆▅▄▃▄▄▅▄▄▃▃▃▅▁▃▃▃▂▂</td></tr><tr><td>val/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>█▇▇▆▅▄▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>1050</td></tr><tr><td>train/train_loss</td><td>-23823.57227</td></tr><tr><td>val/val_accuracy</td><td>0.0</td></tr><tr><td>val/val_loss</td><td>-24267.32852</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">drawn-galaxy-3</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/1m3joy89' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/1m3joy89</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_174905-1m3joy89/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2150e3b49b48bfb9442c2587c59cc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167917132843286, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240603_174945-nywfg3a9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/nywfg3a9' target=\"_blank\">fearless-resonance-4</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/nywfg3a9' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/nywfg3a9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: -30896.69140625\n",
      "Loss: -32597.333984375\n",
      "Loss: -34256.3828125\n",
      "Train loss tensor(-29238.8926, grad_fn=<DivBackward1>)\n",
      "Valid loss -32248.88671875\n",
      "Accuracy 0.0\n",
      "epoch 1\n",
      "Loss: -40151.2734375\n",
      "Loss: -42356.953125\n",
      "Loss: -44633.90625\n",
      "Train loss tensor(-39994.0703, grad_fn=<DivBackward1>)\n",
      "Valid loss -41930.071875\n",
      "Accuracy 0.0\n",
      "epoch 2\n",
      "Loss: -51389.40625\n",
      "Loss: -54196.5703125\n",
      "Loss: -57262.85546875\n",
      "Train loss tensor(-55694.4766, grad_fn=<DivBackward1>)\n",
      "Valid loss -53686.96171875\n",
      "Accuracy 0.0\n",
      "epoch 3\n",
      "Loss: -65020.90625\n",
      "Loss: -68548.5703125\n",
      "Loss: -72620.5703125\n",
      "Train loss tensor(-64720.7617, grad_fn=<DivBackward1>)\n",
      "Valid loss -67951.9046875\n",
      "Accuracy 0.0\n",
      "epoch 4\n",
      "Loss: -81566.0\n",
      "Loss: -85970.2890625\n",
      "Loss: -91312.0\n",
      "Train loss tensor(-81607.1406, grad_fn=<DivBackward1>)\n",
      "Valid loss -85276.915625\n",
      "Accuracy 0.0\n",
      "epoch 5\n",
      "Loss: -101401.5703125\n",
      "Loss: -106857.140625\n",
      "Loss: -113756.5703125\n",
      "Train loss tensor(-101962.6641, grad_fn=<DivBackward1>)\n",
      "Valid loss -106054.7984375\n",
      "Accuracy 0.0\n",
      "epoch 6\n",
      "Loss: -125312.5703125\n",
      "Loss: -132027.421875\n",
      "Loss: -140854.859375\n",
      "Train loss tensor(-135133.7188, grad_fn=<DivBackward1>)\n",
      "Valid loss -131106.96875\n",
      "Accuracy 0.0\n",
      "epoch 7\n",
      "Loss: -153925.71875\n",
      "Loss: -162147.046875\n",
      "Loss: -173334.859375\n",
      "Train loss tensor(-168947.1406, grad_fn=<DivBackward1>)\n",
      "Valid loss -161096.078125\n",
      "Accuracy 0.0\n",
      "epoch 8\n",
      "Loss: -187957.53125\n",
      "Loss: -197970.28125\n",
      "Loss: -212000.0\n",
      "Train loss tensor(-196489.1406, grad_fn=<DivBackward1>)\n",
      "Valid loss -196771.125\n",
      "Accuracy 0.0\n",
      "epoch 9\n",
      "Loss: -228435.421875\n",
      "Loss: -240576.0\n",
      "Loss: -258046.46875\n",
      "Train loss tensor(-240214.0938, grad_fn=<DivBackward1>)\n",
      "Valid loss -239213.8625\n",
      "Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b575b887bfc945e0b88c3a514cf1aa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▆▆▅▅▅▅▅▄▅▄▄▄▃▂▃▂▂▁</td></tr><tr><td>val/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>██▇▇▆▆▅▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>1050</td></tr><tr><td>train/train_loss</td><td>-240214.09375</td></tr><tr><td>val/val_accuracy</td><td>0.0</td></tr><tr><td>val/val_loss</td><td>-239213.8625</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fearless-resonance-4</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/nywfg3a9' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/nywfg3a9</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_174945-nywfg3a9/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ae40e8609145eaa1a8cef04bf49f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011160082411434916, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240603_175026-n9c1skjj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/n9c1skjj' target=\"_blank\">dashing-frog-5</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/n9c1skjj' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/n9c1skjj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "Loss: -719868.9375\n",
      "Loss: -757686.875\n",
      "Loss: -817194.6875\n",
      "Train loss tensor(-638698.6875, grad_fn=<DivBackward1>)\n",
      "Valid loss -754461.2625\n",
      "Accuracy 0.0\n",
      "epoch 1\n",
      "Loss: -1674206.5\n",
      "Loss: -1761426.25\n",
      "Loss: -1905712.75\n",
      "Train loss tensor(-1623500.2500, grad_fn=<DivBackward1>)\n",
      "Valid loss -1755395.65\n",
      "Accuracy 0.0\n",
      "epoch 2\n",
      "Loss: -3314176.0\n",
      "Loss: -3485208.5\n",
      "Loss: -3778023.5\n",
      "Train loss tensor(-2850621., grad_fn=<DivBackward1>)\n",
      "Valid loss -3475358.5\n",
      "Accuracy 0.0\n",
      "epoch 3\n",
      "Loss: -5938578.5\n",
      "Loss: -6243474.5\n",
      "Loss: -6775954.5\n",
      "Train loss tensor(-5307538.5000, grad_fn=<DivBackward1>)\n",
      "Valid loss -6228012.1\n",
      "Accuracy 0.0\n",
      "epoch 4\n",
      "Loss: -9925723.0\n",
      "Loss: -10434658.0\n",
      "Loss: -11333925.0\n",
      "Train loss tensor(-9182927., grad_fn=<DivBackward1>)\n",
      "Valid loss -10410937.4\n",
      "Accuracy 0.0\n",
      "epoch 5\n",
      "Loss: -15737100.0\n",
      "Loss: -16542769.0\n",
      "Loss: -17981244.0\n",
      "Train loss tensor(-14258664., grad_fn=<DivBackward1>)\n",
      "Valid loss -16508196.4\n",
      "Accuracy 0.0\n",
      "epoch 6\n",
      "Loss: -23909742.0\n",
      "Loss: -25133446.0\n",
      "Loss: -27339434.0\n",
      "Train loss tensor(-25929924., grad_fn=<DivBackward1>)\n",
      "Valid loss -25085162.0\n",
      "Accuracy 0.0\n",
      "epoch 7\n",
      "Loss: -35141728.0\n",
      "Loss: -36939680.0\n",
      "Loss: -40199704.0\n",
      "Train loss tensor(-28749044., grad_fn=<DivBackward1>)\n",
      "Valid loss -36872504.0\n",
      "Accuracy 0.0\n",
      "epoch 8\n",
      "Loss: -50026936.0\n",
      "Loss: -52585620.0\n",
      "Loss: -57237112.0\n",
      "Train loss tensor(-41609120., grad_fn=<DivBackward1>)\n",
      "Valid loss -52492444.8\n",
      "Accuracy 0.0\n",
      "epoch 9\n",
      "Loss: -69450360.0\n",
      "Loss: -72999304.0\n",
      "Loss: -79456936.0\n",
      "Train loss tensor(-69997520., grad_fn=<DivBackward1>)\n",
      "Valid loss -72871252.8\n",
      "Accuracy 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d29332dbbad4c7cb5289527abe8e612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/example_ct</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/train_loss</td><td>████████████████▇▇▇▇▇▇▇▇▆▆▆▆▅▅▅▅▅▄▃▃▃▂▂▁</td></tr><tr><td>val/val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/val_loss</td><td>███▇▇▆▆▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test_accuracy</td><td>0.8</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/example_ct</td><td>1050</td></tr><tr><td>train/train_loss</td><td>-69997520.0</td></tr><tr><td>val/val_accuracy</td><td>0.0</td></tr><tr><td>val/val_loss</td><td>-72871252.8</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dashing-frog-5</strong> at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/n9c1skjj' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates/runs/n9c1skjj</a><br/> View project at: <a href='https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/my_project_diff_learning_rates</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240603_175026-n9c1skjj/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Launch 5 experiments, trying different dropout rates\n",
    "\n",
    "learning_rates = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1]  # List of learning rates to iterate over\n",
    "\n",
    "for lr in learning_rates:\n",
    "    # 🐝 initialise a wandb run\n",
    "    wandb.init(\n",
    "        project=\"my_project_diff_learning_rates\",\n",
    "        config={\n",
    "            \"epochs\": 10,\n",
    "            \"batch_size\": 21,\n",
    "            \"lr\": lr\n",
    "    })\n",
    "    \n",
    "    # Copy your config \n",
    "    config = wandb.config\n",
    "\n",
    "    # Get the data\n",
    "    train_dl = get_dataloader(is_train=True, batch_size=config.batch_size)\n",
    "    valid_dl = get_dataloader(is_train=False, batch_size=2*config.batch_size)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "    \n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(gnn.parameters(), lr=config.lr)\n",
    "\n",
    "   # Training\n",
    "    example_ct = 0\n",
    "    step_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        print(\"epoch\", epoch)\n",
    "        gnn.train()\n",
    "        for step, (policy_features, flow_targets) in enumerate(train_dl):            \n",
    "            data, labels = normalize(policy_features.float()), normalize(flow_targets.float().unsqueeze(2))        \n",
    "            outputs = gnn(edge_index, data, labels)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            example_ct += len(data)\n",
    "            metrics = {\"train/train_loss\": train_loss, \n",
    "                       \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "                       \"train/example_ct\": example_ct}\n",
    "            if step + 1 < n_steps_per_epoch:\n",
    "                # 🐝 Log train metrics to wandb \n",
    "                wandb.log(metrics)\n",
    "                \n",
    "            step_ct += 1\n",
    "\n",
    "        val_loss, accuracy = validate_model(gnn, valid_dl, loss_func)\n",
    "\n",
    "        # 🐝 Log train and validation metrics to wandb\n",
    "        val_metrics = {\"val/val_loss\": val_loss, \n",
    "                       \"val/val_accuracy\": accuracy}\n",
    "        wandb.log({**metrics, **val_metrics})\n",
    "        print(\"Train loss\", train_loss)\n",
    "        print(\"Valid loss\", val_loss)\n",
    "        print(\"Accuracy\", accuracy)\n",
    "        # print(f\"Train Loss: {train_loss:.3f}, Valid Loss: {val_loss:3f}, Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "    # If you had a test set, this is how you could log it as a Summary metric\n",
    "    wandb.summary['test_accuracy'] = 0.8\n",
    "\n",
    "    # 🐝 Close your wandb run \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
