{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import math\n",
    "import random\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import gnn_io\n",
    "\n",
    "from my_gnn import GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "This is the new version of the the notebook \"wandb_two_channel_gnn\". \n",
    "\n",
    "What we do in here:\n",
    "\n",
    "1. Load wandb and set reasonable parameters.\n",
    "2. Introduce key parameters: a. Car Volume in area where there was change, b. Car volume overall, c. Car volume per Edge.\n",
    "3. Make predictions for these parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dictionary\n",
    "with open('../results/result_dic.pkl', 'rb') as f:\n",
    "    results_dict = pickle.load(f)\n",
    "\n",
    "datasets = []\n",
    "for key, df in results_dict.items():    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "        gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "        edge_index, car_volume_tensor, policy_tensor, nodes = gnn_io.create_edge_index_and_tensors(gdf)\n",
    "        datasets.append((policy_tensor, car_volume_tensor))\n",
    "    else:\n",
    "        print(f\"The value for key '{key}' is not a GeoDataFrame.\")\n",
    "        \n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_value_total = 0\n",
    "# mean_value_total = 0\n",
    "\n",
    "# for policy_tensor, car_volume_tensor in datasets:\n",
    "#     print(policy_tensor[:1])\n",
    "#     print(car_volume_tensor[:1])\n",
    "#     # Find the maximum and mean value of the car_volume_tensor\n",
    "#     max_value = torch.max(car_volume_tensor)\n",
    "#     mean_value = torch.mean(car_volume_tensor)\n",
    "    \n",
    "#     print(f\"Max value of car_volume_tensor: {max_value.item()}\")\n",
    "#     print(f\"Mean value of car_volume_tensor: {mean_value.item()}\")\n",
    "#     max_value_total = max(max_value_total, max_value)\n",
    "#     mean_value_total = (mean_value_total + mean_value)/2\n",
    "    \n",
    "# print(max_value_total)\n",
    "# print(mean_value_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, valid_dl, loss_func):\n",
    "    \"Compute performance of the model on the validation dataset and log a wandb.Table\"\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total = 0 \n",
    "    with torch.inference_mode():\n",
    "        total = 0  # Add a total counter for accuracy calculation\n",
    "        for i, (policy_features, flow_targets) in enumerate(valid_dl):\n",
    "             # Normalize data and labels\n",
    "            policy_features = gnn_io.min_max_normalize(policy_features.float())  # Shape: [105, 31216, 3]\n",
    "            flow_targets = gnn_io.min_max_normalize(flow_targets.float().unsqueeze(2)) # Shape: [105, 31216, 1]\n",
    "\n",
    "            # Forward pass ‚û°\n",
    "            outputs = model(edge_index, policy_features, flow_targets)\n",
    "            # print(\"flow targets: \", flow_targets[:1])\n",
    "            # print(\"outputs: \", outputs[:1])\n",
    "            # Compute loss\n",
    "            loss = loss_func(outputs, flow_targets)\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "            val_loss += loss.item() * flow_targets.size(0)  # Sum up the batch loss scaled by the number of examples\n",
    "            print(f\"Val loss: {val_loss}\")\n",
    "            total += flow_targets.size(0)  # Increment total by the number of labels\n",
    "            \n",
    "    average_val_loss = val_loss/total\n",
    "    return average_val_loss  # Average loss over all examples, accuracy as correct predictions over total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyGNN()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_input_dim = 3  # Dimensionality of policy features: capacity, freeflow speed, modes\n",
    "traffic_input_dim = 1  # Dimensionality of traffic flow features\n",
    "hidden_dim = 32  # Dimensionality of hidden representations\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "gnn = GNN(policy_input_dim=3, traffic_input_dim=1, hidden_dim=8)\n",
    "gnn.apply(gnn_io.init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenanatterer/Development/MATSim/eqasim-java/ile_de_france/src/main/python/gnn/wandb/run-20240604_170609-u3fkyvmm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tum-traffic-engineering/gnn_version_3/runs/u3fkyvmm' target=\"_blank\">morning-river-3</a></strong> to <a href='https://wandb.ai/tum-traffic-engineering/gnn_version_3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tum-traffic-engineering/gnn_version_3' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_version_3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tum-traffic-engineering/gnn_version_3/runs/u3fkyvmm' target=\"_blank\">https://wandb.ai/tum-traffic-engineering/gnn_version_3/runs/u3fkyvmm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted car volume \n",
      "tensor([[[-0.0005],\n",
      "         [-0.0454],\n",
      "         [-0.1421],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], grad_fn=<SliceBackward0>)\n",
      "tensor([[[-0.0005],\n",
      "         [-0.0454],\n",
      "         [-0.1421],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.0005],\n",
      "         [-0.0848],\n",
      "         [-0.1737],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.0003],\n",
      "         [-0.0293],\n",
      "         [-0.1259],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0002],\n",
      "         [-0.1243],\n",
      "         [-0.1263],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.0003],\n",
      "         [-0.0214],\n",
      "         [-0.0996],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.0003],\n",
      "         [-0.0214],\n",
      "         [-0.0996],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], grad_fn=<ViewBackward0>)\n",
      "torch.Size([20, 31216, 1])\n"
     ]
    }
   ],
   "source": [
    "# Launch 5 experiments, trying different dropout rates\n",
    "\n",
    "learning_rates = [1e-5]  # List of learning rates to iterate over\n",
    "# ,  1e-4, 1e-3, 1e-2, 1e-1\n",
    "for lr in learning_rates:\n",
    "    # üêù initialise a wandb run\n",
    "    wandb.init(\n",
    "        project=\"gnn_version_4\",\n",
    "        config={\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 20,\n",
    "            \"lr\": lr\n",
    "    })\n",
    "    \n",
    "    # Copy your config \n",
    "    config = wandb.config\n",
    "\n",
    "    # Get the data\n",
    "    train_dl = gnn_io.get_dataloader(is_train=True, batch_size=config.batch_size, datasets=datasets)\n",
    "    valid_dl = gnn_io.get_dataloader(is_train=False, batch_size=config.batch_size, datasets=datasets)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "    \n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(gnn.parameters(), lr=config.lr)\n",
    "\n",
    "   # Training\n",
    "    example_ct = 0\n",
    "    step_ct = 0\n",
    "    for epoch in range(config.epochs):\n",
    "        gnn.train()\n",
    "        for step, (policy_features, car_volume) in enumerate(train_dl):            \n",
    "            policy_features_normalized, car_volume_normalized = gnn_io.min_max_normalize(policy_features.float()), gnn_io.min_max_normalize(car_volume.float().unsqueeze(2))        \n",
    "            predicted_car_volume = gnn(edge_index, policy_features_normalized)\n",
    "            # print(\"Policy features: \", policy_features_normalized[:1])\n",
    "            # print(\"Normalized car volume: \", car_volume_normalized[:1])\n",
    "            # print(\"Predicted car volume: \", predicted_car_volume[:1])\n",
    "            train_loss = loss_func(predicted_car_volume, car_volume_normalized)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "            example_ct += len(policy_features_normalized)\n",
    "            print(\"Predicted car volume \")\n",
    "            print(predicted_car_volume[:1])\n",
    "            print(predicted_car_volume)\n",
    "            print(predicted_car_volume.shape)\n",
    "            break\n",
    "        break\n",
    "    #         metrics = {\"train/train_loss\": train_loss, \n",
    "    #                    \"train/epoch\": (step + 1 + (n_steps_per_epoch * epoch)) / n_steps_per_epoch, \n",
    "    #                    \"train/example_ct\": example_ct}\n",
    "    #         if step + 1 < n_steps_per_epoch:\n",
    "    #             # üêù Log train metrics to wandb \n",
    "    #             wandb.log(metrics)\n",
    "    #         step_ct += 1\n",
    "\n",
    "    #     val_loss = validate_model(gnn, valid_dl, loss_func)\n",
    "    #     val_metrics = {\"val/val_loss\": val_loss}\n",
    "    #     wandb.log({**metrics, **val_metrics})\n",
    "    # wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0005],\n",
       "         [-0.0454],\n",
       "         [-0.1421],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]],\n",
       "\n",
       "        [[-0.0005],\n",
       "         [-0.0848],\n",
       "         [-0.1737],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]],\n",
       "\n",
       "        [[-0.0003],\n",
       "         [-0.0293],\n",
       "         [-0.1259],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0002],\n",
       "         [-0.1243],\n",
       "         [-0.1263],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]],\n",
       "\n",
       "        [[-0.0003],\n",
       "         [-0.0214],\n",
       "         [-0.0996],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]],\n",
       "\n",
       "        [[-0.0003],\n",
       "         [-0.0214],\n",
       "         [-0.0996],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_car_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the twenty entries (the first dimension of the tensor)\n",
    "def get_overall_car_volume(car_volume_normalized, predicted_car_volume, batch_size):\n",
    "    absolute_diff = 0\n",
    "    for i in range(batch_size):\n",
    "        print(\" \")\n",
    "        actual_sum = sum(car_volume_normalized[i])\n",
    "        print(actual_sum)\n",
    "        predicted_sum = sum(predicted_car_volume[i])\n",
    "        print(predicted_sum)\n",
    "        diff = abs(actual_sum - predicted_sum)\n",
    "        print(diff)\n",
    "        absolute_diff += diff\n",
    "    print(absolute_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "tensor([995.3181])\n",
      "tensor([-1766.6842], grad_fn=<AddBackward0>)\n",
      "tensor([2762.0022], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([821.2917])\n",
      "tensor([-1467.3787], grad_fn=<AddBackward0>)\n",
      "tensor([2288.6704], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([767.1666])\n",
      "tensor([-1366.2631], grad_fn=<AddBackward0>)\n",
      "tensor([2133.4297], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([835.9867])\n",
      "tensor([-1495.8457], grad_fn=<AddBackward0>)\n",
      "tensor([2331.8325], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([853.7370])\n",
      "tensor([-1529.9232], grad_fn=<AddBackward0>)\n",
      "tensor([2383.6602], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([967.0150])\n",
      "tensor([-1716.1787], grad_fn=<AddBackward0>)\n",
      "tensor([2683.1938], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([894.3200])\n",
      "tensor([-1582.3336], grad_fn=<AddBackward0>)\n",
      "tensor([2476.6536], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([498.1046])\n",
      "tensor([-918.9207], grad_fn=<AddBackward0>)\n",
      "tensor([1417.0254], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([674.7240])\n",
      "tensor([-1208.0469], grad_fn=<AddBackward0>)\n",
      "tensor([1882.7709], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([573.8107])\n",
      "tensor([-1022.6222], grad_fn=<AddBackward0>)\n",
      "tensor([1596.4329], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([957.5980])\n",
      "tensor([-1723.3000], grad_fn=<AddBackward0>)\n",
      "tensor([2680.8979], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([765.9271])\n",
      "tensor([-1366.2714], grad_fn=<AddBackward0>)\n",
      "tensor([2132.1985], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([936.2775])\n",
      "tensor([-1660.5573], grad_fn=<AddBackward0>)\n",
      "tensor([2596.8347], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([816.0409])\n",
      "tensor([-1453.3604], grad_fn=<AddBackward0>)\n",
      "tensor([2269.4014], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([677.5576])\n",
      "tensor([-1230.3330], grad_fn=<AddBackward0>)\n",
      "tensor([1907.8906], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([978.0274])\n",
      "tensor([-1735.2928], grad_fn=<AddBackward0>)\n",
      "tensor([2713.3203], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([930.1171])\n",
      "tensor([-1679.3965], grad_fn=<AddBackward0>)\n",
      "tensor([2609.5137], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([952.9778])\n",
      "tensor([-1696.1245], grad_fn=<AddBackward0>)\n",
      "tensor([2649.1023], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([847.4935])\n",
      "tensor([-1509.7683], grad_fn=<AddBackward0>)\n",
      "tensor([2357.2617], grad_fn=<AbsBackward0>)\n",
      " \n",
      "tensor([893.8057])\n",
      "tensor([-1586.1628], grad_fn=<AddBackward0>)\n",
      "tensor([2479.9685], grad_fn=<AbsBackward0>)\n",
      "tensor([46352.0664], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "get_overall_car_volume(car_volume_normalized, predicted_car_volume, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
