{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import tqdm\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from shapely.geometry import LineString"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "Here we generate the data, and in the notebook gnn_for_policy_traffic_prediction_2 we do the model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data and create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33menatterer\u001b[0m (\u001b[33mtum-traffic-engineering\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../results/results_pop_1pm_first_1400.pkl', 'rb') as f:\n",
    "    results_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class GnnModel(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         torch.manual_seed(12345)\n",
    "#         self.conv1 = torch_geometric.nn.GCNConv(1, 16)\n",
    "#         self.conv2 = torch_geometric.nn.GCNConv(16, 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x, edge_index = data.x, data.edge_index\n",
    "#         x = self.conv1(x, edge_index)\n",
    "#         x = F.relu(x)\n",
    "#         x = F.dropout(x, training=self.training)\n",
    "#         x = self.conv2(x, edge_index)\n",
    "#         return x\n",
    "\n",
    "# def validate_model(model, valid_dl, loss_func, device):\n",
    "#     model.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.inference_mode():\n",
    "#         for idx, data in enumerate(valid_dl):\n",
    "#             input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "#             predicted = model(data)\n",
    "#             # val_loss += loss_func(predicted, targets)*targets.size(0)\n",
    "#             val_loss += loss_func(predicted, targets)\n",
    "#     return val_loss \n",
    "\n",
    "\n",
    "        # input_node_features, targets = data.x.to(device), data.y.to(device)\n",
    "        # predicted = model(data)\n",
    "        # train_loss = loss_fct(predicted, targets)\n",
    "        # optimizer.zero_grad()\n",
    "        # train_loss.backward()\n",
    "        # optimizer.step()\n",
    "        # wandb.log({\"train_loss\": train_loss.item(), \"epoch\": epoch, \"step\": step})\n",
    "\n",
    "# def create_dataloader(is_train, batch_size, dataset):\n",
    "#     dataset_length = len(dataset)\n",
    "#     print(f\"Total dataset length: {dataset_length}\")\n",
    "\n",
    "#     # Calculate split index for training and validation\n",
    "#     split_idx = int(dataset_length * train_ratio)\n",
    "    \n",
    "#     # Calculate the maximum number of samples that fit into complete batches for training and validation\n",
    "#     train_samples = (split_idx // batch_size) * batch_size\n",
    "#     valid_samples = ((dataset_length - split_idx) // batch_size) * batch_size\n",
    "\n",
    "#     if is_train:\n",
    "#         indices = range(0, train_samples)\n",
    "#     else:\n",
    "#         indices = range(split_idx, split_idx + valid_samples)\n",
    "    \n",
    "#     sub_dataset = Subset(dataset, indices)\n",
    "#     print(f\"{'Training' if is_train else 'Validation'} subset length: {len(sub_dataset)}\")\n",
    "#     return DataLoader(dataset=sub_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "class MyGeometricDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data_list = data_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_list[idx]\n",
    "    \n",
    "def collate_fn(data_list):\n",
    "    return Batch.from_data_list(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data objects\n",
    "datalist = []\n",
    "counter = 0\n",
    "linegraph_transformation = LineGraph()\n",
    "\n",
    "for key, df in results_dict.items():\n",
    "    counter += 1\n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "        gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "        gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "        \n",
    "        # Create dictionaries for nodes and edges\n",
    "        nodes = pd.concat([gdf['from_node'], gdf['to_node']]).unique()\n",
    "        node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "        \n",
    "        gdf['from_idx'] = gdf['from_node'].map(node_to_idx)\n",
    "        gdf['to_idx'] = gdf['to_node'].map(node_to_idx)\n",
    "        \n",
    "        edges = gdf[['from_idx', 'to_idx']].values\n",
    "        edge_car_volumes = gdf['vol_car'].values\n",
    "        capacities = gdf['capacity'].values\n",
    "        edge_positions = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                    (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                   for geom in gdf.geometry])\n",
    "\n",
    "        # Convert lists to tensors\n",
    "        edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        edge_positions_tensor = torch.tensor(edge_positions, dtype=torch.float)\n",
    "        x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "        \n",
    "        # Create Data object\n",
    "        target_values = torch.tensor(edge_car_volumes, dtype=torch.float).unsqueeze(1)\n",
    "        data = Data(edge_index=edge_index, x=x, pos=edge_positions_tensor)\n",
    "        \n",
    "        # Transform to line graph\n",
    "        linegraph_data = linegraph_transformation(data)\n",
    "        \n",
    "        # Prepare the x for line graph: index and capacity\n",
    "        linegraph_x = torch.tensor(capacities, dtype=torch.float).unsqueeze(1)\n",
    "        linegraph_data.x = linegraph_x\n",
    "        \n",
    "        # Target tensor for car volumes\n",
    "        linegraph_data.y = target_values\n",
    "        \n",
    "        if linegraph_data.validate(raise_on_error=True):\n",
    "            datalist.append(linegraph_data)\n",
    "        else:\n",
    "            print(\"Invalid line graph data\")\n",
    "            \n",
    "# dataset = MyGeometricDataset(datalist)\n",
    "# Convert dataset to a list of dictionaries\n",
    "data_dict_list = [{'x': data.x, 'edge_index': data.edge_index, 'pos': data.pos, 'y': data.y} for data in datalist]\n",
    "\n",
    "# Save the list of dictionaries\n",
    "torch.save(data_dict_list, 'dataset_1pm_0-1382.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
