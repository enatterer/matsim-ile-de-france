{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "\n",
    "import processing_io as pio\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import shapely.wkt as wkt\n",
    "from tqdm import tqdm\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "import alphashape\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "result_df_name = 'sim_output_1pm_capacity_reduction_10k_PRELIMINARY'\n",
    "result_path = '../../../../data/datasets_simulation_outputs/' + result_df_name + '.pt'\n",
    "string_is_for_1pm = \"pop_1pm\"\n",
    "\n",
    "base_dir_sample_sim_input = '../../../../data/' + string_is_for_1pm + '_simulations/' + string_is_for_1pm + '_policies_combinations_with_normal_dist/'\n",
    "subdirs_pattern = os.path.join(base_dir_sample_sim_input, 'output_networks_*')\n",
    "subdirs = list(set(glob.glob(subdirs_pattern)))\n",
    "subdirs.sort()\n",
    "\n",
    "\n",
    "paris_inside_bvd_peripherique = \"../../../../data/paris_inside_bvd_per/referentiel-comptages-edit.shp\"\n",
    "gdf_paris_inside_bvd_per = gpd.read_file(paris_inside_bvd_peripherique)\n",
    "boundary_df = alphashape.alphashape(gdf_paris_inside_bvd_per, 435).exterior[0]\n",
    "linear_ring_polygon = Polygon(boundary_df)\n",
    "\n",
    "gdf_basecase_output_links = gpd.read_file('results/' + string_is_for_1pm + '_basecase_average_output_links.geojson')\n",
    "gdf_basecase_average_mode_stats = pd.read_csv('results/' + string_is_for_1pm + '_basecase_average_mode_stats.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is further than process_output_of_simulations_with_all_output_links_and_eqasim_info.ipynb, as it also includes more input information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results\n",
    "\n",
    "Process the outputs of the simulations for further usage by GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   0%|          | 0/70 [00:16<?, ?subdir/s]\n",
      "Processing subdirs:   0%|          | 0/70 [00:01<?, ?subdir/s]\n"
     ]
    }
   ],
   "source": [
    "# Read all network data into a dictionary of GeoDataFrames\n",
    "def compute_result_dic_output_links():\n",
    "    result_dic = {}\n",
    "    base_network_no_policies = gdf_basecase_output_links\n",
    "    result_dic[\"base_network_no_policies\"] = base_network_no_policies\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        # print(f'Accessing folder: {subdir}')\n",
    "        # print(len(os.listdir(subdir)))\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            gdf_output_links = pio.read_output_links(file_path)\n",
    "            if (gdf_output_links is not None):\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_output_links, column_to_extend='highway', new_column_name='highway')\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_extended, column_to_extend='vol_car', new_column_name='vol_car_base_case')\n",
    "                result_dic[policy_key] = gdf_extended\n",
    "        break\n",
    "    return result_dic\n",
    "\n",
    "def calculate_averaged_results(trips_df):\n",
    "    \"\"\"Calculate average travel time and routed distance grouped by mode.\"\"\"\n",
    "    return trips_df.groupby('mode').agg(\n",
    "        total_travel_time=('travel_time', 'mean'),\n",
    "        total_routed_distance=('routed_distance', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "def compute_result_dic_mode_stats(calculate_averaged_results):\n",
    "    result_dic_mode_stats = {}\n",
    "    result_dic_mode_stats[\"base_network_no_policies\"] = gdf_basecase_average_mode_stats\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_mode_stats = pd.read_csv(file_path + '/eqasim_trips.csv', delimiter=';')\n",
    "            averaged_results = calculate_averaged_results(df_mode_stats)\n",
    "            if (averaged_results is not None):\n",
    "                result_dic_mode_stats[policy_key] = averaged_results\n",
    "        break\n",
    "    return result_dic_mode_stats\n",
    "\n",
    "def encode_modes(gdf):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': gdf['modes'].str.contains('car', case=False, na=False).astype(int),\n",
    "        'bus': gdf['modes'].str.contains('bus', case=False, na=False).astype(int),\n",
    "        'pt': gdf['modes'].str.contains('pt', case=False, na=False).astype(int),\n",
    "        'train': gdf['modes'].str.contains('train', case=False, na=False).astype(int),\n",
    "        'rail': gdf['modes'].str.contains('rail', case=False, na=False).astype(int),\n",
    "        'subway': gdf['modes'].str.contains('subway', case=False, na=False).astype(int)\n",
    "    }\n",
    "\n",
    "    # Create a DataFrame from the conditions\n",
    "    modes_encoded = pd.DataFrame(modes_conditions)\n",
    "\n",
    "    # Return a tensor of the encoded modes\n",
    "    return torch.tensor(modes_encoded.values, dtype=torch.float)\n",
    "\n",
    "result_dic_output_links = compute_result_dic_output_links()\n",
    "result_dic_mode_stats = compute_result_dic_mode_stats(calculate_averaged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: idf_1pm_persons.csv into variable: persons_df\n",
      "Layer: idf_1pm_commutes\n",
      "Loaded GPKG file: idf_1pm_commutes.gpkg into variable: commutes_gdf\n",
      "Loaded CSV file: idf_1pm_households.csv into variable: households_df\n",
      "Loaded CSV file: idf_1pm_trips.csv into variable: trips_df\n",
      "Loaded CSV file: idf_1pm_activities.csv into variable: activities_df\n",
      "Loaded CSV file: idf_1pm_vehicle_types.csv into variable: vehicle_types_df\n",
      "Layer: idf_1pm_trips\n",
      "Loaded GPKG file: idf_1pm_trips.gpkg into variable: trips_gdf\n",
      "Layer: idf_1pm_activities\n",
      "Loaded GPKG file: idf_1pm_activities.gpkg into variable: activities_gdf\n",
      "Loaded CSV file: idf_1pm_vehicles.csv into variable: vehicles_df\n",
      "Layer: idf_1pm_homes\n",
      "Loaded GPKG file: idf_1pm_homes.gpkg into variable: homes_gdf\n"
     ]
    }
   ],
   "source": [
    "base_dir_sample_sim_input = '../../../../data/pop_1pm_simulations/idf_1pm/' \n",
    "files = os.listdir(base_dir_sample_sim_input)\n",
    "\n",
    "for file in files:\n",
    "    file_path = os.path.join(base_dir_sample_sim_input, file)\n",
    "    base_name, ext = os.path.splitext(file)\n",
    "    if base_name.startswith(\"idf_1pm_\"):\n",
    "        base_name = base_name.replace(\"idf_1pm_\", \"\")\n",
    "    var_name = base_name  # Start with the cleaned base name\n",
    "    \n",
    "    if file.endswith('.csv'):\n",
    "        try:\n",
    "            var_name = f\"{var_name}_df\"  \n",
    "            globals()[var_name] = pd.read_csv(file_path, sep=\";\")\n",
    "            print(f\"Loaded CSV file: {file} into variable: {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file {file}: {e}\")\n",
    "            \n",
    "    elif file.endswith('.gpkg'):\n",
    "        try:\n",
    "            var_name = f\"{var_name}_gdf\"  \n",
    "            layers = fiona.listlayers(file_path)\n",
    "            \n",
    "            # Read each layer into a GeoDataFrame\n",
    "            geodataframes = {layer: gpd.read_file(file_path, layer=layer, geometry = 'geometry', crs=\"EPSG:2154\") for layer in layers}\n",
    "\n",
    "            # Display the first few rows of each GeoDataFrame\n",
    "            for layer, gdf in geodataframes.items():\n",
    "                print(f\"Layer: {layer}\")\n",
    "                gdf = gdf.to_crs(epsg=4326)\n",
    "                \n",
    "                globals()[var_name] = gdf\n",
    "                print(f\"Loaded GPKG file: {file} into variable: {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading CSV file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "links_gdf = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "links_gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf.to_crs(\"EPSG:4326\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   household_id                  geometry  person_id  age_x  employed_x  \\\n",
      "0          1084  POINT (2.37653 48.86644)       1972     23        True   \n",
      "1          1084  POINT (2.37653 48.86644)       1973     23        True   \n",
      "2          1590  POINT (2.37969 48.84590)       2973     39       False   \n",
      "3          2921  POINT (2.32901 48.89036)       5524     32        True   \n",
      "4          2921  POINT (2.32901 48.89036)       5525     31        True   \n",
      "\n",
      "    sex_x  socioprofessional_class  has_driving_license  has_pt_subscription  \\\n",
      "0    male                        4                 True                 True   \n",
      "1  female                        4                 True                 True   \n",
      "2    male                        6                 True                 True   \n",
      "3  female                        4                False                False   \n",
      "4    male                        5                False                False   \n",
      "\n",
      "   census_person_id  hts_id bikeAvailability carAvailability  \\\n",
      "0               749     982             none            none   \n",
      "1               750    1054             none            none   \n",
      "2              1143    5283             none            none   \n",
      "3              2058     442             some            none   \n",
      "4              2059    7081             some            none   \n",
      "\n",
      "   censusHouseholdId  householdIncome  htsHouseholdId  isPassenger  \\\n",
      "0                410      1098.137370             463        False   \n",
      "1                410      1098.137370             502        False   \n",
      "2                615      3287.396810            2380        False   \n",
      "3               1106      2109.866062             177        False   \n",
      "4               1106      2109.866062            3077        False   \n",
      "\n",
      "                                            vehicles  \n",
      "0  {\"car\":\"1972:car\",\"car_passenger\":\"1972:car_pa...  \n",
      "1  {\"car\":\"1973:car\",\"car_passenger\":\"1973:car_pa...  \n",
      "2  {\"car\":\"2973:car\",\"car_passenger\":\"2973:car_pa...  \n",
      "3  {\"car\":\"5524:car\",\"car_passenger\":\"5524:car_pa...  \n",
      "4  {\"car\":\"5525:car\",\"car_passenger\":\"5525:car_pa...  \n"
     ]
    }
   ],
   "source": [
    "homes_gdf = globals()[\"homes_gdf\"]\n",
    "households_df = globals()[\"households_df\"]\n",
    "population_df = pd.read_csv(\"intermediate_results/population.csv\")\n",
    "persons_df = globals()[\"persons_df\"]\n",
    "\n",
    "sorted_population_df = population_df.sort_values(by=\"id\")\n",
    "sorted_persons_df = persons_df.sort_values(by=\"person_id\")\n",
    "merged_df = pd.merge(sorted_persons_df, sorted_population_df, left_on=\"person_id\", right_on=\"id\")\n",
    "removed_some_columns = merged_df.copy()\n",
    "removed_some_columns = removed_some_columns.drop(columns=['employed_y', 'hasPtSubscription', 'householdId', 'sex_y', 'htsPersonId', 'censusPersonId', 'hasLicense', 'id', 'age_y'])\n",
    "updated_persons = removed_some_columns.copy()\n",
    "persons_with_homes = homes_gdf.merge(updated_persons, on='household_id', how='right')\n",
    "\n",
    "print(persons_with_homes.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [01:06, 466.66row/s]\n"
     ]
    }
   ],
   "source": [
    "# Create a GeoDataFrame for homes_gdf if not already a GeoDataFrame\n",
    "if not isinstance(persons_with_homes, gpd.GeoDataFrame):\n",
    "    persons_with_homes = gpd.GeoDataFrame(persons_with_homes, geometry=gpd.points_from_xy(persons_with_homes.longitude, persons_with_homes.latitude), crs= links_gdf.crs)\n",
    "\n",
    "# Ensure both GeoDataFrames have the same CRS\n",
    "utm_crs = 'EPSG:32631'  # UTM zone 31N\n",
    "\n",
    "def compute_close_homes(gdf_base_input:pd.DataFrame, persons_with_homes_input:pd.DataFrame, utm_crs:str):\n",
    "    gdf_base = gdf_base_input.copy()\n",
    "    persons_with_homes = persons_with_homes_input.copy()\n",
    "    close_homes_count = []\n",
    "    gdf_base_utm = gdf_base.to_crs(utm_crs)\n",
    "    persons_with_homes_utm = persons_with_homes.to_crs(utm_crs)\n",
    "    for i, row in tqdm(enumerate(gdf_base_utm.iterrows()), desc=\"Processing rows\", unit=\"row\"):\n",
    "        buffer_utm = row[1].geometry.buffer(distance=50)\n",
    "        buffer = gpd.GeoSeries([buffer_utm], crs=utm_crs).to_crs(gdf_base_utm.crs)[0]\n",
    "        matching_homes = persons_with_homes_utm[persons_with_homes_utm.geometry.within(buffer)]\n",
    "        socioprofessional_classes = matching_homes['socioprofessional_class'].tolist()\n",
    "        close_homes_count.append((len(socioprofessional_classes), socioprofessional_classes))\n",
    "    return close_homes_count\n",
    "\n",
    "links_within_linear_ring_gdf = links_gdf[links_gdf.geometry.within(linear_ring_polygon)]\n",
    "persons_with_home_within_linear_ring = persons_with_homes[persons_with_homes.geometry.within(linear_ring_polygon)]\n",
    "\n",
    "close_homes_count_normal = compute_close_homes(gdf_base_input = links_gdf, persons_with_homes_input = persons_with_homes, utm_crs = utm_crs)\n",
    "# close_homes_count_within_linear_ring = compute_close_homes(gdf_base_input = links_within_linear_ring_gdf, persons_with_homes_input = persons_with_home_within_linear_ring, utm_crs = utm_crs)\n",
    "\n",
    "links_gdf['close_homes_count'] = close_homes_count_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the socio-professional classes and their counts\n",
    "socio_professional_classes = [item[1] for item in close_homes_count_normal]\n",
    "class_counts = [item[0] for item in close_homes_count_normal]\n",
    "\n",
    "# Get unique socio-professional classes\n",
    "unique_classes = set([cls for sublist in socio_professional_classes for cls in sublist])\n",
    "class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "# Initialize a tensor to hold the counts\n",
    "tensor_shape = (len(close_homes_count_normal), len(unique_classes))\n",
    "close_homes_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "# Fill the tensor with counts\n",
    "for i, classes in enumerate(socio_professional_classes):\n",
    "    for cls in classes:\n",
    "        close_homes_tensor[i, class_to_index[cls]] += 1\n",
    "\n",
    "# Convert the tensor to a sparse tensor to save memory\n",
    "close_homes_tensor_sparse = close_homes_tensor.to_sparse()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result_dic: 100%|██████████| 79/79 [00:08<00:00,  9.77dataframe/s]\n"
     ]
    }
   ],
   "source": [
    "def process_result_dic(result_dic, result_dic_mode_stats):\n",
    "    datalist = []\n",
    "    linegraph_transformation = LineGraph()\n",
    "    base_network_no_policies = result_dic.get(\"base_network_no_policies\")\n",
    "    vol_base_case = base_network_no_policies['vol_car'].values\n",
    "    capacity_base_case = base_network_no_policies['capacity'].values\n",
    "    length_base_case = base_network_no_policies['length'].values\n",
    "    freespeed_base_case = base_network_no_policies['freespeed'].values\n",
    "    modes_base_case = encode_modes(base_network_no_policies)\n",
    "    close_homes = close_homes_tensor_sparse.to_dense()\n",
    "    \n",
    "    # Initialize base edge positions\n",
    "    gdf_base = gpd.GeoDataFrame(base_network_no_policies, geometry='geometry')\n",
    "    gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "    gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    edge_positions_base = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                     (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                    for geom in gdf_base.geometry])\n",
    "    \n",
    "    nodes = pd.concat([gdf_base['from_node'], gdf_base['to_node']]).unique()\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "    gdf_base['from_idx'] = gdf_base['from_node'].map(node_to_idx)\n",
    "    gdf_base['to_idx'] = gdf_base['to_node'].map(node_to_idx)\n",
    "    edges_base = gdf_base[['from_idx', 'to_idx']].values\n",
    "    edge_positions_tensor = torch.tensor(edge_positions_base, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges_base, dtype=torch.long).t().contiguous()\n",
    "    x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index, x=x, pos=edge_positions_tensor)\n",
    "    linegraph_data = linegraph_transformation(data)\n",
    "\n",
    "    for key, df in tqdm(result_dic.items(), desc=\"Processing result_dic\", unit=\"dataframe\"):        \n",
    "        if isinstance(df, pd.DataFrame) and key != \"base_network_no_policies\":\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "            gdf.crs = \"EPSG:2154\"  \n",
    "            gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "            capacities_new = gdf['capacity'].values\n",
    "            capacity_reduction = capacities_new - capacity_base_case\n",
    "            highway = gdf['highway'].apply(lambda x: highway_mapping.get(x, -1)).values\n",
    "            length = gdf['length'].values\n",
    "            freespeed= gdf['freespeed'].values\n",
    "            modes = encode_modes(gdf)\n",
    "    \n",
    "            edge_car_volume_difference = gdf['vol_car'].values - vol_base_case\n",
    "            target_values = torch.tensor(edge_car_volume_difference, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "            linegraph_x = torch.tensor(np.column_stack((vol_base_case, capacity_base_case, capacities_new, capacity_reduction, highway, length, freespeed, length_base_case, freespeed_base_case, modes, modes_base_case, close_homes)), dtype=torch.float)\n",
    "            linegraph_data.x = linegraph_x\n",
    "            linegraph_data.y = target_values\n",
    "            \n",
    "            df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            if df_mode_stats is not None:\n",
    "                numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "                mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "                mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "                linegraph_data.mode_stats = mode_stats_tensor\n",
    "            if linegraph_data.validate(raise_on_error=True):\n",
    "                datalist.append(linegraph_data)\n",
    "            else:\n",
    "                print(\"Invalid line graph data\")\n",
    "    data_dict_list = [{'x': lg_data.x, 'edge_index': lg_data.edge_index, 'pos': lg_data.pos, 'y': lg_data.y, 'graph_attr': lg_data.mode_stats} for lg_data in datalist]\n",
    "    return data_dict_list\n",
    "\n",
    "data_processed = process_result_dic(result_dic=result_dic_output_links, result_dic_mode_stats=result_dic_mode_stats)\n",
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[6.6275e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [9.6078e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.4902e+00, 9.6000e+02, 9.6000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]),\n",
       " 'edge_index': tensor([[    0,     1,     1,  ..., 31138, 31139, 31139],\n",
       "         [   19, 10935, 10936,  ..., 30278, 31138, 31139]]),\n",
       " 'pos': tensor([[-1.3631, -5.9839],\n",
       "         [-1.3631, -5.9839],\n",
       "         [-1.3631, -5.9839],\n",
       "         ...,\n",
       "         [-1.3631, -5.9839],\n",
       "         [-1.3631, -5.9839],\n",
       "         [-1.3631, -5.9839]]),\n",
       " 'y': tensor([[ 1.3725],\n",
       "         [-0.6078],\n",
       "         [ 1.5098],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]]),\n",
       " 'graph_attr': tensor([[1.1676e+03, 3.6210e+03],\n",
       "         [1.2798e+03, 4.8323e+03],\n",
       "         [4.3310e+02, 4.3941e+03],\n",
       "         [7.8911e-01, 1.0581e+03],\n",
       "         [1.6101e+03, 5.4772e+03],\n",
       "         [1.0168e+03, 1.2207e+03]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for further processing with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_single_districts = pio.process_result_dic(result_dic_single_districts)\n",
    "# torch.save(data_processed_single_districts, result_path + '_single_districts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(persons_with_homes.geometry.x, persons_with_homes.geometry.y, s=1, color='blue', alpha=0.5)\n",
    "# plt.scatter(persons_with_home_within_linear_ring.geometry.x, persons_with_home_within_linear_ring.geometry.y, s=1, color='red', alpha=0.5)\n",
    "# plt.title('Locations of Persons with Homes')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "# from shapely.geometry import MultiPolygon\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a LineString\n",
    "# line = LineString([(10, 10), (20, 10)])\n",
    "\n",
    "# # Create a buffer around the line\n",
    "# buffered_line = line.buffer(2, cap_style=\"round\")\n",
    "\n",
    "# # Plot the original line and the buffered area\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# x, y = line.xy\n",
    "# plt.plot(x, y, color='blue', label='Original Line')\n",
    "# if isinstance(buffered_line, MultiPolygon):\n",
    "#     for polygon in buffered_line:\n",
    "#         x, y = polygon.exterior.xy\n",
    "#         plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "# else:\n",
    "#     x, y = buffered_line.exterior.xy\n",
    "#     plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "\n",
    "# plt.title('Line with Buffered Area')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.axis('equal')\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
