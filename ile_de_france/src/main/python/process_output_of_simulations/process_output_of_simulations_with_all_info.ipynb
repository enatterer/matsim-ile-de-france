{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "\n",
    "import processing_io as pio\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import shapely.wkt as wkt\n",
    "from tqdm import tqdm\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "import alphashape\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from shapely.geometry import Point\n",
    "\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "result_df_name = 'sim_output_1pm_capacity_reduction_10k_PRELIMINARY'\n",
    "result_path = '../../../../data/datasets_simulation_outputs/' + result_df_name + '.pt'\n",
    "string_is_for_1pm = \"pop_1pm\"\n",
    "\n",
    "base_dir_sample_sim_input = '../../../../data/' + string_is_for_1pm + '_simulations/' + string_is_for_1pm + '_policies_combinations_with_normal_dist/'\n",
    "subdirs_pattern = os.path.join(base_dir_sample_sim_input, 'output_networks_*')\n",
    "subdirs = list(set(glob.glob(subdirs_pattern)))\n",
    "subdirs.sort()\n",
    "\n",
    "paris_inside_bvd_peripherique = \"../../../../data/paris_inside_bvd_per/referentiel-comptages-edit.shp\"\n",
    "gdf_paris_inside_bvd_per = gpd.read_file(paris_inside_bvd_peripherique)\n",
    "boundary_df = alphashape.alphashape(gdf_paris_inside_bvd_per, 435).exterior[0]\n",
    "linear_ring_polygon = Polygon(boundary_df)\n",
    "\n",
    "gdf_basecase_output_links = gpd.read_file('results/' + string_is_for_1pm + '_basecase_average_output_links.geojson')\n",
    "gdf_basecase_average_mode_stats = pd.read_csv('results/' + string_is_for_1pm + '_basecase_average_mode_stats.csv', delimiter=';')\n",
    "districts = gpd.read_file(\"../../../../data/visualisation/districts_paris.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is further than process_output_of_simulations_with_all_output_links_and_eqasim_info.ipynb, as it also includes more input information.\n",
    "\n",
    "Note that there is more than one strategy to deal with the fact that there are more than one district per link. We implement the strategy of stacking the information of all districts together. \n",
    "An alternative strategy would be to use the mean of the information of the districts.\n",
    "\n",
    "Process the districts manually, so that each link belongs to at most 3 districts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results\n",
    "\n",
    "Process the outputs of the simulations for further usage by GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_close_homes(links_gdf_input:pd.DataFrame, information_gdf_input:pd.DataFrame, utm_crs:str, distance:int=50):\n",
    "    links_gdf = links_gdf_input.copy()\n",
    "    information_gdf = information_gdf_input.copy()\n",
    "    close_places = []\n",
    "    links_gdf_utm = links_gdf.to_crs(utm_crs)\n",
    "    information_gdf_utm = information_gdf.to_crs(utm_crs)\n",
    "    for i, row in tqdm(enumerate(links_gdf_utm.iterrows()), desc=\"Processing rows\", unit=\"row\"):\n",
    "        buffer_utm = row[1].geometry.buffer(distance=distance)\n",
    "        buffer = gpd.GeoSeries([buffer_utm], crs=utm_crs).to_crs(links_gdf_utm.crs)[0]\n",
    "        matched_information = information_gdf_utm[information_gdf_utm.geometry.within(buffer)]\n",
    "        socioprofessional_classes = matched_information['socioprofessional_class'].tolist()\n",
    "        close_places.append((len(socioprofessional_classes), socioprofessional_classes))\n",
    "    return close_places\n",
    "\n",
    "def process_close_count_to_tensor(close_count_list: list):\n",
    "    socio_professional_classes = [item[1] for item in close_count_list]\n",
    "    unique_classes = set([2, 3, 4, 5, 6, 7, 8])\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    tensor_shape = (len(close_count_list), len(unique_classes))\n",
    "    close_homes_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "    for i, classes in enumerate(socio_professional_classes):\n",
    "        for cls in classes:\n",
    "            if cls in class_to_index:  # Ensure the class is in the predefined set\n",
    "                close_homes_tensor[i, class_to_index[cls]] += 1\n",
    "    \n",
    "    close_homes_tensor_sparse = close_homes_tensor.to_sparse()\n",
    "    return close_homes_tensor_sparse\n",
    "\n",
    "# Read all network data into a dictionary of GeoDataFrames\n",
    "def compute_result_dic_output_links():\n",
    "    result_dic = {}\n",
    "    base_network_no_policies = gdf_basecase_output_links\n",
    "    result_dic[\"base_network_no_policies\"] = base_network_no_policies\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        # print(f'Accessing folder: {subdir}')\n",
    "        # print(len(os.listdir(subdir)))\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            gdf_output_links = pio.read_output_links(file_path)\n",
    "            if (gdf_output_links is not None):\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_output_links, column_to_extend='highway', new_column_name='highway')\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_extended, column_to_extend='vol_car', new_column_name='vol_car_base_case')\n",
    "                result_dic[policy_key] = gdf_extended\n",
    "        break\n",
    "    return result_dic\n",
    "\n",
    "def calculate_averaged_results(trips_df):\n",
    "    \"\"\"Calculate average travel time and routed distance grouped by mode.\"\"\"\n",
    "    return trips_df.groupby('mode').agg(\n",
    "        total_travel_time=('travel_time', 'mean'),\n",
    "        total_routed_distance=('routed_distance', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "def compute_result_dic_mode_stats(calculate_averaged_results):\n",
    "    result_dic_mode_stats = {}\n",
    "    result_dic_mode_stats[\"base_network_no_policies\"] = gdf_basecase_average_mode_stats\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_mode_stats = pd.read_csv(file_path + '/eqasim_trips.csv', delimiter=';')\n",
    "            averaged_results = calculate_averaged_results(df_mode_stats)\n",
    "            if (averaged_results is not None):\n",
    "                result_dic_mode_stats[policy_key] = averaged_results\n",
    "        break\n",
    "    return result_dic_mode_stats\n",
    "\n",
    "def encode_modes(gdf):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': gdf['modes'].str.contains('car', case=False, na=False).astype(int),\n",
    "        'bus': gdf['modes'].str.contains('bus', case=False, na=False).astype(int),\n",
    "        'pt': gdf['modes'].str.contains('pt', case=False, na=False).astype(int),\n",
    "        'train': gdf['modes'].str.contains('train', case=False, na=False).astype(int),\n",
    "        'rail': gdf['modes'].str.contains('rail', case=False, na=False).astype(int),\n",
    "        'subway': gdf['modes'].str.contains('subway', case=False, na=False).astype(int)\n",
    "    }\n",
    "    modes_encoded = pd.DataFrame(modes_conditions)\n",
    "    return torch.tensor(modes_encoded.values, dtype=torch.float)\n",
    "\n",
    "def get_dfs(base_dir:str):\n",
    "    files = os.listdir(base_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        if base_name.startswith(\"idf_1pm_\"):\n",
    "            base_name = base_name.replace(\"idf_1pm_\", \"\")\n",
    "        var_name = base_name  # Start with the cleaned base name\n",
    "    \n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_df\"  \n",
    "                globals()[var_name] = pd.read_csv(file_path, sep=\";\")\n",
    "                print(f\"Loaded CSV file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "            \n",
    "        elif file.endswith('.gpkg'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_gdf\"  \n",
    "                layers = fiona.listlayers(file_path)\n",
    "                geodataframes = {layer: gpd.read_file(file_path, layer=layer, geometry = 'geometry', crs=\"EPSG:2154\") for layer in layers}\n",
    "                for layer, gdf in geodataframes.items():\n",
    "                # print(f\"Layer: {layer}\")\n",
    "                    gdf = gdf.to_crs(epsg=4326)\n",
    "                    globals()[var_name] = gdf\n",
    "                    print(f\"Loaded GPKG file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "    homes_gdf = globals()[\"homes_gdf\"]\n",
    "    households_df = globals()[\"households_df\"]\n",
    "    persons_df = globals()[\"persons_df\"]\n",
    "    activities_gdf = globals()[\"activities_gdf\"]\n",
    "    trips_df = globals()[\"trips_gdf\"]\n",
    "    return homes_gdf, households_df, persons_df, activities_gdf, trips_df\n",
    "\n",
    "def extract_start_end_points(geometry):\n",
    "    if len(geometry.coords) != 2:\n",
    "        raise ValueError(\"Linestring does not have exactly 2 elements.\")\n",
    "    return geometry.coords[0], geometry.coords[-1]\n",
    "\n",
    "def get_close_trips_tensor(links_gdf_input, trips_gdf_input, utm_crs, distance):\n",
    "    close_trips_count = compute_close_homes(links_gdf_input = links_gdf_input, information_gdf_input = trips_gdf_input, utm_crs = utm_crs, distance=distance)\n",
    "    close_trips_count_tensor = process_close_count_to_tensor(close_trips_count)\n",
    "    return close_trips_count, close_trips_count_tensor\n",
    "\n",
    "def get_start_and_end_gdf(trips_with_socio, crs):\n",
    "    trips_start = trips_with_socio.copy()\n",
    "    trips_end = trips_with_socio.copy()\n",
    "\n",
    "    trips_start_gdf = gpd.GeoDataFrame(\n",
    "    trips_start, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_start['start_point'].apply(lambda p: p[0]), \n",
    "        trips_start['start_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "\n",
    "    trips_end_gdf = gpd.GeoDataFrame(\n",
    "    trips_end, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_end['end_point'].apply(lambda p: p[0]), \n",
    "        trips_end['end_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "    return trips_start_gdf,trips_end_gdf\n",
    "\n",
    "def process_centroid(geom_list):\n",
    "    if not geom_list:  # Empty list\n",
    "        return [np.nan, np.nan]\n",
    "    elif len(geom_list) == 1:\n",
    "        return [geom_list[0], np.nan, np.nan]\n",
    "    elif len(geom_list) == 2:\n",
    "        return [geom_list[0], geom_list[1], np.nan]\n",
    "    else:\n",
    "        return [geom_list[0], geom_list[1], geom_list[2]]\n",
    "    \n",
    "def extract_point_coordinates(geom_list):\n",
    "    coordinates = []\n",
    "    for geom in geom_list:\n",
    "        if isinstance(geom, Point):\n",
    "            coordinates.append((geom.x, geom.y))\n",
    "        else:\n",
    "            coordinates.append((np.nan, np.nan))\n",
    "    return coordinates\n",
    "\n",
    "def process_value_list(perimeter_list):\n",
    "    if not perimeter_list:  # Empty list\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 1:\n",
    "        return [perimeter_list[0], np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 2:\n",
    "        return [perimeter_list[0], perimeter_list[1], np.nan]\n",
    "    else:\n",
    "        return [perimeter_list[0], perimeter_list[1], perimeter_list[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   0%|          | 0/100 [00:25<?, ?subdir/s]\n",
      "Processing subdirs:   0%|          | 0/100 [00:02<?, ?subdir/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31216, 28)\n",
      "(31216, 28)\n",
      "(31216, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_15484/3293343827.py:10: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  districts['district_centroid'] = districts['geometry'].centroid\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "result_dic_output_links = compute_result_dic_output_links()\n",
    "result_dic_mode_stats = compute_result_dic_mode_stats(calculate_averaged_results)\n",
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "print(base_gdf.shape)\n",
    "links_gdf_base = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "print(links_gdf_base.shape)\n",
    "links_gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "print(links_gdf_base.shape)\n",
    "districts['district_centroid'] = districts['geometry'].centroid\n",
    "links_gdf_with_districts = gpd.sjoin(links_gdf_base, districts, how='left', op='intersects')\n",
    "\n",
    "# Group by edge and aggregate the district names\n",
    "links_gdf_with_districts = links_gdf_with_districts.groupby('link').agg({\n",
    "    'from_node': 'first',\n",
    "    'to_node': 'first',\n",
    "    'length': 'first',\n",
    "    'freespeed': 'first',\n",
    "    'capacity': 'first',\n",
    "    'lanes': 'first',\n",
    "    'modes': 'first',\n",
    "    'vol_car': 'first',\n",
    "    'highway': 'first',\n",
    "    'geometry': 'first',\n",
    "    'c_ar': lambda x: list(x.dropna()),\n",
    "    'district_centroid': lambda x: list(x.dropna()),\n",
    "    'perimetre': lambda x: list(x.dropna()),\n",
    "    'surface': lambda x: list(x.dropna()),\n",
    "}).reset_index()\n",
    "gdf_now = gpd.GeoDataFrame(links_gdf_with_districts, geometry='geometry', crs=links_gdf_base.crs)\n",
    "gdf_now = gdf_now.rename(columns={'c_ar': 'district', 'perimetre': 'district_perimeter', 'surface': 'district_surface'})\n",
    "links_gdf_final = gdf_now.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def preprocess_links(links_gdf):\n",
    "    for index, row in links_gdf.iterrows():\n",
    "        if len(row['district']) >= 4:\n",
    "            row['district'].pop(random.randint(0, len(row['district']) - 1))\n",
    "    return links_gdf\n",
    "\n",
    "\n",
    "\n",
    "links_gdf_final = preprocess_links(links_gdf_final)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the list in 'district' in links_gdf_final is: 3\n"
     ]
    }
   ],
   "source": [
    "max_district_length = max(links_gdf_final['district'].apply(len))\n",
    "print(f\"The maximum length of the list in 'district' in links_gdf_final is: {max_district_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: idf_1pm_persons.csv into variable: persons_df\n",
      "Loaded GPKG file: idf_1pm_commutes.gpkg into variable: commutes_gdf\n",
      "Loaded CSV file: idf_1pm_households.csv into variable: households_df\n",
      "Loaded CSV file: idf_1pm_trips.csv into variable: trips_df\n",
      "Loaded CSV file: idf_1pm_activities.csv into variable: activities_df\n",
      "Loaded CSV file: idf_1pm_vehicle_types.csv into variable: vehicle_types_df\n",
      "Loaded GPKG file: idf_1pm_trips.gpkg into variable: trips_gdf\n",
      "Loaded GPKG file: idf_1pm_activities.gpkg into variable: activities_gdf\n",
      "Loaded CSV file: idf_1pm_vehicles.csv into variable: vehicles_df\n",
      "Loaded GPKG file: idf_1pm_homes.gpkg into variable: homes_gdf\n"
     ]
    }
   ],
   "source": [
    "base_dir_sample_sim_input = '../../../../data/pop_1pm_simulations/idf_1pm/' \n",
    "homes_gdf, households_df, persons_df, activities_gdf, trips_df = get_dfs(base_dir=base_dir_sample_sim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"intermediate_results/population.csv\")\n",
    "\n",
    "sorted_population_df = population_df.sort_values(by=\"id\")\n",
    "sorted_persons_df = persons_df.sort_values(by=\"person_id\")\n",
    "merged_df = pd.merge(sorted_persons_df, sorted_population_df, left_on=\"person_id\", right_on=\"id\")\n",
    "removed_some_columns = merged_df.copy()\n",
    "removed_some_columns = removed_some_columns.drop(columns=['employed_y', 'hasPtSubscription', 'householdId', 'sex_y', 'htsPersonId', 'censusPersonId', 'hasLicense', 'id', 'age_y'])\n",
    "updated_persons = removed_some_columns.copy()\n",
    "persons_with_geospatial_information = homes_gdf.merge(updated_persons, on='household_id', how='right')\n",
    "\n",
    "if not isinstance(persons_with_geospatial_information, gpd.GeoDataFrame):\n",
    "    persons_with_geospatial_information = gpd.GeoDataFrame(persons_with_geospatial_information, geometry=gpd.points_from_xy(persons_with_geospatial_information.longitude, persons_with_geospatial_information.latitude), crs= links_gdf.crs)\n",
    "\n",
    "utm_crs = 'EPSG:32631'  # UTM zone 31N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEAL WITH TRIPS\n",
    "\n",
    "trips_with_socio = trips_df.merge(persons_with_geospatial_information[['person_id', 'socioprofessional_class']], on='person_id', how='left')\n",
    "trips_with_socio['start_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[0])\n",
    "trips_with_socio['end_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[1])\n",
    "\n",
    "trips_start_gdf, trips_end_gdf = get_start_and_end_gdf(trips_with_socio=trips_with_socio, crs=links_gdf_final.crs)\n",
    "\n",
    "# close_trips_start, close_start_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=trips_start_gdf, utm_crs=utm_crs, distance=50)\n",
    "# close_trips_end, close_end_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=trips_end_gdf, utm_crs=utm_crs, distance=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing preceding purposes:   0%|          | 0/6 [00:00<?, ?purpose/s]\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "# Create tensors for each combination of \"preceding_purpose\" and \"following_purpose\"\n",
    "unique_purposes = trips_with_socio['preceding_purpose'].unique()\n",
    "close_start_trips_tensor_dict = {}\n",
    "close_start_trips_dict = {}\n",
    "close_end_trips_tensor_dict = {}\n",
    "close_end_trips_dict = {}\n",
    "\n",
    "max_iteration = 3\n",
    "for preceding_purpose in tqdm(unique_purposes, desc=\"Processing preceding purposes\", unit=\"purpose\"):\n",
    "    for following_purpose in tqdm(unique_purposes, desc=\"Processing following purposes\", unit=\"purpose\"):\n",
    "        if preceding_purpose != following_purpose:\n",
    "            filtered_trips = trips_with_socio[(trips_with_socio['preceding_purpose'] == preceding_purpose) & (trips_with_socio['following_purpose'] == following_purpose)]\n",
    "            if not filtered_trips.empty:\n",
    "                filtered_trips_start_gdf, filtered_trips_end_gdf = get_start_and_end_gdf(trips_with_socio=filtered_trips, crs=links_gdf_final.crs)\n",
    "                \n",
    "                close_start_trips, close_start_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_start_gdf, utm_crs=utm_crs, distance=50)\n",
    "                close_end_trips, close_end_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_end_gdf, utm_crs=utm_crs, distance=50)\n",
    "                \n",
    "                tensor_key = f\"{preceding_purpose}_{following_purpose}\"\n",
    "                close_start_trips_tensor_dict[tensor_key] = close_start_trips_tensor\n",
    "                close_start_trips_dict[tensor_key] = close_start_trips\n",
    "                close_end_trips_tensor_dict[tensor_key] = close_end_trips_tensor\n",
    "                close_end_trips_dict[tensor_key] = close_end_trips\n",
    "    break\n",
    "\n",
    "all_close_start_trips_tensors = torch.cat(list(close_start_trips_tensor_dict.values()), dim=0)\n",
    "all_close_end_trips_tensors = torch.cat(list(close_end_trips_tensor_dict.values()), dim=0)\n",
    "\n",
    "for key, tensor in close_start_trips_tensor_dict.items():\n",
    "    print(f\"Size of tensor for {key}: {tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [01:46, 293.01row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH HOMES\n",
    "\n",
    "close_homes_count_normal = compute_close_homes(links_gdf_input = links_gdf_final, information_gdf_input = persons_with_geospatial_information, utm_crs = utm_crs)\n",
    "links_gdf_final['close_homes_count'] = close_homes_count_normal\n",
    "close_homes_tensor = process_close_count_to_tensor(close_homes_count_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [01:48, 287.12row/s]\n",
      "Processing rows: 31216row [02:50, 183.51row/s]\n",
      "Processing rows: 31216row [01:38, 316.66row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH ACTIVITIES\n",
    "\n",
    "activities_with_socio = activities_gdf.merge(persons_with_geospatial_information[['household_id', 'socioprofessional_class']], on='household_id', how='left')\n",
    "grouped_activities = activities_with_socio.groupby('purpose')\n",
    "activities_by_purpose = {purpose: group.reset_index(drop=True) for purpose, group in grouped_activities}\n",
    "activities_by_purpose_tensor = {}\n",
    "max_counter = 3\n",
    "i = 0\n",
    "for purpose, activities in activities_by_purpose.items():\n",
    "    close_activities_count_purpose = f\"close_activities_count_{purpose}\"\n",
    "    close_activity_count = compute_close_homes(links_gdf_input=links_gdf_final, information_gdf_input=activities, utm_crs=utm_crs)\n",
    "    links_gdf_final[close_activities_count_purpose] = close_activity_count\n",
    "    activities_by_purpose_tensor[purpose] = process_close_count_to_tensor(close_activity_count)\n",
    "    i += 1\n",
    "    if i == max_counter:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['education', 'home', 'leisure'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activities_by_purpose_tensor.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT CENTROIDS\n",
    "\n",
    "# centroid_distances = np.array([\n",
    "#     process_centroid(geom_list)\n",
    "#     for geom_list in links_gdf_final['district_centroid']\n",
    "# ])\n",
    "\n",
    "# # Process the centroids\n",
    "# centroid_distance_with_coordinates = np.array([\n",
    "#     extract_point_coordinates(geom_list)\n",
    "#     for geom_list in centroid_distances\n",
    "# ])\n",
    "# district_centroids_tensor = torch.tensor(centroid_distance_with_coordinates, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT PERIMETER AND SURFACE\n",
    "\n",
    "# district_perimeters = np.array([\n",
    "#     process_value_list(perimeter_list)\n",
    "#     for perimeter_list in links_gdf_final['district_perimeter']\n",
    "# ])\n",
    "# district_surfaces = np.array([\n",
    "#     process_value_list(surface_list)\n",
    "#     for surface_list in links_gdf_final['district_surface']\n",
    "# ])\n",
    "\n",
    "# district_perimeters_tensor = torch.tensor(district_perimeters, dtype=torch.float)\n",
    "# district_surfaces_tensor = torch.tensor(district_surfaces, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRICT HOME COUNTS \n",
    "\n",
    "def compute_district_2_information_counts(district_information_counts, column_to_filter_for):\n",
    "    district_group_2_information_counts = {}\n",
    "    for district, group in district_information_counts:        \n",
    "        # ignore groups with more than one district here. \n",
    "        if len(district) == 1:\n",
    "            total_counts = 0\n",
    "            total_distributions = []\n",
    "            counts = group[column_to_filter_for].values            \n",
    "            for c in counts:\n",
    "                total_counts += c[0]\n",
    "                if c[1] is not None and len(c[1]) > 0:\n",
    "                    total_distributions.extend(c[1])\n",
    "            distribution_counts = [total_distributions.count(i) for i in range(2, 9)]   \n",
    "            district_group_2_information_counts[district] = distribution_counts\n",
    "    return district_group_2_information_counts, distribution_counts\n",
    "\n",
    "def compute_district_2_information_tensor(district_2_information_counts, distribution_counts):\n",
    "    district_home_counts_tensor = torch.zeros((len(links_gdf_final), 3, len(distribution_counts)), dtype=torch.float)\n",
    "    nan_tensor = torch.full((len(distribution_counts),), float('nan'))\n",
    "\n",
    "    for idx, row in links_gdf_final.iterrows():\n",
    "        district_combination = row['district']\n",
    "        district_combination_tuple = tuple(district_combination)\n",
    "        if len(district_combination_tuple) == 0:\n",
    "            district_home_counts_tensor[idx] = torch.stack([nan_tensor, nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 1:\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[district_combination_tuple]), nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 2:\n",
    "            a, b = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), nan_tensor])\n",
    "        elif len(district_combination_tuple) == 3:\n",
    "            a, b, c = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), torch.tensor(district_2_information_counts[(c,)])])\n",
    "        else:\n",
    "            print(\"NOT OK!\")\n",
    "            print(district_combination_tuple)\n",
    "    return district_home_counts_tensor\n",
    "\n",
    "links_gdf_final['close_homes_count_tuple'] = links_gdf_final['district'].apply(lambda x: tuple(x))\n",
    "district_home_counts = links_gdf_final.groupby('close_homes_count_tuple')\n",
    "district_2_home_counts, distribution_counts = compute_district_2_information_counts(district_information_counts=district_home_counts, column_to_filter_for = 'close_homes_count')\n",
    "district_home_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_home_counts, distribution_counts=distribution_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRICT ACTIVITIES COUNTS\n",
    "\n",
    "activity_2_district_tensor = {}\n",
    "for purpose in activities_by_purpose_tensor.keys():\n",
    "    links_gdf_final['close_activities_count_tuple'] = links_gdf_final['district'].apply(lambda x: tuple(x))\n",
    "    district_activity_counts = links_gdf_final.groupby('close_activities_count_tuple')\n",
    "    district_2_activity_counts, distribution_counts_activity = compute_district_2_information_counts(district_information_counts=district_activity_counts, column_to_filter_for='close_activities_count_' + purpose)\n",
    "    district_activitiy_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_activity_counts, distribution_counts=distribution_counts_activity)\n",
    "    activity_2_district_tensor[purpose] = district_activitiy_counts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  109,   184,   185,  ..., 31182, 31186, 31208])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = torch.nonzero(~torch.isnan(activity_2_district_tensor['education'][:, 1, 0])).squeeze()\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRICT TRIPS START AND END\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 entries of start_points_tensor: tensor([[ 2.3387, 48.8518],\n",
      "        [ 2.3387, 48.8524],\n",
      "        [ 2.3387, 48.8523],\n",
      "        [ 2.3399, 48.8520],\n",
      "        [ 2.3399, 48.8518]])\n",
      "First 5 entries of end_points_tensor: tensor([[ 2.3385, 48.8518],\n",
      "        [ 2.3387, 48.8523],\n",
      "        [ 2.3387, 48.8524],\n",
      "        [ 2.3399, 48.8518],\n",
      "        [ 2.3391, 48.8515]])\n",
      "First 5 entries of edge_positions_tensor: tensor([[ 2.3386, 48.8518],\n",
      "        [ 2.3387, 48.8524],\n",
      "        [ 2.3387, 48.8524],\n",
      "        [ 2.3399, 48.8519],\n",
      "        [ 2.3395, 48.8517]])\n",
      "First 5 entries of edge_start_end_tensor: tensor([[[ 2.3387, 48.8518],\n",
      "         [ 2.3385, 48.8518]],\n",
      "\n",
      "        [[ 2.3387, 48.8524],\n",
      "         [ 2.3387, 48.8523]],\n",
      "\n",
      "        [[ 2.3387, 48.8523],\n",
      "         [ 2.3387, 48.8524]],\n",
      "\n",
      "        [[ 2.3399, 48.8520],\n",
      "         [ 2.3399, 48.8518]],\n",
      "\n",
      "        [[ 2.3399, 48.8518],\n",
      "         [ 2.3391, 48.8515]]])\n"
     ]
    }
   ],
   "source": [
    "# PROCESS LINK GEOMETRIES\n",
    "\n",
    "edge_midpoints = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                    (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                for geom in links_gdf_final.geometry])\n",
    "\n",
    "nodes = pd.concat([links_gdf_final['from_node'], links_gdf_final['to_node']]).unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "links_gdf_final['from_idx'] = links_gdf_final['from_node'].map(node_to_idx)\n",
    "links_gdf_final['to_idx'] = links_gdf_final['to_node'].map(node_to_idx)\n",
    "edges_base = links_gdf_final[['from_idx', 'to_idx']].values\n",
    "edge_midpoint_tensor = torch.tensor(edge_midpoints, dtype=torch.float)\n",
    "\n",
    "# Initialize start and end points\n",
    "start_points = np.array([geom.coords[0] for geom in links_gdf_final.geometry])\n",
    "end_points = np.array([geom.coords[-1] for geom in links_gdf_final.geometry])\n",
    "\n",
    "# Convert to tensors\n",
    "edge_start_point_tensor = torch.tensor(start_points, dtype=torch.float)\n",
    "edge_end_point_tensor = torch.tensor(end_points, dtype=torch.float)\n",
    "\n",
    "edge_start_end_tensor = torch.stack((edge_start_point_tensor, edge_end_point_tensor), dim=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31216, 28)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'close_homes_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 95\u001b[0m\n\u001b[1;32m     92\u001b[0m     data_dict_list \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m: lg_data\u001b[38;5;241m.\u001b[39mx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_index\u001b[39m\u001b[38;5;124m'\u001b[39m: lg_data\u001b[38;5;241m.\u001b[39medge_index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpos\u001b[39m\u001b[38;5;124m'\u001b[39m: lg_data\u001b[38;5;241m.\u001b[39mpos, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m: lg_data\u001b[38;5;241m.\u001b[39my, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgraph_attr\u001b[39m\u001b[38;5;124m'\u001b[39m: lg_data\u001b[38;5;241m.\u001b[39mmode_stats} \u001b[38;5;28;01mfor\u001b[39;00m lg_data \u001b[38;5;129;01min\u001b[39;00m datalist]\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data_dict_list\n\u001b[0;32m---> 95\u001b[0m data_processed \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_result_dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_dic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dic_output_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_dic_mode_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dic_mode_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     96\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(data_processed, result_path)\n",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m, in \u001b[0;36mprocess_result_dic\u001b[0;34m(result_dic, result_dic_mode_stats)\u001b[0m\n\u001b[1;32m      9\u001b[0m freespeed_base_case \u001b[38;5;241m=\u001b[39m base_network_no_policies[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfreespeed\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     10\u001b[0m modes_base_case \u001b[38;5;241m=\u001b[39m encode_modes(base_network_no_policies)\n\u001b[0;32m---> 11\u001b[0m close_homes \u001b[38;5;241m=\u001b[39m \u001b[43mclose_homes_tensor\u001b[49m\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[1;32m     12\u001b[0m activities_home \u001b[38;5;241m=\u001b[39m activities_by_purpose_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhome\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[1;32m     13\u001b[0m activities_work \u001b[38;5;241m=\u001b[39m activities_by_purpose_tensor[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwork\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto_dense()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'close_homes_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "def process_result_dic(result_dic, result_dic_mode_stats):\n",
    "    datalist = []\n",
    "    linegraph_transformation = LineGraph()\n",
    "    \n",
    "    edge_start_point_tensor = edge_start_point_tensor.to_dense()\n",
    "    edge_end_point_tensor = edge_end_point_tensor.to_dense()\n",
    "    edge_start_end_point_tensor = edge_start_end_tensor.to_dense()\n",
    "    edge_midpoint_tensor = edge_midpoint_tensor.to_dense()\n",
    "    \n",
    "    base_network_no_policies = result_dic.get(\"base_network_no_policies\")\n",
    "    vol_base_case = base_network_no_policies['vol_car'].values\n",
    "    capacity_base_case = base_network_no_policies['capacity'].values\n",
    "    length_base_case = base_network_no_policies['length'].values\n",
    "    freespeed_base_case = base_network_no_policies['freespeed'].values\n",
    "    modes_base_case = encode_modes(base_network_no_policies)\n",
    "    close_homes = close_homes_tensor.to_dense()\n",
    "    activities_home = activities_by_purpose_tensor['home'].to_dense()\n",
    "    activities_work = activities_by_purpose_tensor['work'].to_dense()\n",
    "    activities_education = activities_by_purpose_tensor['education'].to_dense()\n",
    "    activities_shop = activities_by_purpose_tensor['shop'].to_dense()\n",
    "    activities_leisure = activities_by_purpose_tensor['leisure'].to_dense()\n",
    "    activities_other = activities_by_purpose_tensor['other'].to_dense()\n",
    "    # close_start_trips_tensor = all_close_start_trips_tensors.to_dense()\n",
    "    # close_end_trips_tensor = all_close_end_trips_tensors.to_dense()\n",
    "    \n",
    "    district_centroids = district_centroids_tensor.to_dense()\n",
    "    district_perimeters = district_perimeters_tensor.to_dense()\n",
    "    district_surfaces = district_surfaces_tensor.to_dense()\n",
    "    district_home_counts = district_home_counts_tensor.to_dense()\n",
    "    district_activities_home = activity_2_district_tensor['home'].to_dense()\n",
    "    district_activities_work = activity_2_district_tensor['work'].to_dense()\n",
    "    district_activities_education = activity_2_district_tensor['education'].to_dense()\n",
    "    district_activities_shop = activity_2_district_tensor['shop'].to_dense()\n",
    "    district_activities_leisure = activity_2_district_tensor['leisure'].to_dense()\n",
    "    district_activities_other = activity_2_district_tensor['other'].to_dense()\n",
    "    \n",
    "    edge_index = torch.tensor(edges_base, dtype=torch.long).t().contiguous()\n",
    "    x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index, x=x)\n",
    "    # data = Data(edge_index=edge_index, x=x, pos=edge_positions_tensor)\n",
    "    linegraph_data = linegraph_transformation(data)\n",
    "\n",
    "    for key, df in tqdm(result_dic.items(), desc=\"Processing result_dic\", unit=\"dataframe\"):        \n",
    "        if isinstance(df, pd.DataFrame) and key != \"base_network_no_policies\":\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "            gdf.crs = \"EPSG:2154\"  \n",
    "            gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "            capacities_new = gdf['capacity'].values\n",
    "            capacity_reduction = capacities_new - capacity_base_case\n",
    "            highway = gdf['highway'].apply(lambda x: highway_mapping.get(x, -1)).values\n",
    "            length = gdf['length'].values\n",
    "            freespeed= gdf['freespeed'].values\n",
    "            modes = encode_modes(gdf)\n",
    "    \n",
    "            edge_car_volume_difference = gdf['vol_car'].values - vol_base_case\n",
    "            target_values = torch.tensor(edge_car_volume_difference, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "            linegraph_x = torch.tensor(np.column_stack((vol_base_case, capacity_base_case, capacities_new, capacity_reduction, \n",
    "                                                        highway, length, freespeed, length_base_case, freespeed_base_case, \n",
    "                                                        modes, modes_base_case, close_homes, \n",
    "                                                        activities_home, activities_work, activities_education, activities_shop, activities_leisure, activities_other,\n",
    "                                                        # close_start_trips_tensor, close_end_trips_tensor,\n",
    "                                                        district_perimeters,\n",
    "                                                        district_surfaces,\n",
    "                                                        district_home_counts,\n",
    "                                                        district_activities_home,\n",
    "                                                        district_activities_work,\n",
    "                                                        district_activities_education,\n",
    "                                                        district_activities_shop,\n",
    "                                                        district_activities_leisure,\n",
    "                                                        district_activities_other\n",
    "                                                        )), dtype=torch.float)\n",
    "            linegraph_pos = torch.tensor(np.column_stack(edge_start_end_point_tensor, edge_start_point_tensor, edge_end_point_tensor, edge_midpoint_tensor, district_centroids), dtype=torch.float)\n",
    "            linegraph_data.x = linegraph_x\n",
    "            linegraph_data.pos = linegraph_pos\n",
    "            linegraph_data.y = target_values\n",
    "            \n",
    "            df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            if df_mode_stats is not None:\n",
    "                numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "                mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "                mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "                linegraph_data.mode_stats = mode_stats_tensor\n",
    "            if linegraph_data.validate(raise_on_error=True):\n",
    "                datalist.append(linegraph_data)\n",
    "            else:\n",
    "                print(\"Invalid line graph data\")\n",
    "    data_dict_list = [{'x': lg_data.x, 'edge_index': lg_data.edge_index, 'pos': lg_data.pos, 'y': lg_data.y, 'graph_attr': lg_data.mode_stats} for lg_data in datalist]\n",
    "    return data_dict_list\n",
    "\n",
    "data_processed = process_result_dic(result_dic=result_dic_output_links, result_dic_mode_stats=result_dic_mode_stats)\n",
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[6.6275e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [9.6078e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.4902e+00, 9.6000e+02, 9.6000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]),\n",
       " 'edge_index': tensor([[    0,     1,     1,  ..., 31138, 31139, 31139],\n",
       "         [   19, 10935, 10936,  ..., 30278, 31138, 31139]]),\n",
       " 'pos': tensor([[-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         ...,\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836]]),\n",
       " 'y': tensor([[ 1.3725],\n",
       "         [-0.6078],\n",
       "         [ 1.5098],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]]),\n",
       " 'graph_attr': tensor([[1.1676e+03, 3.6210e+03],\n",
       "         [1.2798e+03, 4.8323e+03],\n",
       "         [4.3310e+02, 4.3941e+03],\n",
       "         [7.8911e-01, 1.0581e+03],\n",
       "         [1.6101e+03, 5.4772e+03],\n",
       "         [1.0168e+03, 1.2207e+03]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for further processing with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_single_districts = pio.process_result_dic(result_dic_single_districts)\n",
    "# torch.save(data_processed_single_districts, result_path + '_single_districts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(persons_with_homes.geometry.x, persons_with_homes.geometry.y, s=1, color='blue', alpha=0.5)\n",
    "# plt.scatter(persons_with_home_within_linear_ring.geometry.x, persons_with_home_within_linear_ring.geometry.y, s=1, color='red', alpha=0.5)\n",
    "# plt.title('Locations of Persons with Homes')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "# from shapely.geometry import MultiPolygon\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a LineString\n",
    "# line = LineString([(10, 10), (20, 10)])\n",
    "\n",
    "# # Create a buffer around the line\n",
    "# buffered_line = line.buffer(2, cap_style=\"round\")\n",
    "\n",
    "# # Plot the original line and the buffered area\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# x, y = line.xy\n",
    "# plt.plot(x, y, color='blue', label='Original Line')\n",
    "# if isinstance(buffered_line, MultiPolygon):\n",
    "#     for polygon in buffered_line:\n",
    "#         x, y = polygon.exterior.xy\n",
    "#         plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "# else:\n",
    "#     x, y = buffered_line.exterior.xy\n",
    "#     plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "\n",
    "# plt.title('Line with Buffered Area')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# def check_trips_equivalence(close_trips_start, close_trips_end):\n",
    "#     \"\"\"\n",
    "#     Check if close_trips_start and close_trips_end are equivalent.\n",
    "    \n",
    "#     Args:\n",
    "#     close_trips_start (list): List of tuples for start trips\n",
    "#     close_trips_end (list): List of tuples for end trips\n",
    "    \n",
    "#     Returns:\n",
    "#     bool: True if equivalent, False otherwise\n",
    "#     \"\"\"\n",
    "#     if len(close_trips_start) != len(close_trips_end):\n",
    "#         print(\"Lists have different lengths.\")\n",
    "#         return False\n",
    "    \n",
    "#     differences = []\n",
    "#     for i, (start, end) in enumerate(zip(close_trips_start, close_trips_end)):\n",
    "#         if start != end:\n",
    "#             differences.append((i, start, end))\n",
    "    \n",
    "#     if not differences:\n",
    "#         print(\"The lists are identical.\")\n",
    "#         return True\n",
    "#     else:\n",
    "#         print(f\"Found {len(differences)} differences:\")\n",
    "#         for diff in differences[:10]:  # Print first 10 differences\n",
    "#             print(f\"Index {diff[0]}: Start {diff[1]}, End {diff[2]}\")\n",
    "#         if len(differences) > 10:\n",
    "#             print(f\"... and {len(differences) - 10} more differences.\")\n",
    "#         return False\n",
    "\n",
    "# # Usage\n",
    "# are_equivalent = check_trips_equivalence(close_trips_start, close_trips_end)\n",
    "# print(f\"Are the trip lists equivalent? {are_equivalent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
