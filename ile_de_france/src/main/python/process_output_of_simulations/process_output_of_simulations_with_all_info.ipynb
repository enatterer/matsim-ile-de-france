{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "\n",
    "import processing_io as pio\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import shapely.wkt as wkt\n",
    "from tqdm import tqdm\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "import alphashape\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "result_df_name = 'sim_output_1pm_capacity_reduction_10k_PRELIMINARY'\n",
    "result_path = '../../../../data/datasets_simulation_outputs/' + result_df_name + '.pt'\n",
    "string_is_for_1pm = \"pop_1pm\"\n",
    "\n",
    "base_dir_sample_sim_input = '../../../../data/' + string_is_for_1pm + '_simulations/' + string_is_for_1pm + '_policies_combinations_with_normal_dist/'\n",
    "subdirs_pattern = os.path.join(base_dir_sample_sim_input, 'output_networks_*')\n",
    "subdirs = list(set(glob.glob(subdirs_pattern)))\n",
    "subdirs.sort()\n",
    "\n",
    "paris_inside_bvd_peripherique = \"../../../../data/paris_inside_bvd_per/referentiel-comptages-edit.shp\"\n",
    "gdf_paris_inside_bvd_per = gpd.read_file(paris_inside_bvd_peripherique)\n",
    "boundary_df = alphashape.alphashape(gdf_paris_inside_bvd_per, 435).exterior[0]\n",
    "linear_ring_polygon = Polygon(boundary_df)\n",
    "\n",
    "gdf_basecase_output_links = gpd.read_file('results/' + string_is_for_1pm + '_basecase_average_output_links.geojson')\n",
    "gdf_basecase_average_mode_stats = pd.read_csv('results/' + string_is_for_1pm + '_basecase_average_mode_stats.csv', delimiter=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is further than process_output_of_simulations_with_all_output_links_and_eqasim_info.ipynb, as it also includes more input information."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results\n",
    "\n",
    "Process the outputs of the simulations for further usage by GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   0%|          | 0/70 [00:16<?, ?subdir/s]\n",
      "Processing subdirs:   0%|          | 0/70 [00:01<?, ?subdir/s]\n"
     ]
    }
   ],
   "source": [
    "def compute_close_homes(links_gdf_input:pd.DataFrame, information_gdf_input:pd.DataFrame, utm_crs:str, distance:int=50):\n",
    "    links_gdf = links_gdf_input.copy()\n",
    "    information_gdf = information_gdf_input.copy()\n",
    "    close_places = []\n",
    "    links_gdf_utm = links_gdf.to_crs(utm_crs)\n",
    "    information_gdf_utm = information_gdf.to_crs(utm_crs)\n",
    "    for i, row in tqdm(enumerate(links_gdf_utm.iterrows()), desc=\"Processing rows\", unit=\"row\"):\n",
    "        buffer_utm = row[1].geometry.buffer(distance=distance)\n",
    "        buffer = gpd.GeoSeries([buffer_utm], crs=utm_crs).to_crs(links_gdf_utm.crs)[0]\n",
    "        matched_information = information_gdf_utm[information_gdf_utm.geometry.within(buffer)]\n",
    "        socioprofessional_classes = matched_information['socioprofessional_class'].tolist()\n",
    "        close_places.append((len(socioprofessional_classes), socioprofessional_classes))\n",
    "    return close_places\n",
    "\n",
    "def process_close_count_to_tensor(close_count_list:list):\n",
    "    socio_professional_classes = [item[1] for item in close_count_list]\n",
    "    unique_classes = set([cls for sublist in socio_professional_classes for cls in sublist])\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    tensor_shape = (len(close_count_list), len(unique_classes))\n",
    "    close_homes_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "    for i, classes in enumerate(socio_professional_classes):\n",
    "        for cls in classes:\n",
    "            close_homes_tensor[i, class_to_index[cls]] += 1\n",
    "    \n",
    "    close_homes_tensor_sparse = close_homes_tensor.to_sparse()\n",
    "    return close_homes_tensor_sparse\n",
    "\n",
    "# Read all network data into a dictionary of GeoDataFrames\n",
    "def compute_result_dic_output_links():\n",
    "    result_dic = {}\n",
    "    base_network_no_policies = gdf_basecase_output_links\n",
    "    result_dic[\"base_network_no_policies\"] = base_network_no_policies\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        # print(f'Accessing folder: {subdir}')\n",
    "        # print(len(os.listdir(subdir)))\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            gdf_output_links = pio.read_output_links(file_path)\n",
    "            if (gdf_output_links is not None):\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_output_links, column_to_extend='highway', new_column_name='highway')\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_extended, column_to_extend='vol_car', new_column_name='vol_car_base_case')\n",
    "                result_dic[policy_key] = gdf_extended\n",
    "        break\n",
    "    return result_dic\n",
    "\n",
    "def calculate_averaged_results(trips_df):\n",
    "    \"\"\"Calculate average travel time and routed distance grouped by mode.\"\"\"\n",
    "    return trips_df.groupby('mode').agg(\n",
    "        total_travel_time=('travel_time', 'mean'),\n",
    "        total_routed_distance=('routed_distance', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "def compute_result_dic_mode_stats(calculate_averaged_results):\n",
    "    result_dic_mode_stats = {}\n",
    "    result_dic_mode_stats[\"base_network_no_policies\"] = gdf_basecase_average_mode_stats\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_mode_stats = pd.read_csv(file_path + '/eqasim_trips.csv', delimiter=';')\n",
    "            averaged_results = calculate_averaged_results(df_mode_stats)\n",
    "            if (averaged_results is not None):\n",
    "                result_dic_mode_stats[policy_key] = averaged_results\n",
    "        break\n",
    "    return result_dic_mode_stats\n",
    "\n",
    "def encode_modes(gdf):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': gdf['modes'].str.contains('car', case=False, na=False).astype(int),\n",
    "        'bus': gdf['modes'].str.contains('bus', case=False, na=False).astype(int),\n",
    "        'pt': gdf['modes'].str.contains('pt', case=False, na=False).astype(int),\n",
    "        'train': gdf['modes'].str.contains('train', case=False, na=False).astype(int),\n",
    "        'rail': gdf['modes'].str.contains('rail', case=False, na=False).astype(int),\n",
    "        'subway': gdf['modes'].str.contains('subway', case=False, na=False).astype(int)\n",
    "    }\n",
    "    modes_encoded = pd.DataFrame(modes_conditions)\n",
    "    return torch.tensor(modes_encoded.values, dtype=torch.float)\n",
    "\n",
    "def get_dfs(base_dir:str):\n",
    "    files = os.listdir(base_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        if base_name.startswith(\"idf_1pm_\"):\n",
    "            base_name = base_name.replace(\"idf_1pm_\", \"\")\n",
    "        var_name = base_name  # Start with the cleaned base name\n",
    "    \n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_df\"  \n",
    "                globals()[var_name] = pd.read_csv(file_path, sep=\";\")\n",
    "                print(f\"Loaded CSV file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "            \n",
    "        elif file.endswith('.gpkg'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_gdf\"  \n",
    "                layers = fiona.listlayers(file_path)\n",
    "                geodataframes = {layer: gpd.read_file(file_path, layer=layer, geometry = 'geometry', crs=\"EPSG:2154\") for layer in layers}\n",
    "                for layer, gdf in geodataframes.items():\n",
    "                # print(f\"Layer: {layer}\")\n",
    "                    gdf = gdf.to_crs(epsg=4326)\n",
    "                    globals()[var_name] = gdf\n",
    "                    print(f\"Loaded GPKG file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "    homes_gdf = globals()[\"homes_gdf\"]\n",
    "    households_df = globals()[\"households_df\"]\n",
    "    persons_df = globals()[\"persons_df\"]\n",
    "    activities_gdf = globals()[\"activities_gdf\"]\n",
    "    trips_df = globals()[\"trips_gdf\"]\n",
    "    return homes_gdf, households_df, persons_df, activities_gdf, trips_df\n",
    "\n",
    "def extract_start_end_points(geometry):\n",
    "    if len(geometry.coords) != 2:\n",
    "        raise ValueError(\"Linestring does not have exactly 2 elements.\")\n",
    "    return geometry.coords[0], geometry.coords[-1]\n",
    "\n",
    "def get_close_trips_tensor(links_gdf_input, trips_gdf_input, utm_crs, distance):\n",
    "    close_trips_count = compute_close_homes(links_gdf_input = links_gdf_input, information_gdf_input = trips_gdf_input, utm_crs = utm_crs, distance=distance)\n",
    "    close_trips_count_tensor = process_close_count_to_tensor(close_trips_count)\n",
    "    return close_trips_count, close_trips_count_tensor\n",
    "\n",
    "def get_start_and_end_gdf(trips_with_socio, crs):\n",
    "    trips_start = trips_with_socio.copy()\n",
    "    trips_end = trips_with_socio.copy()\n",
    "\n",
    "    trips_start_gdf = gpd.GeoDataFrame(\n",
    "    trips_start, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_start['start_point'].apply(lambda p: p[0]), \n",
    "        trips_start['start_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "\n",
    "    trips_end_gdf = gpd.GeoDataFrame(\n",
    "    trips_end, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_end['end_point'].apply(lambda p: p[0]), \n",
    "        trips_end['end_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "    return trips_start_gdf,trips_end_gdf\n",
    "\n",
    "result_dic_output_links = compute_result_dic_output_links()\n",
    "result_dic_mode_stats = compute_result_dic_mode_stats(calculate_averaged_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: idf_1pm_persons.csv into variable: persons_df\n",
      "Loaded GPKG file: idf_1pm_commutes.gpkg into variable: commutes_gdf\n",
      "Loaded CSV file: idf_1pm_households.csv into variable: households_df\n",
      "Loaded CSV file: idf_1pm_trips.csv into variable: trips_df\n",
      "Loaded CSV file: idf_1pm_activities.csv into variable: activities_df\n",
      "Loaded CSV file: idf_1pm_vehicle_types.csv into variable: vehicle_types_df\n",
      "Loaded GPKG file: idf_1pm_trips.gpkg into variable: trips_gdf\n",
      "Loaded GPKG file: idf_1pm_activities.gpkg into variable: activities_gdf\n",
      "Loaded CSV file: idf_1pm_vehicles.csv into variable: vehicles_df\n",
      "Loaded GPKG file: idf_1pm_homes.gpkg into variable: homes_gdf\n"
     ]
    }
   ],
   "source": [
    "base_dir_sample_sim_input = '../../../../data/pop_1pm_simulations/idf_1pm/' \n",
    "homes_gdf, households_df, persons_df, activities_gdf, trips_df = get_dfs(base_dir=base_dir_sample_sim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "links_gdf = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "links_gdf.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "population_df = pd.read_csv(\"intermediate_results/population.csv\")\n",
    "\n",
    "sorted_population_df = population_df.sort_values(by=\"id\")\n",
    "sorted_persons_df = persons_df.sort_values(by=\"person_id\")\n",
    "merged_df = pd.merge(sorted_persons_df, sorted_population_df, left_on=\"person_id\", right_on=\"id\")\n",
    "removed_some_columns = merged_df.copy()\n",
    "removed_some_columns = removed_some_columns.drop(columns=['employed_y', 'hasPtSubscription', 'householdId', 'sex_y', 'htsPersonId', 'censusPersonId', 'hasLicense', 'id', 'age_y'])\n",
    "updated_persons = removed_some_columns.copy()\n",
    "persons_with_geospatial_information = homes_gdf.merge(updated_persons, on='household_id', how='right')\n",
    "\n",
    "if not isinstance(persons_with_geospatial_information, gpd.GeoDataFrame):\n",
    "    persons_with_geospatial_information = gpd.GeoDataFrame(persons_with_geospatial_information, geometry=gpd.points_from_xy(persons_with_geospatial_information.longitude, persons_with_geospatial_information.latitude), crs= links_gdf.crs)\n",
    "\n",
    "utm_crs = 'EPSG:32631'  # UTM zone 31N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [01:27, 357.15row/s]\n",
      "Processing rows: 31216row [01:14, 419.14row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH TRIPS\n",
    "\n",
    "trips_with_socio = trips_df.merge(persons_with_geospatial_information[['person_id', 'socioprofessional_class']], on='person_id', how='left')\n",
    "trips_with_socio['start_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[0])\n",
    "trips_with_socio['end_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[1])\n",
    "\n",
    "trips_start_gdf, trips_end_gdf = get_start_and_end_gdf(trips_with_socio=trips_with_socio, crs=links_gdf.crs)\n",
    "\n",
    "close_trips_start, close_start_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf, trips_gdf_input=trips_start_gdf, utm_crs=utm_crs)\n",
    "close_trips_end, close_end_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf, trips_gdf_input=trips_end_gdf, utm_crs=utm_crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 662 differences:\n",
      "Index 221: Start (2, [3, 2]), End (1, [2])\n",
      "Index 223: Start (0, []), End (1, [3])\n",
      "Index 351: Start (1, [4]), End (0, [])\n",
      "Index 514: Start (4, [4, 5, 5, 7]), End (3, [4, 5, 7])\n",
      "Index 515: Start (3, [4, 5, 5]), End (2, [4, 5])\n",
      "Index 544: Start (4, [3, 3, 3, 5]), End (5, [8, 3, 3, 3, 5])\n",
      "Index 586: Start (9, [4, 4, 2, 8, 8, 8, 8, 5, 4]), End (10, [4, 4, 8, 2, 8, 8, 8, 8, 5, 4])\n",
      "Index 587: Start (8, [3, 2, 8, 8, 8, 8, 4, 4]), End (9, [3, 8, 2, 8, 8, 8, 8, 4, 4])\n",
      "Index 625: Start (2, [5, 4]), End (3, [5, 4, 8])\n",
      "Index 687: Start (0, []), End (1, [5])\n",
      "... and 652 more differences.\n",
      "Are the trip lists equivalent? False\n"
     ]
    }
   ],
   "source": [
    "def check_trips_equivalence(close_trips_start, close_trips_end):\n",
    "    \"\"\"\n",
    "    Check if close_trips_start and close_trips_end are equivalent.\n",
    "    \n",
    "    Args:\n",
    "    close_trips_start (list): List of tuples for start trips\n",
    "    close_trips_end (list): List of tuples for end trips\n",
    "    \n",
    "    Returns:\n",
    "    bool: True if equivalent, False otherwise\n",
    "    \"\"\"\n",
    "    if len(close_trips_start) != len(close_trips_end):\n",
    "        print(\"Lists have different lengths.\")\n",
    "        return False\n",
    "    \n",
    "    differences = []\n",
    "    for i, (start, end) in enumerate(zip(close_trips_start, close_trips_end)):\n",
    "        if start != end:\n",
    "            differences.append((i, start, end))\n",
    "    \n",
    "    if not differences:\n",
    "        print(\"The lists are identical.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Found {len(differences)} differences:\")\n",
    "        for diff in differences[:10]:  # Print first 10 differences\n",
    "            print(f\"Index {diff[0]}: Start {diff[1]}, End {diff[2]}\")\n",
    "        if len(differences) > 10:\n",
    "            print(f\"... and {len(differences) - 10} more differences.\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "are_equivalent = check_trips_equivalence(close_trips_start, close_trips_end)\n",
    "print(f\"Are the trip lists equivalent? {are_equivalent}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [00:56, 547.90row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH HOMES\n",
    "\n",
    "close_homes_count_normal = compute_close_homes(links_gdf_input = links_gdf, information_gdf_input = persons_with_geospatial_information, utm_crs = utm_crs)\n",
    "links_gdf['close_homes_count'] = close_homes_count_normal\n",
    "close_homes_tensor = process_close_count_to_tensor(close_homes_count_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 31216row [00:56, 548.20row/s]\n",
      "Processing rows: 15400row [00:55, 275.36row/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m purpose, activities \u001b[38;5;129;01min\u001b[39;00m activities_by_purpose\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m      8\u001b[0m     close_activities_count_purpose \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose_activities_count_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpurpose\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m     close_activity_count \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_close_homes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlinks_gdf_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlinks_gdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minformation_gdf_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactivities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutm_crs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutm_crs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     links_gdf[close_activities_count_purpose] \u001b[38;5;241m=\u001b[39m close_activity_count\n\u001b[1;32m     11\u001b[0m     activities_by_purpose_tensor[purpose] \u001b[38;5;241m=\u001b[39m process_close_count_to_tensor(close_activity_count)\n",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m, in \u001b[0;36mcompute_close_homes\u001b[0;34m(links_gdf_input, information_gdf_input, utm_crs, distance)\u001b[0m\n\u001b[1;32m      8\u001b[0m buffer_utm \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mgeometry\u001b[38;5;241m.\u001b[39mbuffer(distance\u001b[38;5;241m=\u001b[39mdistance)\n\u001b[1;32m      9\u001b[0m buffer \u001b[38;5;241m=\u001b[39m gpd\u001b[38;5;241m.\u001b[39mGeoSeries([buffer_utm], crs\u001b[38;5;241m=\u001b[39mutm_crs)\u001b[38;5;241m.\u001b[39mto_crs(links_gdf_utm\u001b[38;5;241m.\u001b[39mcrs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 10\u001b[0m matched_information \u001b[38;5;241m=\u001b[39m information_gdf_utm[\u001b[43minformation_gdf_utm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeometry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     11\u001b[0m socioprofessional_classes \u001b[38;5;241m=\u001b[39m matched_information[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msocioprofessional_class\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m     12\u001b[0m close_places\u001b[38;5;241m.\u001b[39mappend((\u001b[38;5;28mlen\u001b[39m(socioprofessional_classes), socioprofessional_classes))\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/geopandas/base.py:1896\u001b[0m, in \u001b[0;36mGeoPandasBase.within\u001b[0;34m(self, other, align)\u001b[0m\n\u001b[1;32m   1784\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwithin\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, align\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a ``Series`` of ``dtype('bool')`` with value ``True`` for\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;124;03m    each aligned geometry that is within `other`.\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1894\u001b[0m \u001b[38;5;124;03m    GeoSeries.contains\u001b[39;00m\n\u001b[1;32m   1895\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_binary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwithin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/geopandas/base.py:60\u001b[0m, in \u001b[0;36m_binary_op\u001b[0;34m(op, this, other, align, *args, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Binary operation on GeoSeries objects that returns a Series\"\"\"\u001b[39;00m\n\u001b[1;32m     59\u001b[0m data, index \u001b[38;5;241m=\u001b[39m _delegate_binary_method(op, this, other, align, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/series.py:443\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    441\u001b[0m manager \u001b[38;5;241m=\u001b[39m get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 443\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mSingleBlockManager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    445\u001b[0m     data \u001b[38;5;241m=\u001b[39m SingleArrayManager\u001b[38;5;241m.\u001b[39mfrom_array(data, index)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/internals/managers.py:1574\u001b[0m, in \u001b[0;36mSingleBlockManager.from_array\u001b[0;34m(cls, array, index)\u001b[0m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1570\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_array\u001b[39m(\u001b[38;5;28mcls\u001b[39m, array: ArrayLike, index: Index) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SingleBlockManager:\n\u001b[1;32m   1571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1572\u001b[0m \u001b[38;5;124;03m    Constructor for if we have an array that is not yet a Block.\u001b[39;00m\n\u001b[1;32m   1573\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1574\u001b[0m     block \u001b[38;5;241m=\u001b[39m \u001b[43mnew_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplacement\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mslice\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(block, index)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1940\u001b[0m, in \u001b[0;36mnew_block\u001b[0;34m(values, placement, ndim, klass)\u001b[0m\n\u001b[1;32m   1937\u001b[0m check_ndim(values, placement, ndim)\n\u001b[1;32m   1939\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m klass \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1940\u001b[0m     klass \u001b[38;5;241m=\u001b[39m \u001b[43mget_block_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(values)\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass(values, ndim\u001b[38;5;241m=\u001b[39mndim, placement\u001b[38;5;241m=\u001b[39mplacement)\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/internals/blocks.py:1911\u001b[0m, in \u001b[0;36mget_block_type\u001b[0;34m(values, dtype)\u001b[0m\n\u001b[1;32m   1907\u001b[0m kind \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mkind\n\u001b[1;32m   1909\u001b[0m \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[Block]\n\u001b[0;32m-> 1911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mis_sparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m   1912\u001b[0m     \u001b[38;5;66;03m# Need this first(ish) so that Sparse[datetime] is sparse\u001b[39;00m\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m ExtensionBlock\n\u001b[1;32m   1914\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, CategoricalDtype):\n",
      "File \u001b[0;32m~/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/dtypes/common.py:232\u001b[0m, in \u001b[0;36mis_sparse\u001b[0;34m(arr)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_sparse\u001b[39m(arr) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    191\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m    Check whether an array-like is a 1-D pandas sparse array.\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;124;03m    Returns `False` if the parameter has more than one dimension.\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrays\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparseDtype\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr)\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, SparseDtype)\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1053\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# DEAL WITH ACTIVITIES\n",
    "\n",
    "activities_with_socio = activities_gdf.merge(persons_with_geospatial_information[['household_id', 'socioprofessional_class']], on='household_id', how='left')\n",
    "grouped_activities = activities_with_socio.groupby('purpose')\n",
    "activities_by_purpose = {purpose: group.reset_index(drop=True) for purpose, group in grouped_activities}\n",
    "activities_by_purpose_tensor = {}\n",
    "for purpose, activities in activities_by_purpose.items():\n",
    "    close_activities_count_purpose = f\"close_activities_count_{purpose}\"\n",
    "    close_activity_count = compute_close_homes(links_gdf_input=links_gdf, information_gdf_input=activities, utm_crs=utm_crs)\n",
    "    links_gdf[close_activities_count_purpose] = close_activity_count\n",
    "    activities_by_purpose_tensor[purpose] = process_close_count_to_tensor(close_activity_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result_dic: 100%|██████████| 79/79 [00:08<00:00,  9.64dataframe/s]\n"
     ]
    }
   ],
   "source": [
    "def process_result_dic(result_dic, result_dic_mode_stats):\n",
    "    datalist = []\n",
    "    linegraph_transformation = LineGraph()\n",
    "    base_network_no_policies = result_dic.get(\"base_network_no_policies\")\n",
    "    vol_base_case = base_network_no_policies['vol_car'].values\n",
    "    capacity_base_case = base_network_no_policies['capacity'].values\n",
    "    length_base_case = base_network_no_policies['length'].values\n",
    "    freespeed_base_case = base_network_no_policies['freespeed'].values\n",
    "    modes_base_case = encode_modes(base_network_no_policies)\n",
    "    close_homes = close_homes_tensor.to_dense()\n",
    "    activities_home = activities_by_purpose_tensor['home'].to_dense()\n",
    "    activities_work = activities_by_purpose_tensor['work'].to_dense()\n",
    "    activities_education = activities_by_purpose_tensor['education'].to_dense()\n",
    "    activities_shop = activities_by_purpose_tensor['shop'].to_dense()\n",
    "    activities_leisure = activities_by_purpose_tensor['leisure'].to_dense()\n",
    "    activities_other = activities_by_purpose_tensor['other'].to_dense()\n",
    "    close_start_trips_tensor = close_start_trips_tensor.to_dense()\n",
    "    close_end_trips_tensor = close_end_trips_tensor.to_dense()\n",
    "    \n",
    "    # Initialize base edge positions\n",
    "    gdf_base = gpd.GeoDataFrame(base_network_no_policies, geometry='geometry')\n",
    "    gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "    gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "    edge_positions_base = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                     (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                    for geom in gdf_base.geometry])\n",
    "    \n",
    "    nodes = pd.concat([gdf_base['from_node'], gdf_base['to_node']]).unique()\n",
    "    node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "    gdf_base['from_idx'] = gdf_base['from_node'].map(node_to_idx)\n",
    "    gdf_base['to_idx'] = gdf_base['to_node'].map(node_to_idx)\n",
    "    edges_base = gdf_base[['from_idx', 'to_idx']].values\n",
    "    edge_positions_tensor = torch.tensor(edge_positions_base, dtype=torch.float)\n",
    "    edge_index = torch.tensor(edges_base, dtype=torch.long).t().contiguous()\n",
    "    x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index, x=x, pos=edge_positions_tensor)\n",
    "    linegraph_data = linegraph_transformation(data)\n",
    "\n",
    "    for key, df in tqdm(result_dic.items(), desc=\"Processing result_dic\", unit=\"dataframe\"):        \n",
    "        if isinstance(df, pd.DataFrame) and key != \"base_network_no_policies\":\n",
    "            gdf = gpd.GeoDataFrame(df, geometry='geometry')\n",
    "            gdf.crs = \"EPSG:2154\"  \n",
    "            gdf.to_crs(\"EPSG:4326\", inplace=True)\n",
    "            capacities_new = gdf['capacity'].values\n",
    "            capacity_reduction = capacities_new - capacity_base_case\n",
    "            highway = gdf['highway'].apply(lambda x: highway_mapping.get(x, -1)).values\n",
    "            length = gdf['length'].values\n",
    "            freespeed= gdf['freespeed'].values\n",
    "            modes = encode_modes(gdf)\n",
    "    \n",
    "            edge_car_volume_difference = gdf['vol_car'].values - vol_base_case\n",
    "            target_values = torch.tensor(edge_car_volume_difference, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "            linegraph_x = torch.tensor(np.column_stack((vol_base_case, capacity_base_case, capacities_new, capacity_reduction, \n",
    "                                                        highway, length, freespeed, length_base_case, freespeed_base_case, \n",
    "                                                        modes, modes_base_case, close_homes, \n",
    "                                                        activities_home, activities_work, activities_education, activities_shop, activities_leisure, activities_other,\n",
    "                                                        close_start_trips_tensor, close_end_trips_tensor)), dtype=torch.float)\n",
    "            \n",
    "            linegraph_data.x = linegraph_x\n",
    "            linegraph_data.y = target_values\n",
    "            \n",
    "            df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            if df_mode_stats is not None:\n",
    "                numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "                mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "                mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "                linegraph_data.mode_stats = mode_stats_tensor\n",
    "            if linegraph_data.validate(raise_on_error=True):\n",
    "                datalist.append(linegraph_data)\n",
    "            else:\n",
    "                print(\"Invalid line graph data\")\n",
    "    data_dict_list = [{'x': lg_data.x, 'edge_index': lg_data.edge_index, 'pos': lg_data.pos, 'y': lg_data.y, 'graph_attr': lg_data.mode_stats} for lg_data in datalist]\n",
    "    return data_dict_list\n",
    "\n",
    "data_processed = process_result_dic(result_dic=result_dic_output_links, result_dic_mode_stats=result_dic_mode_stats)\n",
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': tensor([[6.6275e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [9.6078e+00, 4.8000e+02, 4.8000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.4902e+00, 9.6000e+02, 9.6000e+02,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 7.9992e+03, 7.9992e+03,  ..., 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]]),\n",
       " 'edge_index': tensor([[    0,     1,     1,  ..., 31138, 31139, 31139],\n",
       "         [   19, 10935, 10936,  ..., 30278, 31138, 31139]]),\n",
       " 'pos': tensor([[-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         ...,\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836],\n",
       "         [-1.3631, -5.9836]]),\n",
       " 'y': tensor([[ 1.3725],\n",
       "         [-0.6078],\n",
       "         [ 1.5098],\n",
       "         ...,\n",
       "         [ 0.0000],\n",
       "         [ 0.0000],\n",
       "         [ 0.0000]]),\n",
       " 'graph_attr': tensor([[1.1676e+03, 3.6210e+03],\n",
       "         [1.2798e+03, 4.8323e+03],\n",
       "         [4.3310e+02, 4.3941e+03],\n",
       "         [7.8911e-01, 1.0581e+03],\n",
       "         [1.6101e+03, 5.4772e+03],\n",
       "         [1.0168e+03, 1.2207e+03]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for further processing with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_processed_single_districts = pio.process_result_dic(result_dic_single_districts)\n",
    "# torch.save(data_processed_single_districts, result_path + '_single_districts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(persons_with_homes.geometry.x, persons_with_homes.geometry.y, s=1, color='blue', alpha=0.5)\n",
    "# plt.scatter(persons_with_home_within_linear_ring.geometry.x, persons_with_home_within_linear_ring.geometry.y, s=1, color='red', alpha=0.5)\n",
    "# plt.title('Locations of Persons with Homes')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "# from shapely.geometry import MultiPolygon\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a LineString\n",
    "# line = LineString([(10, 10), (20, 10)])\n",
    "\n",
    "# # Create a buffer around the line\n",
    "# buffered_line = line.buffer(2, cap_style=\"round\")\n",
    "\n",
    "# # Plot the original line and the buffered area\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# x, y = line.xy\n",
    "# plt.plot(x, y, color='blue', label='Original Line')\n",
    "# if isinstance(buffered_line, MultiPolygon):\n",
    "#     for polygon in buffered_line:\n",
    "#         x, y = polygon.exterior.xy\n",
    "#         plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "# else:\n",
    "#     x, y = buffered_line.exterior.xy\n",
    "#     plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "\n",
    "# plt.title('Line with Buffered Area')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# def check_trips_equivalence(close_trips_start, close_trips_end):\n",
    "#     \"\"\"\n",
    "#     Check if close_trips_start and close_trips_end are equivalent.\n",
    "    \n",
    "#     Args:\n",
    "#     close_trips_start (list): List of tuples for start trips\n",
    "#     close_trips_end (list): List of tuples for end trips\n",
    "    \n",
    "#     Returns:\n",
    "#     bool: True if equivalent, False otherwise\n",
    "#     \"\"\"\n",
    "#     if len(close_trips_start) != len(close_trips_end):\n",
    "#         print(\"Lists have different lengths.\")\n",
    "#         return False\n",
    "    \n",
    "#     differences = []\n",
    "#     for i, (start, end) in enumerate(zip(close_trips_start, close_trips_end)):\n",
    "#         if start != end:\n",
    "#             differences.append((i, start, end))\n",
    "    \n",
    "#     if not differences:\n",
    "#         print(\"The lists are identical.\")\n",
    "#         return True\n",
    "#     else:\n",
    "#         print(f\"Found {len(differences)} differences:\")\n",
    "#         for diff in differences[:10]:  # Print first 10 differences\n",
    "#             print(f\"Index {diff[0]}: Start {diff[1]}, End {diff[2]}\")\n",
    "#         if len(differences) > 10:\n",
    "#             print(f\"... and {len(differences) - 10} more differences.\")\n",
    "#         return False\n",
    "\n",
    "# # Usage\n",
    "# are_equivalent = check_trips_equivalence(close_trips_start, close_trips_end)\n",
    "# print(f\"Are the trip lists equivalent? {are_equivalent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
