{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import processing_io as pio\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import shapely.wkt as wkt\n",
    "from tqdm import tqdm\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "import alphashape\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from shapely.geometry import Point\n",
    "import random\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "result_df_name = 'sim_output_1pm_capacity_reduction_10k_with_socio_info'\n",
    "# result_df_name = 'sim_output_1pm_capacity_reduction_10k'\n",
    "\n",
    "result_path = '../../../../data/datasets_simulation_outputs/' + result_df_name + '.pt'\n",
    "string_is_for_1pm = \"pop_1pm\"\n",
    "\n",
    "base_dir_sample_sim_input = '../../../../data/' + string_is_for_1pm + '_simulations/' + string_is_for_1pm + '_policies_combinations_with_normal_dist/'\n",
    "subdirs_pattern = os.path.join(base_dir_sample_sim_input, 'output_networks_*')\n",
    "subdirs = list(set(glob.glob(subdirs_pattern)))\n",
    "subdirs.sort()\n",
    "\n",
    "paris_inside_bvd_peripherique = \"../../../../data/paris_inside_bvd_per/referentiel-comptages-edit.shp\"\n",
    "gdf_paris_inside_bvd_per = gpd.read_file(paris_inside_bvd_peripherique)\n",
    "boundary_df = alphashape.alphashape(gdf_paris_inside_bvd_per, 435).exterior[0]\n",
    "linear_ring_polygon = Polygon(boundary_df)\n",
    "\n",
    "gdf_basecase_output_links = gpd.read_file('results/' + string_is_for_1pm + '_basecase_average_output_links.geojson')\n",
    "gdf_basecase_average_mode_stats = pd.read_csv('results/' + string_is_for_1pm + '_basecase_average_mode_stats.csv', delimiter=';')\n",
    "districts = gpd.read_file(\"../../../../data/visualisation/districts_paris.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is further than process_output_of_simulations_with_all_output_links_and_eqasim_info.ipynb, as it also includes more input information.\n",
    "\n",
    "Note that there is more than one strategy to deal with the fact that there are more than one district per link. We implement the strategy of stacking the information of all districts together. \n",
    "An alternative strategy would be to use the mean of the information of the districts.\n",
    "\n",
    "Process the districts manually, so that each link belongs to at most 3 districts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results\n",
    "\n",
    "Process the outputs of the simulations for further usage by GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   3%|▎         | 3/100 [00:23<12:38,  7.82s/subdir]\n",
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_18261/2795716105.py:298: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  districts['district_centroid'] = districts['geometry'].centroid\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "def compute_close_homes(links_gdf_input:pd.DataFrame, information_gdf_input:pd.DataFrame, utm_crs:str, distance:int=50):\n",
    "    links_gdf = links_gdf_input.copy()\n",
    "    information_gdf = information_gdf_input.copy()\n",
    "    close_places = []\n",
    "    links_gdf_utm = links_gdf.to_crs(utm_crs)\n",
    "    information_gdf_utm = information_gdf.to_crs(utm_crs)\n",
    "    for i, row in tqdm(enumerate(links_gdf_utm.iterrows()), desc=\"Processing rows\", unit=\"row\"):\n",
    "        buffer_utm = row[1].geometry.buffer(distance=distance)\n",
    "        buffer = gpd.GeoSeries([buffer_utm], crs=utm_crs).to_crs(links_gdf_utm.crs)[0]\n",
    "        matched_information = information_gdf_utm[information_gdf_utm.geometry.within(buffer)]\n",
    "        socioprofessional_classes = matched_information['socioprofessional_class'].tolist()\n",
    "        close_places.append((len(socioprofessional_classes), socioprofessional_classes))\n",
    "    return close_places\n",
    "\n",
    "def process_close_count_to_tensor(close_count_list: list):\n",
    "    socio_professional_classes = [item[1] for item in close_count_list]\n",
    "    unique_classes = set([2, 3, 4, 5, 6, 7, 8])\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    tensor_shape = (len(close_count_list), len(unique_classes))\n",
    "    close_homes_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "    for i, classes in enumerate(socio_professional_classes):\n",
    "        for cls in classes:\n",
    "            if cls in class_to_index:  # Ensure the class is in the predefined set\n",
    "                close_homes_tensor[i, class_to_index[cls]] += 1\n",
    "    \n",
    "    close_homes_tensor_sparse = close_homes_tensor.to_sparse()\n",
    "    return close_homes_tensor_sparse\n",
    "\n",
    "\n",
    "def calculate_averaged_results(trips_df):\n",
    "    \"\"\"Calculate average travel time and routed distance grouped by mode.\"\"\"\n",
    "    return trips_df.groupby('mode').agg(\n",
    "        total_travel_time=('travel_time', 'mean'),\n",
    "        total_routed_distance=('routed_distance', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "\n",
    "def encode_modes(gdf):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': gdf['modes'].str.contains('car', case=False, na=False).astype(int),\n",
    "        'bus': gdf['modes'].str.contains('bus', case=False, na=False).astype(int),\n",
    "        'pt': gdf['modes'].str.contains('pt', case=False, na=False).astype(int),\n",
    "        'train': gdf['modes'].str.contains('train', case=False, na=False).astype(int),\n",
    "        'rail': gdf['modes'].str.contains('rail', case=False, na=False).astype(int),\n",
    "        'subway': gdf['modes'].str.contains('subway', case=False, na=False).astype(int)\n",
    "    }\n",
    "    modes_encoded = pd.DataFrame(modes_conditions)\n",
    "    tensor_list = [torch.tensor(modes_encoded[col].values, dtype=torch.float) for col in modes_encoded.columns]\n",
    "    print(len(tensor_list))\n",
    "    return tensor_list\n",
    "    # return torch.tensor(modes_encoded.values, dtype=torch.float)\n",
    "\n",
    "\n",
    "def encode_modes_string(mode_string):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': int(\"car\" in mode_string),\n",
    "        'bus': int(\"bus\" in mode_string),\n",
    "        'pt': int(\"pt\" in mode_string),\n",
    "        'train': int(\"train\" in mode_string),\n",
    "        'rail': int(\"rail\" in mode_string),\n",
    "        'subway': int(\"subway\" in mode_string),\n",
    "    }\n",
    "    modes_encoded_tensor = torch.tensor(list(modes_conditions.values()), dtype=torch.float)\n",
    "    return modes_encoded_tensor\n",
    "\n",
    "def get_dfs(base_dir:str):\n",
    "    files = os.listdir(base_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        if base_name.startswith(\"idf_1pm_\"):\n",
    "            base_name = base_name.replace(\"idf_1pm_\", \"\")\n",
    "        var_name = base_name  # Start with the cleaned base name\n",
    "    \n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_df\"  \n",
    "                globals()[var_name] = pd.read_csv(file_path, sep=\";\")\n",
    "                print(f\"Loaded CSV file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "            \n",
    "        elif file.endswith('.gpkg'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_gdf\"  \n",
    "                layers = fiona.listlayers(file_path)\n",
    "                geodataframes = {layer: gpd.read_file(file_path, layer=layer, geometry = 'geometry', crs=\"EPSG:2154\") for layer in layers}\n",
    "                for layer, gdf in geodataframes.items():\n",
    "                # print(f\"Layer: {layer}\")\n",
    "                    gdf = gdf.to_crs(epsg=4326)\n",
    "                    globals()[var_name] = gdf\n",
    "                    print(f\"Loaded GPKG file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "    homes_gdf = globals()[\"homes_gdf\"]\n",
    "    households_df = globals()[\"households_df\"]\n",
    "    persons_df = globals()[\"persons_df\"]\n",
    "    activities_gdf = globals()[\"activities_gdf\"]\n",
    "    trips_df = globals()[\"trips_gdf\"]\n",
    "    return homes_gdf, households_df, persons_df, activities_gdf, trips_df\n",
    "\n",
    "def extract_start_end_points(geometry):\n",
    "    if len(geometry.coords) != 2:\n",
    "        raise ValueError(\"Linestring does not have exactly 2 elements.\")\n",
    "    return geometry.coords[0], geometry.coords[-1]\n",
    "\n",
    "def get_close_trips_tensor(links_gdf_input, trips_gdf_input, utm_crs, distance):\n",
    "    close_trips_count = compute_close_homes(links_gdf_input = links_gdf_input, information_gdf_input = trips_gdf_input, utm_crs = utm_crs, distance=distance)\n",
    "    close_trips_count_tensor = process_close_count_to_tensor(close_trips_count)\n",
    "    return close_trips_count, close_trips_count_tensor\n",
    "\n",
    "def get_start_and_end_gdf(trips_with_socio, crs):\n",
    "    trips_start = trips_with_socio.copy()\n",
    "    trips_end = trips_with_socio.copy()\n",
    "\n",
    "    trips_start_gdf = gpd.GeoDataFrame(\n",
    "    trips_start, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_start['start_point'].apply(lambda p: p[0]), \n",
    "        trips_start['start_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "\n",
    "    trips_end_gdf = gpd.GeoDataFrame(\n",
    "    trips_end, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_end['end_point'].apply(lambda p: p[0]), \n",
    "        trips_end['end_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "    return trips_start_gdf,trips_end_gdf\n",
    "\n",
    "def process_centroid(geom_list):\n",
    "    if not geom_list:  # Empty list\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    elif len(geom_list) == 1:\n",
    "        return [geom_list[0], np.nan, np.nan]\n",
    "    elif len(geom_list) == 2:\n",
    "        return [geom_list[0], geom_list[1], np.nan]\n",
    "    else:\n",
    "        return [geom_list[0], geom_list[1], geom_list[2]]\n",
    "    \n",
    "def extract_point_coordinates(geom_list):\n",
    "    coordinates = []\n",
    "    for geom in geom_list:\n",
    "        if isinstance(geom, Point):\n",
    "            coordinates.append((geom.x, geom.y))\n",
    "        else:\n",
    "            coordinates.append((np.nan, np.nan))\n",
    "    return coordinates\n",
    "\n",
    "def process_value_list(perimeter_list):\n",
    "    if not perimeter_list:  # Empty list\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 1:\n",
    "        return [perimeter_list[0], np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 2:\n",
    "        return [perimeter_list[0], perimeter_list[1], np.nan]\n",
    "    else:\n",
    "        return [perimeter_list[0], perimeter_list[1], perimeter_list[2]]\n",
    "    \n",
    "def compute_district_2_information_counts(district_information_counts, column_to_filter_for):\n",
    "    district_group_2_information_counts = {}\n",
    "    for district, group in district_information_counts:        \n",
    "        # ignore groups with more than one district here. \n",
    "        if len(district) == 1:\n",
    "            total_counts = 0\n",
    "            total_distributions = []\n",
    "            counts = group[column_to_filter_for].values            \n",
    "            for c in counts:\n",
    "                total_counts += c[0]\n",
    "                if c[1] is not None and len(c[1]) > 0:\n",
    "                    total_distributions.extend(c[1])\n",
    "            distribution_counts = [total_distributions.count(i) for i in range(2, 9)]   \n",
    "            district_group_2_information_counts[district] = distribution_counts\n",
    "    return district_group_2_information_counts, distribution_counts\n",
    "\n",
    "def compute_district_2_information_tensor(district_2_information_counts, distribution_counts, gdf_input):\n",
    "    district_home_counts_tensor = torch.zeros((len(gdf_input), 3, len(distribution_counts)), dtype=torch.float)\n",
    "    nan_tensor = torch.full((len(distribution_counts),), float('nan'))\n",
    "\n",
    "    for idx, row in gdf_input.iterrows():\n",
    "        district_combination = row['district']\n",
    "        district_combination_tuple = tuple(district_combination)\n",
    "        if len(district_combination_tuple) == 0:\n",
    "            district_home_counts_tensor[idx] = torch.stack([nan_tensor, nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 1:\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[district_combination_tuple]), nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 2:\n",
    "            a, b = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), nan_tensor])\n",
    "        elif len(district_combination_tuple) == 3:\n",
    "            a, b, c = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), torch.tensor(district_2_information_counts[(c,)])])\n",
    "        else:\n",
    "            print(\"NOT OK!\")\n",
    "            print(district_combination_tuple)\n",
    "    return district_home_counts_tensor\n",
    "\n",
    "def preprocess_links(links_gdf):\n",
    "    for index, row in links_gdf.iterrows():\n",
    "        if len(row['district']) >= 4:\n",
    "            row['district'].pop(random.randint(0, len(row['district']) - 1))\n",
    "    return links_gdf\n",
    "\n",
    "def find_duplicate_edges_in_gdf(gdf):\n",
    "    edge_count = defaultdict(list)\n",
    "    for idx, row in gdf.iterrows():\n",
    "        edge = tuple(sorted([row['from_node'], row['to_node']]))\n",
    "        edge_count[edge].append(idx)\n",
    "    \n",
    "    duplicates = {edge: indices for edge, indices in edge_count.items() if len(indices) > 1}\n",
    "    return duplicates\n",
    "\n",
    "\n",
    "# Read all network data into a dictionary of GeoDataFrames\n",
    "def compute_result_dic():\n",
    "    result_dic_output_links = {}\n",
    "    result_dic_eqasim_trips = {}\n",
    "    base_network_no_policies = gdf_basecase_output_links\n",
    "    result_dic_output_links[\"base_network_no_policies\"] = base_network_no_policies\n",
    "    counter = 0\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        counter += 1\n",
    "        if counter > 3:\n",
    "            break\n",
    "        # print(f'Accessing folder: {subdir}')\n",
    "        # print(len(os.listdir(subdir)))\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_output_links = read_output_links(file_path)\n",
    "            df_eqasim_trips = read_eqasim_trips(file_path)\n",
    "            if (df_output_links is not None and df_eqasim_trips is not None):\n",
    "                df_output_links.drop(columns=['geometry'], inplace=True)\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=df_output_links, column_to_extend='highway', new_column_name='highway')\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_extended, column_to_extend='vol_car', new_column_name='vol_car_base_case')\n",
    "                result_dic_output_links[policy_key] = gdf_extended\n",
    "                mode_stats = calculate_averaged_results(df_eqasim_trips)\n",
    "                result_dic_eqasim_trips[policy_key] = mode_stats\n",
    "    return result_dic_output_links, result_dic_eqasim_trips\n",
    "\n",
    "def read_output_links(folder):\n",
    "    file_path = os.path.join(folder, 'output_links.csv.gz')\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            # Read the CSV file with the correct delimiter\n",
    "            df = pd.read_csv(file_path, delimiter=';')\n",
    "            return df\n",
    "        except Exception:\n",
    "            print(\"empty data error\" + file_path)\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def read_eqasim_trips(folder):\n",
    "    file_path = os.path.join(folder, 'eqasim_trips.csv')\n",
    "    if os.path.exists(file_path):\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, delimiter=';')\n",
    "            return df\n",
    "        except Exception:\n",
    "            print(\"empty data error\" + file_path)\n",
    "            return None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def compute_result_dic_mode_stats(calculate_averaged_results):\n",
    "    result_dic_mode_stats = {}\n",
    "    result_dic_mode_stats[\"base_network_no_policies\"] = gdf_basecase_average_mode_stats\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            if (os.path.exists(file_path + 'eqasim_trips.csv')):\n",
    "                df_mode_stats = pd.read_csv(file_path + 'eqasim_trips.csv', delimiter=';')\n",
    "                averaged_results = calculate_averaged_results(df_mode_stats)\n",
    "                if (averaged_results is not None):\n",
    "                    result_dic_mode_stats[policy_key] = averaged_results\n",
    "            else:\n",
    "                print(f\"File {file_path + '/eqasim_trips.csv'} does not exist\")\n",
    "    return result_dic_mode_stats\n",
    "\n",
    "result_dic_output_links, result_dic_eqasim_trips = compute_result_dic()\n",
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "links_gdf_base = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "links_gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "districts['district_centroid'] = districts['geometry'].centroid\n",
    "links_gdf_with_districts = gpd.sjoin(links_gdf_base, districts, how='left', op='intersects')\n",
    "\n",
    "# Group by edge and aggregate the district names\n",
    "links_gdf_with_districts = links_gdf_with_districts.groupby('link').agg({\n",
    "    'from_node': 'first',\n",
    "    'to_node': 'first',\n",
    "    'length': 'first',\n",
    "    'freespeed': 'first',\n",
    "    'capacity': 'first',\n",
    "    'lanes': 'first',\n",
    "    'modes': 'first',\n",
    "    'vol_car': 'first',\n",
    "    'highway': 'first',\n",
    "    'geometry': 'first',\n",
    "    'c_ar': lambda x: list(x.dropna()),\n",
    "    'district_centroid': lambda x: list(x.dropna()),\n",
    "    'perimetre': lambda x: list(x.dropna()),\n",
    "    'surface': lambda x: list(x.dropna()),\n",
    "}).reset_index()\n",
    "gdf_now = gpd.GeoDataFrame(links_gdf_with_districts, geometry='geometry', crs=links_gdf_base.crs)\n",
    "gdf_now = gdf_now.rename(columns={'c_ar': 'district', 'perimetre': 'district_perimeter', 'surface': 'district_surface'})\n",
    "links_gdf_final = gdf_now.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   3%|▎         | 3/100 [00:23<12:41,  7.85s/subdir]\n",
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_18261/1746536096.py:6: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  districts['district_centroid'] = districts['geometry'].centroid\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "result_dic_output_links, result_dic_eqasim_trips = compute_result_dic()\n",
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "links_gdf_base = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "links_gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "districts['district_centroid'] = districts['geometry'].centroid\n",
    "links_gdf_with_districts = gpd.sjoin(links_gdf_base, districts, how='left', op='intersects')\n",
    "\n",
    "# Group by edge and aggregate the district names\n",
    "links_gdf_with_districts = links_gdf_with_districts.groupby('link').agg({\n",
    "    'from_node': 'first',\n",
    "    'to_node': 'first',\n",
    "    'length': 'first',\n",
    "    'freespeed': 'first',\n",
    "    'capacity': 'first',\n",
    "    'lanes': 'first',\n",
    "    'modes': 'first',\n",
    "    'vol_car': 'first',\n",
    "    'highway': 'first',\n",
    "    'geometry': 'first',\n",
    "    'c_ar': lambda x: list(x.dropna()),\n",
    "    'district_centroid': lambda x: list(x.dropna()),\n",
    "    'perimetre': lambda x: list(x.dropna()),\n",
    "    'surface': lambda x: list(x.dropna()),\n",
    "}).reset_index()\n",
    "gdf_now = gpd.GeoDataFrame(links_gdf_with_districts, geometry='geometry', crs=links_gdf_base.crs)\n",
    "gdf_now = gdf_now.rename(columns={'c_ar': 'district', 'perimetre': 'district_perimeter', 'surface': 'district_surface'})\n",
    "links_gdf_final = gdf_now.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert district_centroid to a tensor of size (20, 2)\n",
    "district_centroids = districts['district_centroid'].apply(lambda point: [point.x, point.y])\n",
    "district_centroids_tensor = torch.tensor(district_centroids.tolist(), dtype=torch.float32)\n",
    "\n",
    "# Ensure the tensor is of size (20, 2)\n",
    "if district_centroids_tensor.size(0) != 20 or district_centroids_tensor.size(1) != 2:\n",
    "    raise ValueError(\"The resulting tensor does not have the expected size of (20, 2)\")\n",
    "# Pad the tensor to size (20, 3, 2) by duplicating entries\n",
    "district_centroids_tensor_padded = district_centroids_tensor.unsqueeze(1).expand(-1, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of edges: 31216\n",
      "Number of edges after summarization: 25309\n",
      "Number of remaining duplicate edges: 0\n"
     ]
    }
   ],
   "source": [
    "def summarize_duplicate_edges(gdf):\n",
    "    # Check if 'vol_car' exists and print its data type\n",
    "    if 'vol_car' not in gdf.columns:\n",
    "        print(\"'vol_car' column does not exist in the dataframe\")\n",
    "        return gdf\n",
    "\n",
    "    # Create a unique identifier for each edge, regardless of direction\n",
    "    gdf['edge_id'] = gdf.apply(lambda row: tuple(sorted([row['from_node'], row['to_node']])), axis=1)\n",
    "    \n",
    "    # Group by the edge_id\n",
    "    grouped = gdf.groupby('edge_id')\n",
    "    \n",
    "    # Function to aggregate the data\n",
    "    def aggregate_edges(group):\n",
    "        # Sum the 'vol_car' column\n",
    "        vol_car_sum = group['vol_car'].sum()\n",
    "        \n",
    "        # Take other attributes from the first entry\n",
    "        first_entry = group.iloc[0]\n",
    "        \n",
    "        # Create a new row with combined data\n",
    "        combined = first_entry.copy()\n",
    "        combined['vol_car'] = vol_car_sum\n",
    "        \n",
    "        # If you want to keep track of the original directions, you can add this info\n",
    "        combined['original_directions'] = list(group[['from_node', 'to_node']].itertuples(index=False, name=None))\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    # Apply the aggregation\n",
    "    summarized_gdf = grouped.apply(aggregate_edges)\n",
    "    \n",
    "    # Reset the index and drop the temporary edge_id column\n",
    "    summarized_gdf = summarized_gdf.reset_index(drop=True)\n",
    "    summarized_gdf = summarized_gdf.drop(columns=['edge_id'])\n",
    "    \n",
    "    return summarized_gdf\n",
    "\n",
    "# Apply the summarization to links_gdf_final\n",
    "links_gdf_summarized = summarize_duplicate_edges(links_gdf_final)\n",
    "\n",
    "# Print some information about the summarization\n",
    "print(f\"Original number of edges: {len(links_gdf_final)}\")\n",
    "print(f\"Number of edges after summarization: {len(links_gdf_summarized)}\")\n",
    "\n",
    "# Check if there are any remaining duplicates\n",
    "remaining_duplicates = find_duplicate_edges_in_gdf(links_gdf_summarized)\n",
    "print(f\"Number of remaining duplicate edges: {len(remaining_duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_gdf_final = links_gdf_summarized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: idf_1pm_persons.csv into variable: persons_df\n",
      "Loaded GPKG file: idf_1pm_commutes.gpkg into variable: commutes_gdf\n",
      "Loaded CSV file: idf_1pm_households.csv into variable: households_df\n",
      "Loaded CSV file: idf_1pm_trips.csv into variable: trips_df\n",
      "Loaded CSV file: idf_1pm_activities.csv into variable: activities_df\n",
      "Loaded CSV file: idf_1pm_vehicle_types.csv into variable: vehicle_types_df\n",
      "Loaded GPKG file: idf_1pm_trips.gpkg into variable: trips_gdf\n",
      "Loaded GPKG file: idf_1pm_activities.gpkg into variable: activities_gdf\n",
      "Loaded CSV file: idf_1pm_vehicles.csv into variable: vehicles_df\n",
      "Loaded GPKG file: idf_1pm_homes.gpkg into variable: homes_gdf\n"
     ]
    }
   ],
   "source": [
    "base_dir_sample_sim_input = '../../../../data/pop_1pm_simulations/idf_1pm/' \n",
    "homes_gdf, households_df, persons_df, activities_gdf, trips_df = get_dfs(base_dir=base_dir_sample_sim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"intermediate_results/population.csv\")\n",
    "\n",
    "sorted_population_df = population_df.sort_values(by=\"id\")\n",
    "sorted_persons_df = persons_df.sort_values(by=\"person_id\")\n",
    "merged_df = pd.merge(sorted_persons_df, sorted_population_df, left_on=\"person_id\", right_on=\"id\")\n",
    "removed_some_columns = merged_df.copy()\n",
    "removed_some_columns = removed_some_columns.drop(columns=['employed_y', 'hasPtSubscription', 'householdId', 'sex_y', 'htsPersonId', 'censusPersonId', 'hasLicense', 'id', 'age_y'])\n",
    "updated_persons = removed_some_columns.copy()\n",
    "persons_with_geospatial_information = homes_gdf.merge(updated_persons, on='household_id', how='right')\n",
    "\n",
    "if not isinstance(persons_with_geospatial_information, gpd.GeoDataFrame):\n",
    "    persons_with_geospatial_information = gpd.GeoDataFrame(persons_with_geospatial_information, geometry=gpd.points_from_xy(persons_with_geospatial_information.longitude, persons_with_geospatial_information.latitude), crs= links_gdf_final.crs)\n",
    "\n",
    "utm_crs = 'EPSG:32631'  # UTM zone 31N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEAL WITH TRIPS\n",
    "\n",
    "# trips_with_socio = trips_df.merge(persons_with_geospatial_information[['person_id', 'socioprofessional_class']], on='person_id', how='left')\n",
    "# trips_with_socio['start_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[0])\n",
    "# trips_with_socio['end_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[1])\n",
    "\n",
    "# trips_start_gdf, trips_end_gdf = get_start_and_end_gdf(trips_with_socio=trips_with_socio, crs=links_gdf_final.crs)\n",
    "\n",
    "# # Create tensors for each combination of \"preceding_purpose\" and \"following_purpose\"\n",
    "# unique_purposes = trips_with_socio['preceding_purpose'].unique()\n",
    "# close_start_trips_tensor_dict = {}\n",
    "# close_start_trips_dict = {}\n",
    "# close_end_trips_tensor_dict = {}\n",
    "# close_end_trips_dict = {}\n",
    "\n",
    "# for preceding_purpose in tqdm(unique_purposes, desc=\"Processing preceding purposes\", unit=\"purpose\"):\n",
    "#     for following_purpose in tqdm(unique_purposes, desc=\"Processing following purposes\", unit=\"purpose\"):\n",
    "#         if preceding_purpose != following_purpose:\n",
    "#             filtered_trips = trips_with_socio[(trips_with_socio['preceding_purpose'] == preceding_purpose) & (trips_with_socio['following_purpose'] == following_purpose)]\n",
    "#             if not filtered_trips.empty:\n",
    "#                 filtered_trips_start_gdf, filtered_trips_end_gdf = get_start_and_end_gdf(trips_with_socio=filtered_trips, crs=links_gdf_final.crs)\n",
    "        \n",
    "#                 tensor_key = f\"{preceding_purpose}_{following_purpose}\"\n",
    "#                 string_trips_start = \"trips_start_\" + tensor_key\n",
    "#                 string_trips_end = \"trips_end_\" + tensor_key\n",
    "                \n",
    "#                 close_start_trips, close_start_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_start_gdf, utm_crs=utm_crs, distance=50)        \n",
    "#                 close_start_trips_tensor_dict[tensor_key] = close_start_trips_tensor\n",
    "#                 close_start_trips_dict[tensor_key] = close_start_trips\n",
    "#                 links_gdf_final[string_trips_start ] = close_start_trips\n",
    "                \n",
    "#                 close_end_trips, close_end_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_end_gdf, utm_crs=utm_crs, distance=50)\n",
    "#                 close_end_trips_tensor_dict[tensor_key] = close_end_trips_tensor\n",
    "#                 close_end_trips_dict[tensor_key] = close_end_trips\n",
    "#                 links_gdf_final[string_trips_end] = close_end_trips\n",
    "\n",
    "# for key, tensor in close_start_trips_tensor_dict.items():\n",
    "#     print(f\"Size of tensor for {key}: {tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for key, tensor in close_end_trips_tensor_dict.items():\n",
    "# #     print(f\"Size of tensor for {key}: {tensor.size()}\")\n",
    "    \n",
    "# for key, tensor in close_end_trips_dict.items():\n",
    "#     print(f\"Size of tensor for {key}: {len(tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 25309row [00:56, 449.81row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH HOMES\n",
    "\n",
    "links_gdf_final.crs = \"EPSG:4326\"\n",
    "\n",
    "close_homes_count_normal = compute_close_homes(links_gdf_input = links_gdf_final, information_gdf_input = persons_with_geospatial_information, utm_crs = utm_crs)\n",
    "links_gdf_final['close_homes_count'] = close_homes_count_normal\n",
    "close_homes_tensor = process_close_count_to_tensor(close_homes_count_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25309, 7])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "close_homes_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEAL WITH ACTIVITIES\n",
    "\n",
    "# activities_with_socio = activities_gdf.merge(persons_with_geospatial_information[['household_id', 'socioprofessional_class']], on='household_id', how='left')\n",
    "# grouped_activities = activities_with_socio.groupby('purpose')\n",
    "# activities_by_purpose = {purpose: group.reset_index(drop=True) for purpose, group in grouped_activities}\n",
    "# activities_by_purpose_tensor = {}\n",
    "# for purpose, activities in activities_by_purpose.items():\n",
    "#     close_activities_count_purpose = f\"close_activities_count_{purpose}\"\n",
    "#     close_activity_count = compute_close_homes(links_gdf_input=links_gdf_final, information_gdf_input=activities, utm_crs=utm_crs)\n",
    "#     links_gdf_final[close_activities_count_purpose] = close_activity_count\n",
    "#     activities_by_purpose_tensor[purpose] = process_close_count_to_tensor(close_activity_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRICTS GENERAL \n",
    "\n",
    "# links_gdf_final['districts_tuple'] = links_gdf_final['district'].apply(lambda x: tuple(x))\n",
    "# district_tuples = links_gdf_final.groupby('districts_tuple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT CENTROIDS\n",
    "\n",
    "centroid_distances = np.array([\n",
    "    process_centroid(geom_list)\n",
    "    for geom_list in links_gdf_final['district_centroid']\n",
    "])\n",
    "\n",
    "# Process the centroids\n",
    "centroid_distance_with_coordinates = np.array([\n",
    "    extract_point_coordinates(geom_list)\n",
    "    for geom_list in centroid_distances\n",
    "])\n",
    "district_centroids_tensor = torch.tensor(centroid_distance_with_coordinates, dtype=torch.float)\n",
    "\n",
    "# # FIND DISTRICT POLYGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT HOME COUNTS \n",
    "\n",
    "# def compute_district_2_information_counts(district_information_counts, column_to_filter_for):\n",
    "#     district_group_2_information_counts = {}\n",
    "#     for district, group in district_information_counts:        \n",
    "#         # ignore groups with more than one district here. \n",
    "#         if len(district) == 1:\n",
    "#             total_counts = 0\n",
    "#             total_distributions = []\n",
    "#             counts = group[column_to_filter_for].values            \n",
    "#             for c in counts:\n",
    "#                 total_counts += c[0]\n",
    "#                 if c[1] is not None and len(c[1]) > 0:\n",
    "#                     total_distributions.extend(c[1])\n",
    "#             distribution_counts = [total_distributions.count(i) for i in range(2, 9)]   \n",
    "#             district_group_2_information_counts[district] = distribution_counts\n",
    "#     return district_group_2_information_counts, distribution_counts\n",
    "\n",
    "# district_2_home_counts, distribution_counts = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = 'close_homes_count')\n",
    "# district_home_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_home_counts, distribution_counts=distribution_counts, gdf_input=links_gdf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT ACTIVITIES COUNTS\n",
    "\n",
    "# activity_2_district_tensor = {}\n",
    "# for purpose in activities_by_purpose_tensor.keys():\n",
    "#     district_2_activity_counts, distribution_counts_activity = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for='close_activities_count_' + purpose)\n",
    "#     district_activitiy_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_activity_counts, distribution_counts=distribution_counts_activity, gdf_input=links_gdf_final)\n",
    "#     activity_2_district_tensor[purpose] = district_activitiy_counts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = torch.nonzero(~torch.isnan(activity_2_district_tensor['education'][:, 1, 0])).squeeze()\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT TRIPS START AND END\n",
    "\n",
    "# unique_purposes = trips_with_socio['preceding_purpose'].unique()\n",
    "# district_close_start_trips_tensor_dict = {}\n",
    "# district_close_end_trips_tensor_dict = {}\n",
    "\n",
    "# for preceding_purpose in tqdm(unique_purposes, desc=\"Processing preceding purposes\", unit=\"purpose\"):\n",
    "#     for following_purpose in tqdm(unique_purposes, desc=\"Processing following purposes\", unit=\"purpose\"):\n",
    "#         if preceding_purpose != following_purpose:\n",
    "#             filtered_trips = trips_with_socio[(trips_with_socio['preceding_purpose'] == preceding_purpose) & (trips_with_socio['following_purpose'] == following_purpose)]\n",
    "#             if not filtered_trips.empty:\n",
    "#                 tensor_key = f\"{preceding_purpose}_{following_purpose}\"\n",
    "#                 print(tensor_key)\n",
    "\n",
    "#                 string_trips_start_purpose = \"trips_start_\" + tensor_key\n",
    "#                 string_trips_end_purpose = \"trips_start_\" + tensor_key\n",
    "                \n",
    "#                 district_2_start_trips, start_trip_distributions = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = string_trips_start_purpose)\n",
    "#                 district_trip_starts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_start_trips, distribution_counts=start_trip_distributions, gdf_input=links_gdf_final)\n",
    "#                 district_close_start_trips_tensor_dict[string_trips_start_purpose] = district_trip_starts_tensor\n",
    "                \n",
    "#                 district_2_end_trips, end_trip_distributions = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = string_trips_end_purpose)\n",
    "#                 district_trip_end_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_end_trips, distribution_counts=end_trip_distributions, gdf_input=links_gdf_final)\n",
    "#                 district_close_end_trips_tensor_dict[string_trips_end_purpose] = district_trip_end_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25309, 3, 2])\n",
      "torch.Size([25309, 2])\n",
      "torch.Size([25309, 2])\n"
     ]
    }
   ],
   "source": [
    "# PROCESS LINK GEOMETRIES\n",
    "\n",
    "edge_midpoints = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                    (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                for geom in links_gdf_final.geometry])\n",
    "\n",
    "nodes = pd.concat([links_gdf_final['from_node'], links_gdf_final['to_node']]).unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "links_gdf_final['from_idx'] = links_gdf_final['from_node'].map(node_to_idx)\n",
    "links_gdf_final['to_idx'] = links_gdf_final['to_node'].map(node_to_idx)\n",
    "edges_base = links_gdf_final[['from_idx', 'to_idx']].values\n",
    "edge_midpoint_tensor = torch.tensor(edge_midpoints, dtype=torch.float)\n",
    "\n",
    "# Initialize start and end points\n",
    "start_points = np.array([geom.coords[0] for geom in links_gdf_final.geometry])\n",
    "end_points = np.array([geom.coords[-1] for geom in links_gdf_final.geometry])\n",
    "\n",
    "# Convert to tensors\n",
    "edge_start_point_tensor = torch.tensor(start_points, dtype=torch.float)\n",
    "edge_end_point_tensor = torch.tensor(end_points, dtype=torch.float)\n",
    "\n",
    "edge_start_end_tensor = torch.stack((edge_start_point_tensor, edge_end_point_tensor), dim=1)\n",
    "\n",
    "stacked_edge_geometries_tensor = torch.stack([edge_start_point_tensor, edge_end_point_tensor, edge_midpoint_tensor], dim=1)\n",
    "\n",
    "print(stacked_edge_geometries_tensor.shape)\n",
    "print(edge_start_point_tensor.shape)\n",
    "print(edge_start_point_tensor.to_dense().shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all entries of trips in one tensor\n",
    "# close_start_trips_tensors = [tensor.to_dense() for tensor in close_start_trips_tensor_dict.values()]\n",
    "# stacked_close_start_trips_tensor = torch.cat(close_start_trips_tensors, dim=1)\n",
    "\n",
    "# close_end_trips_tensors = [tensor.to_dense() for tensor in close_end_trips_tensor_dict.values()]\n",
    "# stacked_close_end_trips_tensor = torch.cat(close_end_trips_tensors, dim=1)\n",
    "\n",
    "# district_start_trips_tensors = [tensor.to_dense() for tensor in district_close_start_trips_tensor_dict.values()]\n",
    "# stacked_district_start_trips_tensor = torch.cat(district_start_trips_tensors, dim=1)\n",
    "\n",
    "# district_end_trips_tensors = [tensor.to_dense() for tensor in district_close_end_trips_tensor_dict.values()]\n",
    "# stacked_district_end_trips_tensor = torch.cat(district_end_trips_tensors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_district_information(links_gdf, tensors_edge_information):\n",
    "    \n",
    "    # Assuming tensors_edge_information is a list of tensors\n",
    "    vol_base_case = tensors_edge_information[0]  # Adjust index if needed\n",
    "    capacities_base = tensors_edge_information[1]  \n",
    "    capacities_new = tensors_edge_information[2] \n",
    "    capacity_reduction = tensors_edge_information[3]  \n",
    "    freespeed_base = tensors_edge_information[4]\n",
    "    freespeed = tensors_edge_information[5]\n",
    "    highway = tensors_edge_information[6]\n",
    "    length = tensors_edge_information[7]\n",
    "    cars_allowed = tensors_edge_information[8]\n",
    "    bus_allowed = tensors_edge_information[9]\n",
    "    pt_allowed = tensors_edge_information[10]\n",
    "    train_allowed = tensors_edge_information[11]\n",
    "    rail_allowed = tensors_edge_information[12]\n",
    "    subway_allowed = tensors_edge_information[13]\n",
    "    \n",
    "    district_info = {}\n",
    "            \n",
    "    # modes_str = \"\"\n",
    "    for idx, row in links_gdf.iterrows():\n",
    "        districts = row['district']\n",
    "        modes = row['modes']\n",
    "        # modes_str += modes + \",\"\n",
    "        for district in districts:\n",
    "            if district not in district_info:\n",
    "                district_info[district] = {\n",
    "                    'vol_base_case': 0,\n",
    "                    'capacity_base': 0,\n",
    "                    'capacity_new': 0,\n",
    "                    'capacity_reduction': 0,\n",
    "                    'freespeed_base_sum': 0,\n",
    "                    'freespeed_base_count': 0,\n",
    "                    'freespeed_sum': 0,\n",
    "                    'freespeed_count': 0,\n",
    "                    'highway_sum': 0,\n",
    "                    'highway_count': 0,\n",
    "                    'length': 0,\n",
    "                    'edge_count': 0,\n",
    "                    'cars_allowed': 0,\n",
    "                    'bus_allowed': 0,\n",
    "                    'pt_allowed': 0,\n",
    "                    'train_allowed': 0,\n",
    "                    'rail_allowed': 0,\n",
    "                    'subway_allowed': 0,\n",
    "                }\n",
    "            \n",
    "            if \"car\" in modes:\n",
    "                district_info[district]['capacity_base'] += capacities_base[idx].item()\n",
    "                district_info[district]['capacity_new'] += capacities_new[idx].item()\n",
    "                district_info[district]['capacity_reduction'] += capacity_reduction[idx].item()\n",
    "                district_info[district]['freespeed_sum'] += freespeed[idx].item()\n",
    "                district_info[district]['freespeed_base_sum'] += freespeed_base[idx].item()\n",
    "                district_info[district]['freespeed_base_count'] += 1\n",
    "                district_info[district]['freespeed_count'] += 1\n",
    "            \n",
    "            district_info[district]['length'] += length[idx].item()\n",
    "\n",
    "            highway_value = highway_mapping.get(row['highway'], -1)\n",
    "            district_info[district]['highway_sum'] += highway_value\n",
    "            district_info[district]['highway_count'] += 1\n",
    "            district_info[district]['edge_count'] += 1\n",
    "            \n",
    "            district_info[district]['cars_allowed'] += cars_allowed[idx].item()\n",
    "            district_info[district]['bus_allowed'] += bus_allowed[idx].item()\n",
    "            district_info[district]['pt_allowed'] += pt_allowed[idx].item()\n",
    "            district_info[district]['train_allowed'] += train_allowed[idx].item()\n",
    "            district_info[district]['rail_allowed'] += rail_allowed[idx].item()\n",
    "            district_info[district]['subway_allowed'] += subway_allowed[idx].item()\n",
    "\n",
    "    # Compute averages \n",
    "    for district in district_info:\n",
    "        district_info[district]['freespeed_base'] = district_info[district]['freespeed_base_sum'] / district_info[district]['freespeed_base_count']\n",
    "        district_info[district]['freespeed'] = district_info[district]['freespeed_sum'] / district_info[district]['freespeed_count']\n",
    "        district_info[district]['highway'] = district_info[district]['highway_sum'] / district_info[district]['highway_count']\n",
    "            \n",
    "    # Sort districts by their identifiers\n",
    "    districts = sorted(district_info.keys())\n",
    "    \n",
    "    vol_base_case_tensor = torch.tensor([district_info[d]['vol_base_case'] for d in districts])\n",
    "    capacity_base_tensor = torch.tensor([district_info[d]['capacity_base'] for d in districts])\n",
    "    capacity_new_tensor = torch.tensor([district_info[d]['capacity_new'] for d in districts])\n",
    "    capacity_reduction_tensor = torch.tensor([district_info[d]['capacity_reduction'] for d in districts])\n",
    "    \n",
    "    length_tensor = torch.tensor([district_info[d]['length'] for d in districts])\n",
    "    edge_count_tensor = torch.tensor([district_info[d]['edge_count'] for d in districts])\n",
    "    highway_tensor = torch.tensor([district_info[d]['highway'] for d in districts])\n",
    "    freespeed_base_tensor = torch.tensor([district_info[d]['freespeed_base'] for d in districts])\n",
    "    freespeed_tensor = torch.tensor([district_info[d]['freespeed'] for d in districts])\n",
    "    # allowed_modes_tensor = torch.stack([district_info[d]['allowed_modes'] for d in districts])\n",
    "    \n",
    "    cars_allowed_tensor = torch.tensor([district_info[d]['cars_allowed'] for d in districts])\n",
    "    bus_allowed_tensor = torch.tensor([district_info[d]['bus_allowed'] for d in districts])\n",
    "    pt_allowed_tensor = torch.tensor([district_info[d]['pt_allowed'] for d in districts])\n",
    "    train_allowed_tensor = torch.tensor([district_info[d]['train_allowed'] for d in districts])\n",
    "    rail_allowed_tensor = torch.tensor([district_info[d]['rail_allowed'] for d in districts])\n",
    "    subway_allowed_tensor = torch.tensor([district_info[d]['subway_allowed'] for d in districts])\n",
    "\n",
    "    return {\n",
    "        'districts': districts,\n",
    "        'vol_base_case': vol_base_case_tensor,\n",
    "        'capacity_base': capacity_base_tensor,\n",
    "        'capacity_new': capacity_new_tensor,\n",
    "        'capacity_reduction': capacity_reduction_tensor,\n",
    "        'length': length_tensor,\n",
    "        'highway': highway_tensor,\n",
    "        'freespeed_base': freespeed_base_tensor,\n",
    "        'freespeed': freespeed_tensor,\n",
    "        # 'allowed_modes': allowed_modes_tensor,\n",
    "        'cars_allowed': cars_allowed_tensor,\n",
    "        'bus_allowed': bus_allowed_tensor,\n",
    "        'pt_allowed': pt_allowed_tensor,\n",
    "        'train_allowed': train_allowed_tensor,\n",
    "        'rail_allowed': rail_allowed_tensor,\n",
    "        'subway_allowed': subway_allowed_tensor,\n",
    "        'edge_count': edge_count_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_combined_tensor(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed):\n",
    "    edge_tensors = [\n",
    "                torch.tensor(vol_base_case), \n",
    "                torch.tensor(capacity_base_case), \n",
    "                torch.tensor(capacities_new), \n",
    "                torch.tensor(capacity_reduction), \n",
    "                torch.tensor(freespeed_base_case), \n",
    "                torch.tensor(freespeed), \n",
    "                torch.tensor(highway), \n",
    "                torch.tensor(length), \n",
    "                allowed_modes[0],\n",
    "                allowed_modes[1],\n",
    "                allowed_modes[2],\n",
    "                allowed_modes[3],\n",
    "                allowed_modes[4],\n",
    "                allowed_modes[5],\n",
    "            ]\n",
    "\n",
    "    district_info = aggregate_district_information(links_gdf=gdf, tensors_edge_information= edge_tensors)\n",
    "    district_tensors = [\n",
    "                district_info['vol_base_case'],\n",
    "                district_info['capacity_base'],\n",
    "                district_info['capacity_new'],\n",
    "                district_info['capacity_reduction'],\n",
    "                district_info['freespeed_base'],\n",
    "                district_info['freespeed'],\n",
    "                district_info['highway'],\n",
    "                district_info['length'],\n",
    "                district_info['cars_allowed'],\n",
    "                district_info['bus_allowed'],\n",
    "                district_info['pt_allowed'],\n",
    "                district_info['train_allowed'],\n",
    "                district_info['rail_allowed'],\n",
    "                district_info['subway_allowed'],\n",
    "            ]\n",
    "    stacked_tensor1 = torch.stack(edge_tensors, dim=1)  # Shape: (25209, 14)\n",
    "    stacked_tensor2 = torch.stack(district_tensors, dim=1)  # Shape: (20, 14)\n",
    "    combined_tensor = torch.cat((stacked_tensor1, stacked_tensor2), dim=0)  # Shape: (25229, 14)\n",
    "    return district_info,combined_tensor\n",
    "\n",
    "def compute_node_attributes(district_info, len_edges):\n",
    "    num_edge_nodes = len_edges\n",
    "    num_district_nodes = len(district_info['districts'])\n",
    "    node_type_feature = torch.zeros((num_edge_nodes + num_district_nodes, 1), dtype=torch.long)\n",
    "    node_type_feature[num_edge_nodes:, :] = 1\n",
    "    return node_type_feature\n",
    "\n",
    "def compute_edge_attributes(district_info, linegraph_data, len_edges):\n",
    "    district_node_offset = len_edges\n",
    "    edge_to_district_edges = []\n",
    "    for idx, row in links_gdf_final.iterrows():\n",
    "        for district in row['district']:\n",
    "            district_idx = district_info['districts'].index(district) + district_node_offset\n",
    "            edge_to_district_edges.append([idx, district_idx])\n",
    "            edge_to_district_edges.append([district_idx, idx])  # Add reverse edge for undirected graph  # TODO is one way enough ? \n",
    "    edge_to_district_index = torch.tensor(edge_to_district_edges, dtype=torch.long).t()\n",
    "    linegraph_data.edge_index = torch.cat([linegraph_data.edge_index, edge_to_district_index], dim=1)\n",
    "    edge_to_district_index = torch.tensor(edge_to_district_edges, dtype=torch.long).t()\n",
    "    edge_to_district_attr = torch.ones((edge_to_district_index.shape[1], 1), dtype=torch.long)\n",
    "    return edge_to_district_index, edge_to_district_attr\n",
    "\n",
    "def compute_target_tensor(vol_base_case, gdf, district_info):\n",
    "    edge_car_volume_difference = gdf['vol_car'].values - vol_base_case\n",
    "    district_car_volume_difference = []\n",
    "    for district in district_info['districts']:\n",
    "        district_edges = gdf[gdf['district'].apply(lambda x: district in x)]\n",
    "        district_volume_diff = district_edges['vol_car'].sum() - district_edges['vol_car_base_case'].sum()\n",
    "        district_car_volume_difference.append(district_volume_diff)\n",
    "    district_car_volume_difference = torch.tensor(district_car_volume_difference, dtype=torch.float).unsqueeze(1)\n",
    "    target_values = torch.cat([torch.tensor(edge_car_volume_difference, dtype=torch.float).unsqueeze(1), district_car_volume_difference], dim=0)\n",
    "    return target_values\n",
    "\n",
    "def get_basic_edge_attributes(capacity_base_case, gdf):\n",
    "    capacities_new = np.where(gdf['modes'].str.contains('car'), gdf['capacity'], 0)\n",
    "    capacity_reduction = capacities_new - capacity_base_case\n",
    "    highway = gdf['highway'].apply(lambda x: highway_mapping.get(x, -1)).values\n",
    "    freespeed = np.where(gdf['modes'].str.contains('car'), gdf['freespeed'], 0)\n",
    "    return capacities_new,capacity_reduction,highway,freespeed\n",
    "\n",
    "def prepare_gdf(df):\n",
    "    gdf = links_gdf_final[['link', 'district', 'geometry']].merge(df, on='link', how='left')\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n",
    "    gdf.crs = links_gdf_final.crs\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result_dic:   0%|          | 1/251 [00:01<05:56,  1.43s/dataframe]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Call the function\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mprocess_result_dic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_dic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dic_output_links\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_dic_mode_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_dic_eqasim_trips\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresult_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[26], line 29\u001b[0m, in \u001b[0;36mprocess_result_dic\u001b[0;34m(result_dic, result_dic_mode_stats, save_path, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m linegraph_data\u001b[38;5;241m.\u001b[39mnum_nodes \u001b[38;5;241m=\u001b[39m combined_tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# add edge attributes: 1 if edge to district, 0 if edge to edge\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m edge_to_district_index, edge_to_district_attr \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_edge_attributes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistrict_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinegraph_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlen_edges\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m linegraph_data\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     linegraph_data\u001b[38;5;241m.\u001b[39medge_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((linegraph_data\u001b[38;5;241m.\u001b[39medge_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m edge_to_district_index\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[0;32mIn[25], line 59\u001b[0m, in \u001b[0;36mcompute_edge_attributes\u001b[0;34m(district_info, linegraph_data, len_edges)\u001b[0m\n\u001b[1;32m     57\u001b[0m linegraph_data\u001b[38;5;241m.\u001b[39medge_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([linegraph_data\u001b[38;5;241m.\u001b[39medge_index, edge_to_district_index], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     58\u001b[0m edge_to_district_index \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(edge_to_district_edges, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\u001b[38;5;241m.\u001b[39mt()\n\u001b[0;32m---> 59\u001b[0m edge_to_district_attr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones((\u001b[43medge_to_district_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[38;5;241m1\u001b[39m), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m edge_to_district_index, edge_to_district_attr\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "def process_result_dic(result_dic, result_dic_mode_stats, save_path=None, batch_size=500):\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    datalist = []\n",
    "    linegraph_transformation = LineGraph()\n",
    "    \n",
    "    vol_base_case = links_gdf_final['vol_car'].values\n",
    "    capacity_base_case = np.where(links_gdf_final['modes'].str.contains('car'), links_gdf_final['capacity'], 0)\n",
    "    length = links_gdf_final['length'].values\n",
    "    freespeed_base_case = links_gdf_final['freespeed'].values\n",
    "    allowed_modes = encode_modes(links_gdf_final)\n",
    "    edge_index = torch.tensor(edges_base, dtype=torch.long).t().contiguous()\n",
    "    x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index, x=x)\n",
    "    \n",
    "    batch_counter = 0\n",
    "    for key, df in tqdm(result_dic.items(), desc=\"Processing result_dic\", unit=\"dataframe\"):   \n",
    "        if isinstance(df, pd.DataFrame) and key != \"base_network_no_policies\":\n",
    "            gdf = prepare_gdf(df)\n",
    "            len_edges = len(gdf)\n",
    "            \n",
    "            capacities_new, capacity_reduction, highway, freespeed = get_basic_edge_attributes(capacity_base_case, gdf)\n",
    "            district_info, combined_tensor = compute_combined_tensor(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed)\n",
    "            \n",
    "            linegraph_data = linegraph_transformation(data)\n",
    "            linegraph_data.x = combined_tensor\n",
    "            linegraph_data.num_nodes = combined_tensor.shape[0]\n",
    "        \n",
    "            # add edge attributes: 1 if edge to district, 0 if edge to edge\n",
    "            edge_to_district_index, edge_to_district_attr = compute_edge_attributes(district_info, linegraph_data, len_edges)\n",
    "            if linegraph_data.edge_attr is None:\n",
    "                linegraph_data.edge_attr = torch.zeros((linegraph_data.edge_index.shape[1] - edge_to_district_index.shape[1], 1), dtype=torch.long)\n",
    "            linegraph_data.edge_attr = torch.cat([linegraph_data.edge_attr, edge_to_district_attr], dim=0)\n",
    "\n",
    "            # add node attributes: 1 if district, 0 if edge\n",
    "            node_type_feature = compute_node_attributes(district_info, len_edges)\n",
    "            linegraph_data.x = torch.cat([linegraph_data.x, node_type_feature], dim=1)\n",
    "            \n",
    "            linegraph_data.pos = torch.cat([stacked_edge_geometries_tensor, district_centroids_tensor_padded], dim=0)\n",
    "            linegraph_data.y = compute_target_tensor(vol_base_case, gdf, district_info)\n",
    "                        \n",
    "            df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            if df_mode_stats is not None:\n",
    "                numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "                mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "                mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "                linegraph_data.mode_stats = mode_stats_tensor\n",
    "            \n",
    "            if linegraph_data.validate(raise_on_error=True):\n",
    "                datalist.append(linegraph_data)\n",
    "                batch_counter += 1\n",
    "\n",
    "                # Save intermediate result every batch_size data points\n",
    "                if batch_counter % batch_size == 0:\n",
    "                    batch_index = batch_counter // batch_size\n",
    "                    torch.save(datalist, os.path.join(save_path, f'datalist_batch_{batch_index}.pt'))\n",
    "                    datalist = []  # Reset datalist for the next batch\n",
    "            else:\n",
    "                print(\"Invalid line graph data\")\n",
    "    \n",
    "    # Save any remaining data points\n",
    "    if datalist:\n",
    "        batch_index = (batch_counter // batch_size) + 1\n",
    "        torch.save(datalist, os.path.join(save_path, f'datalist_batch_{batch_index}.pt'))\n",
    "    return\n",
    "\n",
    "# Call the function\n",
    "process_result_dic(result_dic=result_dic_output_links, result_dic_mode_stats=result_dic_eqasim_trips, save_path=result_path, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors_districts_information = []\n",
    "            \n",
    "            # tensors_with_sociographic_information = [close_homes, \n",
    "            #     activities_home, activities_work, activities_education, activities_shop, activities_leisure, activities_other,\n",
    "            #     stacked_close_start_trips_tensor, stacked_close_end_trips_tensor]\n",
    "                \n",
    "            # tensors_districts_with_sociographic_information = [\n",
    "            #     district_home_counts,\n",
    "            #     district_activities_home, district_activities_work, district_activities_education, district_activities_shop, district_activities_leisure, district_activities_other,\n",
    "            #     stacked_district_start_trips_tensor, stacked_district_end_trips_tensor]\n",
    "            \n",
    "            # linegraph_x_districts = torch.tensor(np.column_stack(tensors_districts_information), dtype=torch.float)\n",
    "            # stack linegraph_x and linegraph_x_districts\n",
    "            \n",
    "            \n",
    "            # linegraph_pos = torch.tensor(np.column_stack(edge_midpoint_t), dtype=torch.float)\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            # for i, t in enumerate(tensors_edge_information):\n",
    "            #     print(f\"Shape of tensor {i}: {t.shape}\")\n",
    "\n",
    "            # linegraph_data.x = linegraph_x\n",
    "            # linegraph_data.pos = linegraph_pos\n",
    "            # linegraph_data.y = target_values\n",
    "            \n",
    "            # df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            # if df_mode_stats is not None:\n",
    "            #     numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "            #     mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "            #     mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "            #     linegraph_data.mode_stats = mode_stats_tensor\n",
    "            # if linegraph_data.validate(raise_on_error=True):\n",
    "            #     datalist.append(linegraph_data)\n",
    "            # else:\n",
    "            #     print(\"Invalid line graph data\")\n",
    "            # print(\"LG DATA EDGE INDEx\")\n",
    "            # for lg_data in datalist:\n",
    "            #     print(lg_data.edge_index.shape)\n",
    "            #     print(lg_data.edge_index)\n",
    "                \n",
    "            # print(lg_data.edge_index.shape for lg_data in datalist)\n",
    "\n",
    "            # print(lg_data.edge_index for lg_data in datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [2.0037e+01, 2.0037e+01, 2.0037e+01, 2.0037e+01, 2.0037e+01,\n",
       "          2.0037e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [9.7760e+00, 9.7760e+00, 9.7760e+00, 9.7760e+00, 9.7760e+00,\n",
       "          9.7760e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [8.3333e+00, 8.3333e+00, 8.3333e+00, 8.3333e+00, 8.3333e+00,\n",
       "          8.3333e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.7440e+06, 2.7440e+06, 2.7440e+06, 2.7440e+06, 2.7440e+06,\n",
       "          2.7440e+06],\n",
       "         [2.7033e+06, 2.7033e+06, 2.7033e+06, 2.7033e+06, 2.7033e+06,\n",
       "          2.7033e+06],\n",
       "         ...,\n",
       "         [1.5976e+05, 1.5976e+05, 1.5976e+05, 1.5976e+05, 1.5976e+05,\n",
       "          1.5976e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [3.4090e+06, 3.4090e+06, 3.4090e+06, 3.4090e+06, 3.4090e+06,\n",
       "          3.4090e+06],\n",
       "         [2.3307e+06, 2.3307e+06, 2.3307e+06, 2.3307e+06, 2.3307e+06,\n",
       "          2.3307e+06],\n",
       "         ...,\n",
       "         [1.4539e+05, 1.4539e+05, 1.4539e+05, 1.4539e+05, 1.4539e+05,\n",
       "          1.4539e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.8598e+06, 2.8598e+06, 2.8598e+06, 2.8598e+06, 2.8598e+06,\n",
       "          2.8598e+06],\n",
       "         [2.8126e+06, 2.8126e+06, 2.8126e+06, 2.8126e+06, 2.8126e+06,\n",
       "          2.8126e+06],\n",
       "         ...,\n",
       "         [1.2795e+05, 1.2795e+05, 1.2795e+05, 1.2795e+05, 1.2795e+05,\n",
       "          1.2795e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed[0]['x']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for further processing with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(persons_with_homes.geometry.x, persons_with_homes.geometry.y, s=1, color='blue', alpha=0.5)\n",
    "# plt.scatter(persons_with_home_within_linear_ring.geometry.x, persons_with_home_within_linear_ring.geometry.y, s=1, color='red', alpha=0.5)\n",
    "# plt.title('Locations of Persons with Homes')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "# from shapely.geometry import MultiPolygon\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a LineString\n",
    "# line = LineString([(10, 10), (20, 10)])\n",
    "\n",
    "# # Create a buffer around the line\n",
    "# buffered_line = line.buffer(2, cap_style=\"round\")\n",
    "\n",
    "# # Plot the original line and the buffered area\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# x, y = line.xy\n",
    "# plt.plot(x, y, color='blue', label='Original Line')\n",
    "# if isinstance(buffered_line, MultiPolygon):\n",
    "#     for polygon in buffered_line:\n",
    "#         x, y = polygon.exterior.xy\n",
    "#         plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "# else:\n",
    "#     x, y = buffered_line.exterior.xy\n",
    "#     plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "\n",
    "# plt.title('Line with Buffered Area')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# def check_trips_equivalence(close_trips_start, close_trips_end):\n",
    "#     \"\"\"\n",
    "#     Check if close_trips_start and close_trips_end are equivalent.\n",
    "    \n",
    "#     Args:\n",
    "#     close_trips_start (list): List of tuples for start trips\n",
    "#     close_trips_end (list): List of tuples for end trips\n",
    "    \n",
    "#     Returns:\n",
    "#     bool: True if equivalent, False otherwise\n",
    "#     \"\"\"\n",
    "#     if len(close_trips_start) != len(close_trips_end):\n",
    "#         print(\"Lists have different lengths.\")\n",
    "#         return False\n",
    "    \n",
    "#     differences = []\n",
    "#     for i, (start, end) in enumerate(zip(close_trips_start, close_trips_end)):\n",
    "#         if start != end:\n",
    "#             differences.append((i, start, end))\n",
    "    \n",
    "#     if not differences:\n",
    "#         print(\"The lists are identical.\")\n",
    "#         return True\n",
    "#     else:\n",
    "#         print(f\"Found {len(differences)} differences:\")\n",
    "#         for diff in differences[:10]:  # Print first 10 differences\n",
    "#             print(f\"Index {diff[0]}: Start {diff[1]}, End {diff[2]}\")\n",
    "#         if len(differences) > 10:\n",
    "#             print(f\"... and {len(differences) - 10} more differences.\")\n",
    "#         return False\n",
    "\n",
    "# # Usage\n",
    "# are_equivalent = check_trips_equivalence(close_trips_start, close_trips_end)\n",
    "# print(f\"Are the trip lists equivalent? {are_equivalent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
