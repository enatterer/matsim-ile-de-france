{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "import processing_io as pio\n",
    "from torch_geometric.transforms import LineGraph\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "import shapely.wkt as wkt\n",
    "from tqdm import tqdm\n",
    "import fiona\n",
    "import os\n",
    "\n",
    "import alphashape\n",
    "from shapely.geometry import Polygon\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from shapely.geometry import Point\n",
    "import random\n",
    "\n",
    "highway_mapping = {\n",
    "    'trunk': 0, 'trunk_link': 0, 'motorway_link': 0,\n",
    "    'primary': 1, 'primary_link': 1,\n",
    "    'secondary': 2, 'secondary_link': 2,\n",
    "    'tertiary': 3, 'tertiary_link': 3,\n",
    "    'residential': 4, 'living_street': 5,\n",
    "    'pedestrian': 6, 'service': 7,\n",
    "    'construction': 8, 'unclassified': 9,\n",
    "    'np.nan': -1\n",
    "}\n",
    "result_df_name = 'sim_output_1pm_capacity_reduction_10k_PRELIMINARY'\n",
    "# result_df_name = 'sim_output_1pm_capacity_reduction_10k'\n",
    "\n",
    "result_path = '../../../../data/datasets_simulation_outputs/' + result_df_name + '.pt'\n",
    "string_is_for_1pm = \"pop_1pm\"\n",
    "\n",
    "base_dir_sample_sim_input = '../../../../data/' + string_is_for_1pm + '_simulations/' + string_is_for_1pm + '_policies_combinations_with_normal_dist/'\n",
    "subdirs_pattern = os.path.join(base_dir_sample_sim_input, 'output_networks_*')\n",
    "subdirs = list(set(glob.glob(subdirs_pattern)))\n",
    "subdirs.sort()\n",
    "\n",
    "paris_inside_bvd_peripherique = \"../../../../data/paris_inside_bvd_per/referentiel-comptages-edit.shp\"\n",
    "gdf_paris_inside_bvd_per = gpd.read_file(paris_inside_bvd_peripherique)\n",
    "boundary_df = alphashape.alphashape(gdf_paris_inside_bvd_per, 435).exterior[0]\n",
    "linear_ring_polygon = Polygon(boundary_df)\n",
    "\n",
    "gdf_basecase_output_links = gpd.read_file('results/' + string_is_for_1pm + '_basecase_average_output_links.geojson')\n",
    "gdf_basecase_average_mode_stats = pd.read_csv('results/' + string_is_for_1pm + '_basecase_average_mode_stats.csv', delimiter=';')\n",
    "districts = gpd.read_file(\"../../../../data/visualisation/districts_paris.geojson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "This is further than process_output_of_simulations_with_all_output_links_and_eqasim_info.ipynb, as it also includes more input information.\n",
    "\n",
    "Note that there is more than one strategy to deal with the fact that there are more than one district per link. We implement the strategy of stacking the information of all districts together. \n",
    "An alternative strategy would be to use the mean of the information of the districts.\n",
    "\n",
    "Process the districts manually, so that each link belongs to at most 3 districts."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process results\n",
    "\n",
    "Process the outputs of the simulations for further usage by GNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_close_homes(links_gdf_input:pd.DataFrame, information_gdf_input:pd.DataFrame, utm_crs:str, distance:int=50):\n",
    "    links_gdf = links_gdf_input.copy()\n",
    "    information_gdf = information_gdf_input.copy()\n",
    "    close_places = []\n",
    "    links_gdf_utm = links_gdf.to_crs(utm_crs)\n",
    "    information_gdf_utm = information_gdf.to_crs(utm_crs)\n",
    "    for i, row in tqdm(enumerate(links_gdf_utm.iterrows()), desc=\"Processing rows\", unit=\"row\"):\n",
    "        buffer_utm = row[1].geometry.buffer(distance=distance)\n",
    "        buffer = gpd.GeoSeries([buffer_utm], crs=utm_crs).to_crs(links_gdf_utm.crs)[0]\n",
    "        matched_information = information_gdf_utm[information_gdf_utm.geometry.within(buffer)]\n",
    "        socioprofessional_classes = matched_information['socioprofessional_class'].tolist()\n",
    "        close_places.append((len(socioprofessional_classes), socioprofessional_classes))\n",
    "    return close_places\n",
    "\n",
    "def process_close_count_to_tensor(close_count_list: list):\n",
    "    socio_professional_classes = [item[1] for item in close_count_list]\n",
    "    unique_classes = set([2, 3, 4, 5, 6, 7, 8])\n",
    "    class_to_index = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "\n",
    "    tensor_shape = (len(close_count_list), len(unique_classes))\n",
    "    close_homes_tensor = torch.zeros(tensor_shape)\n",
    "\n",
    "    for i, classes in enumerate(socio_professional_classes):\n",
    "        for cls in classes:\n",
    "            if cls in class_to_index:  # Ensure the class is in the predefined set\n",
    "                close_homes_tensor[i, class_to_index[cls]] += 1\n",
    "    \n",
    "    close_homes_tensor_sparse = close_homes_tensor.to_sparse()\n",
    "    return close_homes_tensor_sparse\n",
    "\n",
    "# Read all network data into a dictionary of GeoDataFrames\n",
    "def compute_result_dic_output_links():\n",
    "    result_dic = {}\n",
    "    base_network_no_policies = gdf_basecase_output_links\n",
    "    result_dic[\"base_network_no_policies\"] = base_network_no_policies\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        # print(f'Accessing folder: {subdir}')\n",
    "        # print(len(os.listdir(subdir)))\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_output_links = pio.read_output_links(file_path)\n",
    "            df_output_links.drop(columns=['geometry'], inplace=True)\n",
    "            if (df_output_links is not None):\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=df_output_links, column_to_extend='highway', new_column_name='highway')\n",
    "                gdf_extended = pio.extend_geodataframe(gdf_base=gdf_basecase_output_links, gdf_to_extend=gdf_extended, column_to_extend='vol_car', new_column_name='vol_car_base_case')\n",
    "                result_dic[policy_key] = gdf_extended\n",
    "        break\n",
    "    return result_dic\n",
    "\n",
    "def calculate_averaged_results(trips_df):\n",
    "    \"\"\"Calculate average travel time and routed distance grouped by mode.\"\"\"\n",
    "    return trips_df.groupby('mode').agg(\n",
    "        total_travel_time=('travel_time', 'mean'),\n",
    "        total_routed_distance=('routed_distance', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "def compute_result_dic_mode_stats(calculate_averaged_results):\n",
    "    result_dic_mode_stats = {}\n",
    "    result_dic_mode_stats[\"base_network_no_policies\"] = gdf_basecase_average_mode_stats\n",
    "    for subdir in tqdm(subdirs, desc=\"Processing subdirs\", unit=\"subdir\"):\n",
    "        networks = [network for network in os.listdir(subdir) if not network.endswith(\".DS_Store\")]\n",
    "        for network in networks:\n",
    "            file_path = os.path.join(subdir, network)\n",
    "            policy_key = pio.create_policy_key_1pm(network)\n",
    "            df_mode_stats = pd.read_csv(file_path + '/eqasim_trips.csv', delimiter=';')\n",
    "            averaged_results = calculate_averaged_results(df_mode_stats)\n",
    "            if (averaged_results is not None):\n",
    "                result_dic_mode_stats[policy_key] = averaged_results\n",
    "        break\n",
    "    return result_dic_mode_stats\n",
    "\n",
    "def encode_modes(gdf):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': gdf['modes'].str.contains('car', case=False, na=False).astype(int),\n",
    "        'bus': gdf['modes'].str.contains('bus', case=False, na=False).astype(int),\n",
    "        'pt': gdf['modes'].str.contains('pt', case=False, na=False).astype(int),\n",
    "        'train': gdf['modes'].str.contains('train', case=False, na=False).astype(int),\n",
    "        'rail': gdf['modes'].str.contains('rail', case=False, na=False).astype(int),\n",
    "        'subway': gdf['modes'].str.contains('subway', case=False, na=False).astype(int)\n",
    "    }\n",
    "    modes_encoded = pd.DataFrame(modes_conditions)\n",
    "    return torch.tensor(modes_encoded.values, dtype=torch.float)\n",
    "\n",
    "\n",
    "def encode_modes_string(mode_string):\n",
    "    \"\"\"Encode the 'modes' attribute based on specific strings.\"\"\"\n",
    "    modes_conditions = {\n",
    "        'car': int(\"car\" in mode_string),\n",
    "        'bus': int(\"bus\" in mode_string),\n",
    "        'pt': int(\"pt\" in mode_string),\n",
    "        'train': int(\"train\" in mode_string),\n",
    "        'rail': int(\"rail\" in mode_string),\n",
    "        'subway': int(\"subway\" in mode_string),\n",
    "    }\n",
    "    modes_encoded_tensor = torch.tensor(list(modes_conditions.values()), dtype=torch.float)\n",
    "    return modes_encoded_tensor\n",
    "\n",
    "def get_dfs(base_dir:str):\n",
    "    files = os.listdir(base_dir)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(base_dir, file)\n",
    "        base_name, ext = os.path.splitext(file)\n",
    "        if base_name.startswith(\"idf_1pm_\"):\n",
    "            base_name = base_name.replace(\"idf_1pm_\", \"\")\n",
    "        var_name = base_name  # Start with the cleaned base name\n",
    "    \n",
    "        if file.endswith('.csv'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_df\"  \n",
    "                globals()[var_name] = pd.read_csv(file_path, sep=\";\")\n",
    "                print(f\"Loaded CSV file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "            \n",
    "        elif file.endswith('.gpkg'):\n",
    "            try:\n",
    "                var_name = f\"{var_name}_gdf\"  \n",
    "                layers = fiona.listlayers(file_path)\n",
    "                geodataframes = {layer: gpd.read_file(file_path, layer=layer, geometry = 'geometry', crs=\"EPSG:2154\") for layer in layers}\n",
    "                for layer, gdf in geodataframes.items():\n",
    "                # print(f\"Layer: {layer}\")\n",
    "                    gdf = gdf.to_crs(epsg=4326)\n",
    "                    globals()[var_name] = gdf\n",
    "                    print(f\"Loaded GPKG file: {file} into variable: {var_name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading CSV file {file}: {e}\")\n",
    "    homes_gdf = globals()[\"homes_gdf\"]\n",
    "    households_df = globals()[\"households_df\"]\n",
    "    persons_df = globals()[\"persons_df\"]\n",
    "    activities_gdf = globals()[\"activities_gdf\"]\n",
    "    trips_df = globals()[\"trips_gdf\"]\n",
    "    return homes_gdf, households_df, persons_df, activities_gdf, trips_df\n",
    "\n",
    "def extract_start_end_points(geometry):\n",
    "    if len(geometry.coords) != 2:\n",
    "        raise ValueError(\"Linestring does not have exactly 2 elements.\")\n",
    "    return geometry.coords[0], geometry.coords[-1]\n",
    "\n",
    "def get_close_trips_tensor(links_gdf_input, trips_gdf_input, utm_crs, distance):\n",
    "    close_trips_count = compute_close_homes(links_gdf_input = links_gdf_input, information_gdf_input = trips_gdf_input, utm_crs = utm_crs, distance=distance)\n",
    "    close_trips_count_tensor = process_close_count_to_tensor(close_trips_count)\n",
    "    return close_trips_count, close_trips_count_tensor\n",
    "\n",
    "def get_start_and_end_gdf(trips_with_socio, crs):\n",
    "    trips_start = trips_with_socio.copy()\n",
    "    trips_end = trips_with_socio.copy()\n",
    "\n",
    "    trips_start_gdf = gpd.GeoDataFrame(\n",
    "    trips_start, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_start['start_point'].apply(lambda p: p[0]), \n",
    "        trips_start['start_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "\n",
    "    trips_end_gdf = gpd.GeoDataFrame(\n",
    "    trips_end, \n",
    "    geometry=gpd.points_from_xy(\n",
    "        trips_end['end_point'].apply(lambda p: p[0]), \n",
    "        trips_end['end_point'].apply(lambda p: p[1])\n",
    "    ), \n",
    "    crs=crs\n",
    ")\n",
    "    return trips_start_gdf,trips_end_gdf\n",
    "\n",
    "def process_centroid(geom_list):\n",
    "    if not geom_list:  # Empty list\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    elif len(geom_list) == 1:\n",
    "        return [geom_list[0], np.nan, np.nan]\n",
    "    elif len(geom_list) == 2:\n",
    "        return [geom_list[0], geom_list[1], np.nan]\n",
    "    else:\n",
    "        return [geom_list[0], geom_list[1], geom_list[2]]\n",
    "    \n",
    "def extract_point_coordinates(geom_list):\n",
    "    coordinates = []\n",
    "    for geom in geom_list:\n",
    "        if isinstance(geom, Point):\n",
    "            coordinates.append((geom.x, geom.y))\n",
    "        else:\n",
    "            coordinates.append((np.nan, np.nan))\n",
    "    return coordinates\n",
    "\n",
    "def process_value_list(perimeter_list):\n",
    "    if not perimeter_list:  # Empty list\n",
    "        return [np.nan, np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 1:\n",
    "        return [perimeter_list[0], np.nan, np.nan]\n",
    "    elif len(perimeter_list) == 2:\n",
    "        return [perimeter_list[0], perimeter_list[1], np.nan]\n",
    "    else:\n",
    "        return [perimeter_list[0], perimeter_list[1], perimeter_list[2]]\n",
    "    \n",
    "def compute_district_2_information_counts(district_information_counts, column_to_filter_for):\n",
    "    district_group_2_information_counts = {}\n",
    "    for district, group in district_information_counts:        \n",
    "        # ignore groups with more than one district here. \n",
    "        if len(district) == 1:\n",
    "            total_counts = 0\n",
    "            total_distributions = []\n",
    "            counts = group[column_to_filter_for].values            \n",
    "            for c in counts:\n",
    "                total_counts += c[0]\n",
    "                if c[1] is not None and len(c[1]) > 0:\n",
    "                    total_distributions.extend(c[1])\n",
    "            distribution_counts = [total_distributions.count(i) for i in range(2, 9)]   \n",
    "            district_group_2_information_counts[district] = distribution_counts\n",
    "    return district_group_2_information_counts, distribution_counts\n",
    "\n",
    "def compute_district_2_information_tensor(district_2_information_counts, distribution_counts, gdf_input):\n",
    "    district_home_counts_tensor = torch.zeros((len(gdf_input), 3, len(distribution_counts)), dtype=torch.float)\n",
    "    nan_tensor = torch.full((len(distribution_counts),), float('nan'))\n",
    "\n",
    "    for idx, row in gdf_input.iterrows():\n",
    "        district_combination = row['district']\n",
    "        district_combination_tuple = tuple(district_combination)\n",
    "        if len(district_combination_tuple) == 0:\n",
    "            district_home_counts_tensor[idx] = torch.stack([nan_tensor, nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 1:\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[district_combination_tuple]), nan_tensor, nan_tensor])\n",
    "        elif len(district_combination_tuple) == 2:\n",
    "            a, b = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), nan_tensor])\n",
    "        elif len(district_combination_tuple) == 3:\n",
    "            a, b, c = district_combination_tuple\n",
    "            district_home_counts_tensor[idx] = torch.stack([torch.tensor(district_2_information_counts[(a,)]), torch.tensor(district_2_information_counts[(b,)]), torch.tensor(district_2_information_counts[(c,)])])\n",
    "        else:\n",
    "            print(\"NOT OK!\")\n",
    "            print(district_combination_tuple)\n",
    "    return district_home_counts_tensor\n",
    "\n",
    "def preprocess_links(links_gdf):\n",
    "    for index, row in links_gdf.iterrows():\n",
    "        if len(row['district']) >= 4:\n",
    "            row['district'].pop(random.randint(0, len(row['district']) - 1))\n",
    "    return links_gdf\n",
    "\n",
    "def find_duplicate_edges_in_gdf(gdf):\n",
    "    edge_count = defaultdict(list)\n",
    "    for idx, row in gdf.iterrows():\n",
    "        edge = tuple(sorted([row['from_node'], row['to_node']]))\n",
    "        edge_count[edge].append(idx)\n",
    "    \n",
    "    duplicates = {edge: indices for edge, indices in edge_count.items() if len(indices) > 1}\n",
    "    return duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing subdirs:   0%|          | 0/100 [00:07<?, ?subdir/s]\n",
      "Processing subdirs:   0%|          | 0/100 [00:01<?, ?subdir/s]\n",
      "/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_86986/1320727273.py:7: UserWarning: Geometry is in a geographic CRS. Results from 'centroid' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n",
      "\n",
      "  districts['district_centroid'] = districts['geometry'].centroid\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3448: FutureWarning: The `op` parameter is deprecated and will be removed in a future release. Please use the `predicate` parameter instead.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "/Users/elenanatterer/anaconda3/envs/Paris_Analysis/lib/python3.10/site-packages/pandas/core/reshape/merge.py:1203: RuntimeWarning: invalid value encountered in cast\n",
      "  if not (lk == lk.astype(rk.dtype))[~np.isnan(lk)].all():\n"
     ]
    }
   ],
   "source": [
    "result_dic_output_links = compute_result_dic_output_links()\n",
    "result_dic_mode_stats = compute_result_dic_mode_stats(calculate_averaged_results)\n",
    "base_gdf = result_dic_output_links[\"base_network_no_policies\"]\n",
    "links_gdf_base = gpd.GeoDataFrame(base_gdf, geometry='geometry')\n",
    "links_gdf_base.crs = \"EPSG:2154\"  # Assuming the original CRS is EPSG:2154\n",
    "links_gdf_base.to_crs(\"EPSG:4326\", inplace=True)\n",
    "districts['district_centroid'] = districts['geometry'].centroid\n",
    "links_gdf_with_districts = gpd.sjoin(links_gdf_base, districts, how='left', op='intersects')\n",
    "\n",
    "# Group by edge and aggregate the district names\n",
    "links_gdf_with_districts = links_gdf_with_districts.groupby('link').agg({\n",
    "    'from_node': 'first',\n",
    "    'to_node': 'first',\n",
    "    'length': 'first',\n",
    "    'freespeed': 'first',\n",
    "    'capacity': 'first',\n",
    "    'lanes': 'first',\n",
    "    'modes': 'first',\n",
    "    'vol_car': 'first',\n",
    "    'highway': 'first',\n",
    "    'geometry': 'first',\n",
    "    'c_ar': lambda x: list(x.dropna()),\n",
    "    'district_centroid': lambda x: list(x.dropna()),\n",
    "    'perimetre': lambda x: list(x.dropna()),\n",
    "    'surface': lambda x: list(x.dropna()),\n",
    "}).reset_index()\n",
    "gdf_now = gpd.GeoDataFrame(links_gdf_with_districts, geometry='geometry', crs=links_gdf_base.crs)\n",
    "gdf_now = gdf_now.rename(columns={'c_ar': 'district', 'perimetre': 'district_perimeter', 'surface': 'district_surface'})\n",
    "links_gdf_final = gdf_now.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_dic_mode_stats['Policy introduced in Arrondissement(s) d, 8, 11, 14, 17, 19'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_ar</th>\n",
       "      <th>surface</th>\n",
       "      <th>perimetre</th>\n",
       "      <th>geometry</th>\n",
       "      <th>district_centroid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.824613e+06</td>\n",
       "      <td>6054.936862</td>\n",
       "      <td>POLYGON ((2.32801 48.86992, 2.32997 48.86851, ...</td>\n",
       "      <td>POINT (2.33644 48.86256)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>9.911537e+05</td>\n",
       "      <td>4554.104360</td>\n",
       "      <td>POLYGON ((2.35152 48.86443, 2.35095 48.86341, ...</td>\n",
       "      <td>POINT (2.34280 48.86828)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.170883e+06</td>\n",
       "      <td>4519.263648</td>\n",
       "      <td>POLYGON ((2.36383 48.86750, 2.36389 48.86747, ...</td>\n",
       "      <td>POINT (2.36000 48.86287)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.600586e+06</td>\n",
       "      <td>5420.908434</td>\n",
       "      <td>POLYGON ((2.36851 48.85573, 2.36900 48.85374, ...</td>\n",
       "      <td>POINT (2.35763 48.85434)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2.539375e+06</td>\n",
       "      <td>6239.195396</td>\n",
       "      <td>POLYGON ((2.36443 48.84614, 2.36484 48.84584, ...</td>\n",
       "      <td>POINT (2.35071 48.84444)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   c_ar       surface    perimetre  \\\n",
       "0     1  1.824613e+06  6054.936862   \n",
       "1     2  9.911537e+05  4554.104360   \n",
       "2     3  1.170883e+06  4519.263648   \n",
       "3     4  1.600586e+06  5420.908434   \n",
       "4     5  2.539375e+06  6239.195396   \n",
       "\n",
       "                                            geometry         district_centroid  \n",
       "0  POLYGON ((2.32801 48.86992, 2.32997 48.86851, ...  POINT (2.33644 48.86256)  \n",
       "1  POLYGON ((2.35152 48.86443, 2.35095 48.86341, ...  POINT (2.34280 48.86828)  \n",
       "2  POLYGON ((2.36383 48.86750, 2.36389 48.86747, ...  POINT (2.36000 48.86287)  \n",
       "3  POLYGON ((2.36851 48.85573, 2.36900 48.85374, ...  POINT (2.35763 48.85434)  \n",
       "4  POLYGON ((2.36443 48.84614, 2.36484 48.84584, ...  POINT (2.35071 48.84444)  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "districts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.3364, 48.8626],\n",
      "        [ 2.3428, 48.8683],\n",
      "        [ 2.3600, 48.8629],\n",
      "        [ 2.3576, 48.8543],\n",
      "        [ 2.3507, 48.8444],\n",
      "        [ 2.3329, 48.8491],\n",
      "        [ 2.3122, 48.8562],\n",
      "        [ 2.3126, 48.8727],\n",
      "        [ 2.3375, 48.8772],\n",
      "        [ 2.3607, 48.8761],\n",
      "        [ 2.3801, 48.8591],\n",
      "        [ 2.4213, 48.8350],\n",
      "        [ 2.3623, 48.8284],\n",
      "        [ 2.3265, 48.8292],\n",
      "        [ 2.2928, 48.8401],\n",
      "        [ 2.2620, 48.8604],\n",
      "        [ 2.3068, 48.8873],\n",
      "        [ 2.3482, 48.8926],\n",
      "        [ 2.3848, 48.8871],\n",
      "        [ 2.4012, 48.8635]])\n",
      "torch.Size([20, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "# Convert district_centroid to a tensor of size (20, 2)\n",
    "district_centroids = districts['district_centroid'].apply(lambda point: [point.x, point.y])\n",
    "district_centroids_tensor = torch.tensor(district_centroids.tolist(), dtype=torch.float32)\n",
    "\n",
    "# Ensure the tensor is of size (20, 2)\n",
    "if district_centroids_tensor.size(0) != 20 or district_centroids_tensor.size(1) != 2:\n",
    "    raise ValueError(\"The resulting tensor does not have the expected size of (20, 2)\")\n",
    "# Pad the tensor to size (20, 3, 2) by duplicating entries\n",
    "district_centroids_tensor_padded = district_centroids_tensor.unsqueeze(1).expand(-1, 3, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of edges: 31216\n",
      "Number of edges after summarization: 25309\n",
      "Number of remaining duplicate edges: 0\n"
     ]
    }
   ],
   "source": [
    "def summarize_duplicate_edges(gdf):\n",
    "    # Check if 'vol_car' exists and print its data type\n",
    "    if 'vol_car' not in gdf.columns:\n",
    "        print(\"'vol_car' column does not exist in the dataframe\")\n",
    "        return gdf\n",
    "\n",
    "    # Create a unique identifier for each edge, regardless of direction\n",
    "    gdf['edge_id'] = gdf.apply(lambda row: tuple(sorted([row['from_node'], row['to_node']])), axis=1)\n",
    "    \n",
    "    # Group by the edge_id\n",
    "    grouped = gdf.groupby('edge_id')\n",
    "    \n",
    "    # Function to aggregate the data\n",
    "    def aggregate_edges(group):\n",
    "        # Sum the 'vol_car' column\n",
    "        vol_car_sum = group['vol_car'].sum()\n",
    "        \n",
    "        # Take other attributes from the first entry\n",
    "        first_entry = group.iloc[0]\n",
    "        \n",
    "        # Create a new row with combined data\n",
    "        combined = first_entry.copy()\n",
    "        combined['vol_car'] = vol_car_sum\n",
    "        \n",
    "        # If you want to keep track of the original directions, you can add this info\n",
    "        combined['original_directions'] = list(group[['from_node', 'to_node']].itertuples(index=False, name=None))\n",
    "        \n",
    "        return combined\n",
    "    \n",
    "    # Apply the aggregation\n",
    "    summarized_gdf = grouped.apply(aggregate_edges)\n",
    "    \n",
    "    # Reset the index and drop the temporary edge_id column\n",
    "    summarized_gdf = summarized_gdf.reset_index(drop=True)\n",
    "    summarized_gdf = summarized_gdf.drop(columns=['edge_id'])\n",
    "    \n",
    "    return summarized_gdf\n",
    "\n",
    "# Apply the summarization to links_gdf_final\n",
    "links_gdf_summarized = summarize_duplicate_edges(links_gdf_final)\n",
    "\n",
    "# Print some information about the summarization\n",
    "print(f\"Original number of edges: {len(links_gdf_final)}\")\n",
    "print(f\"Number of edges after summarization: {len(links_gdf_summarized)}\")\n",
    "\n",
    "# Check if there are any remaining duplicates\n",
    "remaining_duplicates = find_duplicate_edges_in_gdf(links_gdf_summarized)\n",
    "print(f\"Number of remaining duplicate edges: {len(remaining_duplicates)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "links_gdf_final = links_gdf_summarized.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum length of the list in 'district' in links_gdf_final is: 4\n"
     ]
    }
   ],
   "source": [
    "max_district_length = max(links_gdf_final['district'].apply(len))\n",
    "print(f\"The maximum length of the list in 'district' in links_gdf_final is: {max_district_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: idf_1pm_persons.csv into variable: persons_df\n",
      "Loaded GPKG file: idf_1pm_commutes.gpkg into variable: commutes_gdf\n",
      "Loaded CSV file: idf_1pm_households.csv into variable: households_df\n",
      "Loaded CSV file: idf_1pm_trips.csv into variable: trips_df\n",
      "Loaded CSV file: idf_1pm_activities.csv into variable: activities_df\n",
      "Loaded CSV file: idf_1pm_vehicle_types.csv into variable: vehicle_types_df\n",
      "Loaded GPKG file: idf_1pm_trips.gpkg into variable: trips_gdf\n",
      "Loaded GPKG file: idf_1pm_activities.gpkg into variable: activities_gdf\n",
      "Loaded CSV file: idf_1pm_vehicles.csv into variable: vehicles_df\n",
      "Loaded GPKG file: idf_1pm_homes.gpkg into variable: homes_gdf\n"
     ]
    }
   ],
   "source": [
    "base_dir_sample_sim_input = '../../../../data/pop_1pm_simulations/idf_1pm/' \n",
    "homes_gdf, households_df, persons_df, activities_gdf, trips_df = get_dfs(base_dir=base_dir_sample_sim_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"intermediate_results/population.csv\")\n",
    "\n",
    "sorted_population_df = population_df.sort_values(by=\"id\")\n",
    "sorted_persons_df = persons_df.sort_values(by=\"person_id\")\n",
    "merged_df = pd.merge(sorted_persons_df, sorted_population_df, left_on=\"person_id\", right_on=\"id\")\n",
    "removed_some_columns = merged_df.copy()\n",
    "removed_some_columns = removed_some_columns.drop(columns=['employed_y', 'hasPtSubscription', 'householdId', 'sex_y', 'htsPersonId', 'censusPersonId', 'hasLicense', 'id', 'age_y'])\n",
    "updated_persons = removed_some_columns.copy()\n",
    "persons_with_geospatial_information = homes_gdf.merge(updated_persons, on='household_id', how='right')\n",
    "\n",
    "if not isinstance(persons_with_geospatial_information, gpd.GeoDataFrame):\n",
    "    persons_with_geospatial_information = gpd.GeoDataFrame(persons_with_geospatial_information, geometry=gpd.points_from_xy(persons_with_geospatial_information.longitude, persons_with_geospatial_information.latitude), crs= links_gdf_final.crs)\n",
    "\n",
    "utm_crs = 'EPSG:32631'  # UTM zone 31N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEAL WITH TRIPS\n",
    "\n",
    "# trips_with_socio = trips_df.merge(persons_with_geospatial_information[['person_id', 'socioprofessional_class']], on='person_id', how='left')\n",
    "# trips_with_socio['start_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[0])\n",
    "# trips_with_socio['end_point'] = trips_with_socio['geometry'].apply(lambda geom: extract_start_end_points(geom)[1])\n",
    "\n",
    "# trips_start_gdf, trips_end_gdf = get_start_and_end_gdf(trips_with_socio=trips_with_socio, crs=links_gdf_final.crs)\n",
    "\n",
    "# # Create tensors for each combination of \"preceding_purpose\" and \"following_purpose\"\n",
    "# unique_purposes = trips_with_socio['preceding_purpose'].unique()\n",
    "# close_start_trips_tensor_dict = {}\n",
    "# close_start_trips_dict = {}\n",
    "# close_end_trips_tensor_dict = {}\n",
    "# close_end_trips_dict = {}\n",
    "\n",
    "# for preceding_purpose in tqdm(unique_purposes, desc=\"Processing preceding purposes\", unit=\"purpose\"):\n",
    "#     for following_purpose in tqdm(unique_purposes, desc=\"Processing following purposes\", unit=\"purpose\"):\n",
    "#         if preceding_purpose != following_purpose:\n",
    "#             filtered_trips = trips_with_socio[(trips_with_socio['preceding_purpose'] == preceding_purpose) & (trips_with_socio['following_purpose'] == following_purpose)]\n",
    "#             if not filtered_trips.empty:\n",
    "#                 filtered_trips_start_gdf, filtered_trips_end_gdf = get_start_and_end_gdf(trips_with_socio=filtered_trips, crs=links_gdf_final.crs)\n",
    "        \n",
    "#                 tensor_key = f\"{preceding_purpose}_{following_purpose}\"\n",
    "#                 string_trips_start = \"trips_start_\" + tensor_key\n",
    "#                 string_trips_end = \"trips_end_\" + tensor_key\n",
    "                \n",
    "#                 close_start_trips, close_start_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_start_gdf, utm_crs=utm_crs, distance=50)        \n",
    "#                 close_start_trips_tensor_dict[tensor_key] = close_start_trips_tensor\n",
    "#                 close_start_trips_dict[tensor_key] = close_start_trips\n",
    "#                 links_gdf_final[string_trips_start ] = close_start_trips\n",
    "                \n",
    "#                 close_end_trips, close_end_trips_tensor = get_close_trips_tensor(links_gdf_input=links_gdf_final, trips_gdf_input=filtered_trips_end_gdf, utm_crs=utm_crs, distance=50)\n",
    "#                 close_end_trips_tensor_dict[tensor_key] = close_end_trips_tensor\n",
    "#                 close_end_trips_dict[tensor_key] = close_end_trips\n",
    "#                 links_gdf_final[string_trips_end] = close_end_trips\n",
    "\n",
    "# for key, tensor in close_start_trips_tensor_dict.items():\n",
    "#     print(f\"Size of tensor for {key}: {tensor.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for key, tensor in close_end_trips_tensor_dict.items():\n",
    "# #     print(f\"Size of tensor for {key}: {tensor.size()}\")\n",
    "    \n",
    "# for key, tensor in close_end_trips_dict.items():\n",
    "#     print(f\"Size of tensor for {key}: {len(tensor)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing rows: 25309row [01:04, 389.50row/s]\n"
     ]
    }
   ],
   "source": [
    "# DEAL WITH HOMES\n",
    "\n",
    "links_gdf_final.crs = \"EPSG:4326\"\n",
    "\n",
    "close_homes_count_normal = compute_close_homes(links_gdf_input = links_gdf_final, information_gdf_input = persons_with_geospatial_information, utm_crs = utm_crs)\n",
    "links_gdf_final['close_homes_count'] = close_homes_count_normal\n",
    "close_homes_tensor = process_close_count_to_tensor(close_homes_count_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DEAL WITH ACTIVITIES\n",
    "\n",
    "# activities_with_socio = activities_gdf.merge(persons_with_geospatial_information[['household_id', 'socioprofessional_class']], on='household_id', how='left')\n",
    "# grouped_activities = activities_with_socio.groupby('purpose')\n",
    "# activities_by_purpose = {purpose: group.reset_index(drop=True) for purpose, group in grouped_activities}\n",
    "# activities_by_purpose_tensor = {}\n",
    "# for purpose, activities in activities_by_purpose.items():\n",
    "#     close_activities_count_purpose = f\"close_activities_count_{purpose}\"\n",
    "#     close_activity_count = compute_close_homes(links_gdf_input=links_gdf_final, information_gdf_input=activities, utm_crs=utm_crs)\n",
    "#     links_gdf_final[close_activities_count_purpose] = close_activity_count\n",
    "#     activities_by_purpose_tensor[purpose] = process_close_count_to_tensor(close_activity_count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deal with districts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISTRICTS GENERAL \n",
    "\n",
    "links_gdf_final['districts_tuple'] = links_gdf_final['district'].apply(lambda x: tuple(x))\n",
    "district_tuples = links_gdf_final.groupby('districts_tuple')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT CENTROIDS\n",
    "\n",
    "centroid_distances = np.array([\n",
    "    process_centroid(geom_list)\n",
    "    for geom_list in links_gdf_final['district_centroid']\n",
    "])\n",
    "\n",
    "# Process the centroids\n",
    "centroid_distance_with_coordinates = np.array([\n",
    "    extract_point_coordinates(geom_list)\n",
    "    for geom_list in centroid_distances\n",
    "])\n",
    "district_centroids_tensor = torch.tensor(centroid_distance_with_coordinates, dtype=torch.float)\n",
    "\n",
    "# # FIND DISTRICT POLYGON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([25309, 3, 2])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "district_centroids_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT HOME COUNTS \n",
    "\n",
    "# district_2_home_counts, distribution_counts = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = 'close_homes_count')\n",
    "# district_home_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_home_counts, distribution_counts=distribution_counts, gdf_input=links_gdf_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT ACTIVITIES COUNTS\n",
    "\n",
    "# activity_2_district_tensor = {}\n",
    "# for purpose in activities_by_purpose_tensor.keys():\n",
    "#     district_2_activity_counts, distribution_counts_activity = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for='close_activities_count_' + purpose)\n",
    "#     district_activitiy_counts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_activity_counts, distribution_counts=distribution_counts_activity, gdf_input=links_gdf_final)\n",
    "#     activity_2_district_tensor[purpose] = district_activitiy_counts_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = torch.nonzero(~torch.isnan(activity_2_district_tensor['education'][:, 1, 0])).squeeze()\n",
    "# indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DISTRICT TRIPS START AND END\n",
    "\n",
    "# unique_purposes = trips_with_socio['preceding_purpose'].unique()\n",
    "# district_close_start_trips_tensor_dict = {}\n",
    "# district_close_end_trips_tensor_dict = {}\n",
    "\n",
    "# for preceding_purpose in tqdm(unique_purposes, desc=\"Processing preceding purposes\", unit=\"purpose\"):\n",
    "#     for following_purpose in tqdm(unique_purposes, desc=\"Processing following purposes\", unit=\"purpose\"):\n",
    "#         if preceding_purpose != following_purpose:\n",
    "#             filtered_trips = trips_with_socio[(trips_with_socio['preceding_purpose'] == preceding_purpose) & (trips_with_socio['following_purpose'] == following_purpose)]\n",
    "#             if not filtered_trips.empty:\n",
    "#                 tensor_key = f\"{preceding_purpose}_{following_purpose}\"\n",
    "#                 print(tensor_key)\n",
    "\n",
    "#                 string_trips_start_purpose = \"trips_start_\" + tensor_key\n",
    "#                 string_trips_end_purpose = \"trips_start_\" + tensor_key\n",
    "                \n",
    "#                 district_2_start_trips, start_trip_distributions = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = string_trips_start_purpose)\n",
    "#                 district_trip_starts_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_start_trips, distribution_counts=start_trip_distributions, gdf_input=links_gdf_final)\n",
    "#                 district_close_start_trips_tensor_dict[string_trips_start_purpose] = district_trip_starts_tensor\n",
    "                \n",
    "#                 district_2_end_trips, end_trip_distributions = compute_district_2_information_counts(district_information_counts=district_tuples, column_to_filter_for = string_trips_end_purpose)\n",
    "#                 district_trip_end_tensor = compute_district_2_information_tensor(district_2_information_counts=district_2_end_trips, distribution_counts=end_trip_distributions, gdf_input=links_gdf_final)\n",
    "#                 district_close_end_trips_tensor_dict[string_trips_end_purpose] = district_trip_end_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25309, 3, 2])\n",
      "torch.Size([25309, 2])\n",
      "torch.Size([25309, 2])\n"
     ]
    }
   ],
   "source": [
    "# PROCESS LINK GEOMETRIES\n",
    "\n",
    "edge_midpoints = np.array([((geom.coords[0][0] + geom.coords[-1][0]) / 2, \n",
    "                                    (geom.coords[0][1] + geom.coords[-1][1]) / 2) \n",
    "                                for geom in links_gdf_final.geometry])\n",
    "\n",
    "nodes = pd.concat([links_gdf_final['from_node'], links_gdf_final['to_node']]).unique()\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "links_gdf_final['from_idx'] = links_gdf_final['from_node'].map(node_to_idx)\n",
    "links_gdf_final['to_idx'] = links_gdf_final['to_node'].map(node_to_idx)\n",
    "edges_base = links_gdf_final[['from_idx', 'to_idx']].values\n",
    "edge_midpoint_tensor = torch.tensor(edge_midpoints, dtype=torch.float)\n",
    "\n",
    "# Initialize start and end points\n",
    "start_points = np.array([geom.coords[0] for geom in links_gdf_final.geometry])\n",
    "end_points = np.array([geom.coords[-1] for geom in links_gdf_final.geometry])\n",
    "\n",
    "# Convert to tensors\n",
    "edge_start_point_tensor = torch.tensor(start_points, dtype=torch.float)\n",
    "edge_end_point_tensor = torch.tensor(end_points, dtype=torch.float)\n",
    "\n",
    "edge_start_end_tensor = torch.stack((edge_start_point_tensor, edge_end_point_tensor), dim=1)\n",
    "\n",
    "stacked_edge_geometries_tensor = torch.stack([edge_start_point_tensor, edge_end_point_tensor, edge_midpoint_tensor], dim=1)\n",
    "\n",
    "print(stacked_edge_geometries_tensor.shape)\n",
    "print(edge_start_point_tensor.shape)\n",
    "print(edge_start_point_tensor.to_dense().shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pio.analyze_geodataframes(result_dic=result_dic, consider_only_highway_edges=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Stack all entries of trips in one tensor\n",
    "# close_start_trips_tensors = [tensor.to_dense() for tensor in close_start_trips_tensor_dict.values()]\n",
    "# stacked_close_start_trips_tensor = torch.cat(close_start_trips_tensors, dim=1)\n",
    "\n",
    "# close_end_trips_tensors = [tensor.to_dense() for tensor in close_end_trips_tensor_dict.values()]\n",
    "# stacked_close_end_trips_tensor = torch.cat(close_end_trips_tensors, dim=1)\n",
    "\n",
    "# district_start_trips_tensors = [tensor.to_dense() for tensor in district_close_start_trips_tensor_dict.values()]\n",
    "# stacked_district_start_trips_tensor = torch.cat(district_start_trips_tensors, dim=1)\n",
    "\n",
    "# district_end_trips_tensors = [tensor.to_dense() for tensor in district_close_end_trips_tensor_dict.values()]\n",
    "# stacked_district_end_trips_tensor = torch.cat(district_end_trips_tensors, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_district_information(links_gdf, tensors_edge_information):\n",
    "    \n",
    "    # Assuming tensors_edge_information is a list of tensors\n",
    "    vol_base_case = tensors_edge_information[0]  # Adjust index if needed\n",
    "    capacities_base = tensors_edge_information[1]  # Assuming capacities_new is the second tensor\n",
    "    capacities_new = tensors_edge_information[2]  # Assuming capacities_new is the second tensor\n",
    "    capacity_reduction = tensors_edge_information[3]  # Assuming capacity_reduction is the third tensor, etc. \n",
    "    freespeed_base = tensors_edge_information[4]\n",
    "    freespeed = tensors_edge_information[5]\n",
    "    highway = tensors_edge_information[6]\n",
    "    length = tensors_edge_information[7]\n",
    "    allowed_modes = tensors_edge_information[8]\n",
    "    \n",
    "    district_info = {}\n",
    "            \n",
    "    modes_str = \"\"\n",
    "    for idx, row in links_gdf.iterrows():\n",
    "        districts = row['district']\n",
    "        modes = row['modes']\n",
    "        modes_str += modes + \",\"\n",
    "        for district in districts:\n",
    "            if district not in district_info:\n",
    "                district_info[district] = {\n",
    "                    'vol_base_case': 0,\n",
    "                    'capacity_base': 0,\n",
    "                    'capacity_new': 0,\n",
    "                    'capacity_reduction': 0,\n",
    "                    'freespeed_base_sum': 0,\n",
    "                    'freespeed_base_count': 0,\n",
    "                    'freespeed_sum': 0,\n",
    "                    'freespeed_count': 0,\n",
    "                    'highway_sum': 0,\n",
    "                    'highway_count': 0,\n",
    "                    'length': 0,\n",
    "                    'edge_count': 0,\n",
    "                }\n",
    "            \n",
    "            if \"car\" in modes:\n",
    "                district_info[district]['capacity_base'] += capacities_base[idx].item()\n",
    "                district_info[district]['capacity_new'] += capacities_new[idx].item()\n",
    "                district_info[district]['capacity_reduction'] += capacity_reduction[idx].item()\n",
    "                district_info[district]['freespeed_sum'] += freespeed[idx].item()\n",
    "                district_info[district]['freespeed_base_sum'] += freespeed_base[idx].item()\n",
    "                district_info[district]['freespeed_base_count'] += 1\n",
    "                district_info[district]['freespeed_count'] += 1\n",
    "            else:\n",
    "                district_info[district]['capacity_base'] += 0\n",
    "                district_info[district]['capacity_new'] += 0\n",
    "                district_info[district]['capacity_reduction'] += 0\n",
    "                district_info[district]['freespeed_sum'] += 0\n",
    "                district_info[district]['freespeed_base_sum'] += 0\n",
    "            \n",
    "            district_info[district]['capacity_base'] += capacities_base[idx].item()  # Assuming capacity is the second tensor\n",
    "            district_info[district]['capacity_new'] += capacities_new[idx].item()\n",
    "            district_info[district]['capacity_reduction'] += capacity_reduction[idx].item()\n",
    "    \n",
    "            district_info[district]['length'] += length[idx].item()\n",
    "\n",
    "            highway_value = highway_mapping.get(row['highway'], -1)\n",
    "            district_info[district]['highway_sum'] += highway_value\n",
    "            district_info[district]['highway_count'] += 1\n",
    "            district_info[district]['edge_count'] += 1\n",
    "\n",
    "\n",
    "    # Convert allowed_modes to a list for each district\n",
    "    for district in district_info:\n",
    "        district_info[district]['freespeed_base'] = district_info[district]['freespeed_base_sum'] / district_info[district]['freespeed_base_count']\n",
    "        district_info[district]['freespeed'] = district_info[district]['freespeed_sum'] / district_info[district]['freespeed_count']\n",
    "        district_info[district]['highway'] = district_info[district]['highway_sum'] / district_info[district]['highway_count']\n",
    "        district_info[district]['allowed_modes'] = encode_modes_string(modes_str)\n",
    "    \n",
    "    # Sort districts by their identifiers\n",
    "    districts = sorted(district_info.keys())\n",
    "    \n",
    "    vol_base_case_tensor = torch.tensor([district_info[d]['vol_base_case'] for d in districts])\n",
    "    capacity_base_tensor = torch.tensor([district_info[d]['capacity_base'] for d in districts])\n",
    "    capacity_new_tensor = torch.tensor([district_info[d]['capacity_new'] for d in districts])\n",
    "    capacity_reduction_tensor = torch.tensor([district_info[d]['capacity_reduction'] for d in districts])\n",
    "    \n",
    "    length_tensor = torch.tensor([district_info[d]['length'] for d in districts])\n",
    "    edge_count_tensor = torch.tensor([district_info[d]['edge_count'] for d in districts])\n",
    "    highway_tensor = torch.tensor([district_info[d]['highway'] for d in districts])\n",
    "    freespeed_base_tensor = torch.tensor([district_info[d]['freespeed_base'] for d in districts])\n",
    "    freespeed_tensor = torch.tensor([district_info[d]['freespeed'] for d in districts])\n",
    "    allowed_modes_tensor = torch.stack([district_info[d]['allowed_modes'] for d in districts])\n",
    "\n",
    "    return {\n",
    "        'districts': districts,\n",
    "        'vol_base_case': vol_base_case_tensor,\n",
    "        'capacity_base': capacity_base_tensor,\n",
    "        'capacity_new': capacity_new_tensor,\n",
    "        'capacity_reduction': capacity_reduction_tensor,\n",
    "        'length': length_tensor,\n",
    "        'highway': highway_tensor,\n",
    "        'freespeed_base': freespeed_base_tensor,\n",
    "        'freespeed': freespeed_tensor,\n",
    "        'allowed_modes': allowed_modes_tensor,\n",
    "        'edge_count': edge_count_tensor,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the tensors of shape (20,) to shape (20, 6)\n",
    "def pad_tensor(tensor, target_shape):\n",
    "    if tensor.dim() < len(target_shape):\n",
    "        return tensor.unsqueeze(1).repeat(1, target_shape[1])\n",
    "    return tensor\n",
    "\n",
    "def compute_stacked_edge_and_district_tensors(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed):\n",
    "    edge_tensors = [\n",
    "                torch.tensor(vol_base_case), \n",
    "                torch.tensor(capacity_base_case), \n",
    "                torch.tensor(capacities_new), \n",
    "                torch.tensor(capacity_reduction), \n",
    "                torch.tensor(freespeed_base_case), \n",
    "                torch.tensor(freespeed), \n",
    "                torch.tensor(highway), \n",
    "                torch.tensor(length), \n",
    "                torch.tensor(allowed_modes)\n",
    "            ]\n",
    "    district_info = aggregate_district_information(gdf, edge_tensors)\n",
    "    district_tensors = [\n",
    "                district_info['vol_base_case'],\n",
    "                district_info['capacity_base'],\n",
    "                district_info['capacity_new'],\n",
    "                district_info['capacity_reduction'],\n",
    "                district_info['freespeed_base'],\n",
    "                district_info['freespeed'],\n",
    "                district_info['highway'],\n",
    "                district_info['length'],\n",
    "                district_info['allowed_modes']\n",
    "            ]\n",
    "    target_shape = district_info['allowed_modes'].shape # this shape is the only one that is not of shape (20,) but of shape (20, 6)\n",
    "    stacked_tensors_edge_information = torch.stack(\n",
    "                [pad_tensor(tensor, target_shape) for tensor in edge_tensors], \n",
    "                dim=1\n",
    "            )\n",
    "    stacked_district_tensors = torch.stack(\n",
    "                [pad_tensor(tensor, target_shape) for tensor in district_tensors], \n",
    "                dim=1\n",
    "            )\n",
    "    \n",
    "    return district_info,stacked_tensors_edge_information,stacked_district_tensors\n",
    "\n",
    "\n",
    "def compute_node_attributes(district_info, linegraph_data):\n",
    "    num_edge_nodes = linegraph_data.num_nodes\n",
    "    num_district_nodes = len(district_info['districts'])\n",
    "    existing_feature_dim1 = linegraph_data.x.size(1) if linegraph_data.x is not None else 9\n",
    "    existing_feature_dim2 = linegraph_data.x.size(2) if linegraph_data.x is not None else 6 \n",
    "    node_type_feature = torch.zeros((num_edge_nodes + num_district_nodes, 1, existing_feature_dim2), dtype=torch.long)\n",
    "    node_type_feature[num_edge_nodes:, :, :] = 1\n",
    "    return num_edge_nodes,num_district_nodes,existing_feature_dim1,existing_feature_dim2,node_type_feature\n",
    "\n",
    "def compute_edge_attributes(district_info, linegraph_data):\n",
    "    district_node_offset = linegraph_data.num_nodes\n",
    "    edge_to_district_edges = []\n",
    "    for idx, row in links_gdf_final.iterrows():\n",
    "        for district in row['district']:\n",
    "            district_idx = district_info['districts'].index(district) + district_node_offset\n",
    "            edge_to_district_edges.append([idx, district_idx])\n",
    "            edge_to_district_edges.append([district_idx, idx])  # Add reverse edge for undirected graph  # TODO is one way enough ? \n",
    "            \n",
    "    edge_to_district_index = torch.tensor(edge_to_district_edges, dtype=torch.long).t()\n",
    "    linegraph_data.edge_index = torch.cat([linegraph_data.edge_index, edge_to_district_index], dim=1)\n",
    "    edge_to_district_index = torch.tensor(edge_to_district_edges, dtype=torch.long).t()\n",
    "    edge_to_district_attr = torch.ones((edge_to_district_index.shape[1], 1), dtype=torch.long)\n",
    "    return edge_to_district_index,edge_to_district_attr\n",
    "\n",
    "def compute_target_tensor(vol_base_case, gdf, district_info):\n",
    "    edge_car_volume_difference = gdf['vol_car'].values - vol_base_case\n",
    "    district_car_volume_difference = []\n",
    "    for district in district_info['districts']:\n",
    "        district_edges = gdf[gdf['district'].apply(lambda x: district in x)]\n",
    "        district_volume_diff = district_edges['vol_car'].sum() - district_edges['vol_car_base_case'].sum()\n",
    "        district_car_volume_difference.append(district_volume_diff)\n",
    "    district_car_volume_difference = torch.tensor(district_car_volume_difference, dtype=torch.float).unsqueeze(1)\n",
    "    target_values = torch.cat([torch.tensor(edge_car_volume_difference, dtype=torch.float).unsqueeze(1), district_car_volume_difference], dim=0)\n",
    "    return target_values\n",
    "\n",
    "\n",
    "def combine_stacked_tensors(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed):\n",
    "    district_info, stacked_tensors_edge_information, stacked_district_tensors = compute_stacked_edge_and_district_tensors(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed)\n",
    "    combined_tensor = torch.cat((stacked_tensors_edge_information, stacked_district_tensors), dim=0)\n",
    "    return district_info,combined_tensor\n",
    "\n",
    "def get_basic_edge_attributes(capacity_base_case, gdf):\n",
    "    capacities_new = np.where(gdf['modes'].str.contains('car'), gdf['capacity'], 0)\n",
    "    capacity_reduction = capacities_new - capacity_base_case\n",
    "    highway = gdf['highway'].apply(lambda x: highway_mapping.get(x, -1)).values\n",
    "    freespeed = np.where(gdf['modes'].str.contains('car'), gdf['freespeed'], 0)\n",
    "    return capacities_new,capacity_reduction,highway,freespeed\n",
    "\n",
    "def prepare_gdf(df):\n",
    "    gdf = links_gdf_final[['link', 'district', 'geometry']].merge(df, on='link', how='left')\n",
    "    gdf = gpd.GeoDataFrame(gdf, geometry='geometry')\n",
    "    gdf.crs = links_gdf_final.crs\n",
    "    return gdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing result_dic:   0%|          | 0/79 [00:00<?, ?dataframe/s]/var/folders/m_/fjnjc1sn0ggc7z_2y7n27xfh0000gn/T/ipykernel_86986/3301693577.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  torch.tensor(allowed_modes)\n",
      "Processing result_dic:   1%|▏         | 1/79 [00:01<02:15,  1.74s/dataframe]\n"
     ]
    }
   ],
   "source": [
    "def process_result_dic(result_dic, result_dic_mode_stats):\n",
    "    datalist = []\n",
    "    linegraph_transformation = LineGraph()\n",
    "    \n",
    "    vol_base_case = links_gdf_final['vol_car'].values\n",
    "    capacity_base_case = np.where(links_gdf_final['modes'].str.contains('car'), links_gdf_final['capacity'], 0)\n",
    "    length = links_gdf_final['length'].values\n",
    "    freespeed_base_case = links_gdf_final['freespeed'].values\n",
    "    allowed_modes = encode_modes(links_gdf_final)\n",
    "    close_homes = close_homes_tensor.to_dense()\n",
    "    \n",
    "    edge_index = torch.tensor(edges_base, dtype=torch.long).t().contiguous()\n",
    "    x = torch.zeros((len(nodes), 1), dtype=torch.float)\n",
    "    data = Data(edge_index=edge_index, x=x)\n",
    "    \n",
    "    for key, df in tqdm(result_dic.items(), desc=\"Processing result_dic\", unit=\"dataframe\"):    \n",
    "        if isinstance(df, pd.DataFrame) and key != \"base_network_no_policies\":\n",
    "            gdf = prepare_gdf(df)\n",
    "            capacities_new, capacity_reduction, highway, freespeed = get_basic_edge_attributes(capacity_base_case, gdf)\n",
    "            district_info, combined_tensor = combine_stacked_tensors(vol_base_case, capacity_base_case, length, freespeed_base_case, allowed_modes, gdf, capacities_new, capacity_reduction, highway, freespeed)\n",
    "            \n",
    "            linegraph_data = linegraph_transformation(data)\n",
    "            linegraph_data.x = combined_tensor\n",
    "        \n",
    "            # add edge attributes: 1 if edge to district, 0 if edge to edge\n",
    "            edge_to_district_index, edge_to_district_attr = compute_edge_attributes(district_info, linegraph_data)\n",
    "            if linegraph_data.edge_attr is None:\n",
    "                linegraph_data.edge_attr = torch.zeros((linegraph_data.edge_index.shape[1] - edge_to_district_index.shape[1], 1), dtype=torch.long)\n",
    "            linegraph_data.edge_attr = torch.cat([linegraph_data.edge_attr, edge_to_district_attr], dim=0)\n",
    "\n",
    "            # add node attributes: 1 if district, 0 if edge\n",
    "            num_edge_nodes, num_district_nodes, existing_feature_dim1, existing_feature_dim2, node_type_feature = compute_node_attributes(district_info, linegraph_data)\n",
    "            if linegraph_data.x is None:\n",
    "                linegraph_data.x = torch.zeros((num_edge_nodes + num_district_nodes, existing_feature_dim1, existing_feature_dim2), dtype=torch.float)\n",
    "            linegraph_data.x = torch.cat([linegraph_data.x, node_type_feature], dim=1)\n",
    "            \n",
    "            linegraph_data.num_nodes = num_edge_nodes + num_district_nodes\n",
    "            linegraph_data.pos = torch.cat([stacked_edge_geometries_tensor, district_centroids_tensor_padded], dim=0)\n",
    "            linegraph_data.y = compute_target_tensor(vol_base_case, gdf, district_info)\n",
    "                        \n",
    "            df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            if df_mode_stats is not None:\n",
    "                numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "                mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "                mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "                linegraph_data.mode_stats = mode_stats_tensor\n",
    "            \n",
    "            if linegraph_data.validate(raise_on_error=True):\n",
    "                datalist.append(linegraph_data)\n",
    "            else:\n",
    "                print(\"Invalid line graph data\")\n",
    "            break\n",
    "    return datalist\n",
    "\n",
    "# Call the function\n",
    "data_processed = process_result_dic(result_dic=result_dic_output_links, result_dic_mode_stats=result_dic_mode_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensors_districts_information = []\n",
    "            \n",
    "            # tensors_with_sociographic_information = [close_homes, \n",
    "            #     activities_home, activities_work, activities_education, activities_shop, activities_leisure, activities_other,\n",
    "            #     stacked_close_start_trips_tensor, stacked_close_end_trips_tensor]\n",
    "                \n",
    "            # tensors_districts_with_sociographic_information = [\n",
    "            #     district_home_counts,\n",
    "            #     district_activities_home, district_activities_work, district_activities_education, district_activities_shop, district_activities_leisure, district_activities_other,\n",
    "            #     stacked_district_start_trips_tensor, stacked_district_end_trips_tensor]\n",
    "            \n",
    "            # linegraph_x_districts = torch.tensor(np.column_stack(tensors_districts_information), dtype=torch.float)\n",
    "            # stack linegraph_x and linegraph_x_districts\n",
    "            \n",
    "            \n",
    "            # linegraph_pos = torch.tensor(np.column_stack(edge_midpoint_t), dtype=torch.float)\n",
    "\n",
    "            # Print shapes for debugging\n",
    "            # for i, t in enumerate(tensors_edge_information):\n",
    "            #     print(f\"Shape of tensor {i}: {t.shape}\")\n",
    "\n",
    "            # linegraph_data.x = linegraph_x\n",
    "            # linegraph_data.pos = linegraph_pos\n",
    "            # linegraph_data.y = target_values\n",
    "            \n",
    "            # df_mode_stats = result_dic_mode_stats.get(key)\n",
    "            # if df_mode_stats is not None:\n",
    "            #     numeric_cols = df_mode_stats.select_dtypes(include=[np.number]).columns\n",
    "            #     mode_stats_numeric = df_mode_stats[numeric_cols].astype(float)\n",
    "            #     mode_stats_tensor = torch.tensor(mode_stats_numeric.values, dtype=torch.float)\n",
    "            #     linegraph_data.mode_stats = mode_stats_tensor\n",
    "            # if linegraph_data.validate(raise_on_error=True):\n",
    "            #     datalist.append(linegraph_data)\n",
    "            # else:\n",
    "            #     print(\"Invalid line graph data\")\n",
    "            # print(\"LG DATA EDGE INDEx\")\n",
    "            # for lg_data in datalist:\n",
    "            #     print(lg_data.edge_index.shape)\n",
    "            #     print(lg_data.edge_index)\n",
    "                \n",
    "            # print(lg_data.edge_index.shape for lg_data in datalist)\n",
    "\n",
    "            # print(lg_data.edge_index for lg_data in datalist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [2.0037e+01, 2.0037e+01, 2.0037e+01, 2.0037e+01, 2.0037e+01,\n",
       "          2.0037e+01],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [9.7760e+00, 9.7760e+00, 9.7760e+00, 9.7760e+00, 9.7760e+00,\n",
       "          9.7760e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         ...,\n",
       "         [8.3333e+00, 8.3333e+00, 8.3333e+00, 8.3333e+00, 8.3333e+00,\n",
       "          8.3333e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.7440e+06, 2.7440e+06, 2.7440e+06, 2.7440e+06, 2.7440e+06,\n",
       "          2.7440e+06],\n",
       "         [2.7033e+06, 2.7033e+06, 2.7033e+06, 2.7033e+06, 2.7033e+06,\n",
       "          2.7033e+06],\n",
       "         ...,\n",
       "         [1.5976e+05, 1.5976e+05, 1.5976e+05, 1.5976e+05, 1.5976e+05,\n",
       "          1.5976e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [3.4090e+06, 3.4090e+06, 3.4090e+06, 3.4090e+06, 3.4090e+06,\n",
       "          3.4090e+06],\n",
       "         [2.3307e+06, 2.3307e+06, 2.3307e+06, 2.3307e+06, 2.3307e+06,\n",
       "          2.3307e+06],\n",
       "         ...,\n",
       "         [1.4539e+05, 1.4539e+05, 1.4539e+05, 1.4539e+05, 1.4539e+05,\n",
       "          1.4539e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]],\n",
       "\n",
       "        [[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "          0.0000e+00],\n",
       "         [2.8598e+06, 2.8598e+06, 2.8598e+06, 2.8598e+06, 2.8598e+06,\n",
       "          2.8598e+06],\n",
       "         [2.8126e+06, 2.8126e+06, 2.8126e+06, 2.8126e+06, 2.8126e+06,\n",
       "          2.8126e+06],\n",
       "         ...,\n",
       "         [1.2795e+05, 1.2795e+05, 1.2795e+05, 1.2795e+05, 1.2795e+05,\n",
       "          1.2795e+05],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00],\n",
       "         [1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00, 1.0000e+00,\n",
       "          1.0000e+00]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processed[0]['x']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save for further processing with GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(data_processed, result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.scatter(persons_with_homes.geometry.x, persons_with_homes.geometry.y, s=1, color='blue', alpha=0.5)\n",
    "# plt.scatter(persons_with_home_within_linear_ring.geometry.x, persons_with_home_within_linear_ring.geometry.y, s=1, color='red', alpha=0.5)\n",
    "# plt.title('Locations of Persons with Homes')\n",
    "# plt.xlabel('Longitude')\n",
    "# plt.ylabel('Latitude')\n",
    "# plt.show()\n",
    "\n",
    "# from shapely.geometry import LineString\n",
    "# from shapely.geometry import MultiPolygon\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create a LineString\n",
    "# line = LineString([(10, 10), (20, 10)])\n",
    "\n",
    "# # Create a buffer around the line\n",
    "# buffered_line = line.buffer(2, cap_style=\"round\")\n",
    "\n",
    "# # Plot the original line and the buffered area\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# x, y = line.xy\n",
    "# plt.plot(x, y, color='blue', label='Original Line')\n",
    "# if isinstance(buffered_line, MultiPolygon):\n",
    "#     for polygon in buffered_line:\n",
    "#         x, y = polygon.exterior.xy\n",
    "#         plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "# else:\n",
    "#     x, y = buffered_line.exterior.xy\n",
    "#     plt.fill(x, y, alpha=0.5, color='lightblue', label='Buffered Area')\n",
    "\n",
    "# plt.title('Line with Buffered Area')\n",
    "# plt.xlabel('X-axis')\n",
    "# plt.ylabel('Y-axis')\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.axis('equal')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# def check_trips_equivalence(close_trips_start, close_trips_end):\n",
    "#     \"\"\"\n",
    "#     Check if close_trips_start and close_trips_end are equivalent.\n",
    "    \n",
    "#     Args:\n",
    "#     close_trips_start (list): List of tuples for start trips\n",
    "#     close_trips_end (list): List of tuples for end trips\n",
    "    \n",
    "#     Returns:\n",
    "#     bool: True if equivalent, False otherwise\n",
    "#     \"\"\"\n",
    "#     if len(close_trips_start) != len(close_trips_end):\n",
    "#         print(\"Lists have different lengths.\")\n",
    "#         return False\n",
    "    \n",
    "#     differences = []\n",
    "#     for i, (start, end) in enumerate(zip(close_trips_start, close_trips_end)):\n",
    "#         if start != end:\n",
    "#             differences.append((i, start, end))\n",
    "    \n",
    "#     if not differences:\n",
    "#         print(\"The lists are identical.\")\n",
    "#         return True\n",
    "#     else:\n",
    "#         print(f\"Found {len(differences)} differences:\")\n",
    "#         for diff in differences[:10]:  # Print first 10 differences\n",
    "#             print(f\"Index {diff[0]}: Start {diff[1]}, End {diff[2]}\")\n",
    "#         if len(differences) > 10:\n",
    "#             print(f\"... and {len(differences) - 10} more differences.\")\n",
    "#         return False\n",
    "\n",
    "# # Usage\n",
    "# are_equivalent = check_trips_equivalence(close_trips_start, close_trips_end)\n",
    "# print(f\"Are the trip lists equivalent? {are_equivalent}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Paris_Analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
